# Day 15: Prompt结构设计与可测试性原则

## 🎯 1. 核心风险与测试目标 (20分钟)

> **测试工程师视角**：Prompt是AI系统的"软代码"，但大多数Prompt像 spaghetti code 一样难以测试。如果不关注Prompt的可测试性，线上会出现：
> - 输出格式不稳定，下游解析频繁失败
> - 相同输入产生不同输出，回归测试无法通过
> - 边界情况未定义，异常输入导致不可预期行为
> - Prompt变更影响未知，每次上线都是一次冒险

### 1.1 业务风险点

| 风险等级 | 风险场景 | 业务影响 |
|---------|---------|---------|
| 🔴 L1 阻断性 | 输出格式不一致导致下游解析失败 | 数据管道断裂，业务流程中断 |
| 🔴 L1 阻断性 | 温度参数波动导致关键决策变化 | 金融/医疗决策错误，法律责任 |
| 🟡 L2 高优先级 | 边界情况未定义，异常输入处理不当 | 用户体验差，客服压力增加 |
| 🟡 L2 高优先级 | Prompt变更无法回归验证 | 每次发布引入未知风险 |

### 1.2 测试思路

**核心策略**：将Prompt视为"软代码"，应用软件工程的可测试性原则：

1. **确定性验证**：相同输入必须产生相同输出（温度=0时）
2. **边界覆盖**：定义明确的任务边界和超出范围的处理方式
3. **契约测试**：验证输出格式承诺是否兑现
4. **变更影响分析**：Prompt修改前后的行为对比

**测试输入设计原则**：
- 格式一致性测试：相同输入多次执行，输出差异度测量
- 边界值测试：任务边界内/边界外/恰好在边界上的输入
- 噪声敏感性测试：输入微小变化对输出的影响
- 温度稳定性测试：不同温度参数下的输出分布

---

## 📚 2. 理论基础

### 2.1 可测试Prompt的设计要素

| 要素 | 设计技术 | 测试价值 | 反模式 |
|-----|---------|---------|--------|
| **确定性增强** | Few-shot示例、输出格式强制、约束条件明确 | 回归测试可靠、输出可预期 | 模糊描述、过度开放 |
| **边界明确** | 任务范围声明、超出范围的处理指令 | 边界值测试有依据 | 范围模糊、默认行为未定义 |
| **评估接口** | 自评估请求、结构化输出、置信度分数 | 自动化质量监控 | 纯文本输出、无质量指标 |
| **版本标识** | Prompt版本号、变更记录、哈希标识 | 支持A/B测试和回归分析 | 版本混乱、变更不可追溯 |

### 2.2 CO-STAR框架与测试映射

```
CO-STAR框架：
┌─────────────┬─────────────────────────────┬─────────────────────────┐
│ 维度        │ 设计选择                     │ 测试关注点               │
├─────────────┼─────────────────────────────┼─────────────────────────┤
│ Context     │ 背景信息                     │ 上下文缺失时的默认行为    │
│ Objective   │ 目标任务                     │ 目标边界、成功标准       │
│ Style       │ 输出风格                     │ 风格一致性、风格漂移     │
│ Tone        │ 语气态度                     │ 语气稳定性、极端输入响应  │
│ Audience    │ 目标受众                     │ 受众适配性测试           │
│ Response    │ 响应格式                     │ 格式契约验证、Schema合规  │
└─────────────┴─────────────────────────────┴─────────────────────────┘
```

### 2.3 Prompt确定性测试模型

```
确定性 = f(温度参数, Prompt约束强度, 输入明确性)

高确定性区域：
- 温度 = 0
- Prompt包含Few-shot示例
- 输出格式强制（JSON/XML Schema）
- 输入约束明确

低确定性区域：
- 温度 > 0.7
- Prompt开放式描述
- 自由文本输出
- 模糊输入
```

---

## 🧪 3. 实验验证任务

请运行本目录下的 `test_day15.py`，观察控制台输出的真实日志。

### 3.1 测试覆盖范围

```python
# 测试用例分类
├── 确定性测试（6个）
│   ├── 温度稳定性（2个）    # 温度=0时输出一致性
│   ├── Few-shot效果（2个）  # 示例对确定性的影响
│   └── 格式强制（2个）      # Schema约束效果
│
├── 边界明确性测试（6个）
│   ├── 任务边界（3个）      # 边界内/边界外/恰好在边界
│   ├── 范围声明（2个）      # 超出范围处理
│   └── 默认行为（1个）      # 未定义输入的处理
│
└── 可观测性测试（4个）
    ├── 结构化输出（2个）    # JSON/XML输出验证
    ├── 置信度指标（1个）    # 置信度分数可用性
    └── 自评估能力（1个）    # 质量自评准确性
```

### 3.2 预期观察指标

| 指标 | 通过标准 | 风险信号 |
|-----|---------|---------|
| 输出一致性 | 温度=0时，相同输入输出相似度≥95% | 相同输入产生显著不同输出 |
| 格式合规率 | 结构化输出Schema合规率≥98% | JSON解析失败、字段缺失 |
| 边界识别率 | 边界外输入正确处理率≥90% | 超出范围任务被错误执行 |
| 回归稳定性 | Prompt变更前后行为差异<5% | 微小改动导致行为剧变 |

---

## 📝 4. 产出要求

将运行结果贴回给 Trae，让其生成 `report_day15.md` 质量分析报告。

### 4.1 报告应包含

1. **确定性评估**：不同Prompt设计的输出稳定性对比
2. **边界覆盖分析**：边界情况处理的成功率统计
3. **可观测性评分**：结构化输出、置信度指标的质量
4. **可测试性改进建议**：针对当前Prompt的具体优化方案

### 4.2 关键问题

- 温度参数对输出稳定性的量化影响？
- Few-shot示例数量与确定性提升的关系曲线？
- 哪些边界情况最容易被错误处理？
- 结构化输出 vs 自由文本输出的测试效率对比？

---

## 🔗 5. 延伸学习

- [OpenAI: Prompt Engineering Best Practices](https://platform.openai.com/docs/guides/prompt-engineering)
- [Google: Prompt Engineering for Text Generation](https://cloud.google.com/vertex-ai/docs/generative-ai/text/text-prompts)
- [Microsoft: Prompt Engineering Techniques](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering)
- 明日预告：Day 16 将深入CO-STAR框架各维度的质量测试方法
