# Day 01: LLM工作原理与测试视角的模型理解（温度参数验证）

## 🎯 1. 核心风险与测试目标

### 1.1 测试工程师视角
> **核心原则**：开发只管跑通，我们要想办法把它搞崩溃。理解LLM底层机制是发现风险的前提。

### 1.2 业务风险点

| 风险类别 | 具体风险 | 线上事故场景 |
|---------|---------|-------------|
| **输出不稳定** | 温度参数设置不当导致输出随机性过高 | 客服机器人同一问题给出矛盾答案，用户投诉 |
| **确定性缺失** | 关键业务场景需要稳定输出但参数配置错误 | 医疗/金融建议每次回答不同，引发合规风险 |
| **可重复性缺陷** | 无法复现问题导致调试困难 | 用户报告bug但开发无法复现，问题悬而未决 |
| **创意vs精确失衡** | 创意任务过于死板或精确任务过于随机 | 文案生成千篇一律，代码生成充满随机错误 |

### 1.3 测试思路

**边界值探测策略**：
- **温度=0**：测试确定性输出，验证是否始终选择最高概率token
- **温度=0.5-0.7**：测试平衡状态，验证输出合理性
- **温度=1.0-2.0**：测试高随机性，验证输出发散程度
- **极端值**：温度接近0或远大于1时的边界行为

**异常输入探测**：
- 相同prompt多次调用，对比输出一致性
- 语义等价但表述不同的prompt，验证输出稳定性
- 超长prompt下的温度效应

---

## 📚 2. LLM核心工作原理（测试必备知识）

### 2.1 自回归生成机制

```
输入Prompt → Tokenize → 模型推理 → 概率分布 → 采样 → 输出Token → 循环
```

**关键理解**：
1. **Tokenization**：文本被切分为token（非单词级），影响概率计算粒度
2. **概率分布**：模型输出的是词汇表上每个token的概率分布 `P(token|context)`
3. **自回归**：生成下一个token时，之前生成的token会成为新的上下文

### 2.2 温度参数（Temperature）数学原理

温度参数控制概率分布的"尖锐程度"：

```python
# 原始概率分布
logits = [2.0, 1.0, 0.1]  # 模型输出的原始分数

# 应用温度参数 T
T = 0.7
adjusted_logits = [l / T for l in logits]  # [2.86, 1.43, 0.14]

# Softmax转换为概率
probabilities = softmax(adjusted_logits)
```

**温度效应**：

| 温度值 | 效果 | 适用场景 |
|-------|------|---------|
| T→0 | 概率分布极度尖锐，几乎总是选择最高概率token | 代码生成、数学计算、事实问答 |
| T=0.3-0.5 | 低随机性，输出稳定但有轻微变化 | 结构化数据提取、分类任务 |
| T=0.7-1.0 | 中等随机性，平衡创意与准确 | 通用对话、内容生成 |
| T>1.0 | 概率分布趋于平坦，随机性显著增加 | 创意写作、头脑风暴 |
| T→∞ | 接近均匀分布，完全随机 | 测试极端情况 |

### 2.3 采样策略对比

```
Greedy Decoding (T=0): 始终选择 argmax(P)
Beam Search: 维护多个候选序列，选择整体概率最高的
Top-k Sampling: 仅从概率最高的k个token中采样
Top-p (Nucleus) Sampling: 从累积概率达到p的最小token集合中采样
Temperature Sampling: 通过温度调整概率分布后采样
```

---

## 🧪 3. 实验验证任务

### 3.1 实验环境准备

确保已安装依赖：
```bash
pip install openai pytest python-dotenv
```

配置环境变量（`.env`文件）：
```
OPENAI_API_KEY=your_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1  # 或其他兼容端点
```

### 3.2 运行测试

执行本目录下的测试脚本：
```bash
cd AI-QA-60Days/Phase1_LLM_Foundation/Day01
pytest test_day01.py -v -s
```

### 3.3 观察重点

1. **温度=0时**：多次调用同一prompt，输出是否完全一致？
2. **温度变化时**：输出多样性如何变化？
3. **语义稳定性**：相同语义的不同表述是否产生一致输出？
4. **边界行为**：极端温度值下的模型表现

---

## 📝 4. 产出要求

将运行结果（控制台输出）贴回给 Trae，让其生成 `report_day01.md` 质量分析报告。

报告应包含：
- 温度参数对输出稳定性的量化影响
- 不同温度设置下的风险等级评估
- 针对业务场景的温度参数建议
- 发现的异常行为或边界情况

---

## 🔍 5. 测试思维拓展

### 5.1 温度参数测试的检查清单

- [ ] 相同prompt、相同温度、多次调用的一致性
- [ ] 温度从0到2的梯度变化测试
- [ ] 长文本生成中的温度累积效应
- [ ] 多轮对话中温度对上下文理解的影响
- [ ] 不同模型（GPT-3.5/4/Claude等）的温度敏感度差异

### 5.2 进阶测试方向

1. **对抗性测试**：构造让温度参数失效的边缘case
2. **组合测试**：温度 + Top-p + Top-k 的交互效应
3. **性能测试**：不同温度下的推理延迟差异
4. **安全测试**：高温度是否更容易产生有害输出
