"""
è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬ï¼šDay 02 - Tokenizerç¼–ç æœºåˆ¶ä¸ä¸Šä¸‹æ–‡çª—å£è¾¹ç•Œæµ‹è¯•
ç›®æ ‡ï¼šéªŒè¯Tokenizerç¼–ç å·®å¼‚ã€ä¸Šä¸‹æ–‡æˆªæ–­é£é™©ã€ä½ç½®æ•æ„Ÿæ€§
"""

import os
import pytest
from typing import List, Dict, Tuple
from dataclasses import dataclass
from datetime import datetime


@dataclass
class TokenizerTestResult:
    """Tokenizeræµ‹è¯•ç»“æœæ•°æ®ç±»"""
    text: str
    text_length: int
    token_count: int
    chars_per_token: float
    encoding_name: str
    timestamp: str


@dataclass
class PositionTestResult:
    """ä½ç½®æ•æ„Ÿæ€§æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    key_position: str
    context_length: int
    recall_success: bool
    response: str


class TokenizerTester:
    """Tokenizerä¸ä¸Šä¸‹æ–‡çª—å£æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.encoding_name = "cl100k_base"  # GPT-4é»˜è®¤ç¼–ç 
        self._encoder = None
    
    @property
    def encoder(self):
        """å»¶è¿Ÿåˆå§‹åŒ–tiktokenç¼–ç å™¨"""
        if self._encoder is None:
            try:
                import tiktoken
                self._encoder = tiktoken.get_encoding(self.encoding_name)
            except ImportError:
                print("âš ï¸ tiktokenæœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                self._encoder = None
        return self._encoder
    
    def count_tokens(self, text: str) -> int:
        """
        è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            tokenæ•°é‡
        """
        if self.encoder is None:
            # æ¨¡æ‹Ÿæ¨¡å¼ï¼šåŸºäºå­—ç¬¦æ•°çš„ä¼°ç®—
            return self._estimate_tokens(text)
        
        try:
            tokens = self.encoder.encode(text)
            return len(tokens)
        except Exception as e:
            print(f"âš ï¸ Tokenè®¡æ•°å¤±è´¥: {e}")
            return self._estimate_tokens(text)
    
    def _estimate_tokens(self, text: str) -> int:
        """
        ä¼°ç®—tokenæ•°é‡ï¼ˆæ¨¡æ‹Ÿæ¨¡å¼ï¼‰
        ä¸­æ–‡çº¦1.5å­—ç¬¦/tokenï¼Œè‹±æ–‡çº¦4å­—ç¬¦/token
        """
        import re
        
        # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡æŒ‰1.5å­—ç¬¦/tokenï¼Œå…¶ä»–æŒ‰4å­—ç¬¦/token
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        other_chars = len(text) - chinese_chars
        
        estimated = int(chinese_chars / 1.5 + other_chars / 4)
        return max(1, estimated)
    
    def analyze_text(self, text: str, label: str = "") -> TokenizerTestResult:
        """
        åˆ†ææ–‡æœ¬çš„Tokenizerç‰¹å¾
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            label: æ–‡æœ¬æ ‡ç­¾
            
        Returns:
            åˆ†æç»“æœ
        """
        token_count = self.count_tokens(text)
        text_length = len(text)
        chars_per_token = text_length / token_count if token_count > 0 else 0
        
        return TokenizerTestResult(
            text=text if len(text) < 100 else text[:100] + "...",
            text_length=text_length,
            token_count=token_count,
            chars_per_token=round(chars_per_token, 2),
            encoding_name=self.encoding_name if self.encoder else "estimated",
            timestamp=datetime.now().isoformat()
        )
    
    def generate_context(self, base_text: str, target_tokens: int) -> str:
        """
        ç”ŸæˆæŒ‡å®štokenæ•°é‡çš„ä¸Šä¸‹æ–‡æ–‡æœ¬
        
        Args:
            base_text: åŸºç¡€æ–‡æœ¬ç‰‡æ®µ
            target_tokens: ç›®æ ‡tokenæ•°é‡
            
        Returns:
            ç”Ÿæˆçš„ä¸Šä¸‹æ–‡æ–‡æœ¬
        """
        tokens_per_base = self.count_tokens(base_text)
        repeats = max(1, target_tokens // tokens_per_base)
        
        context = ""
        for i in range(repeats):
            context += f"\n[æ®µè½ {i+1}] {base_text}"
        
        # å¾®è°ƒè‡³æ¥è¿‘ç›®æ ‡tokenæ•°
        while self.count_tokens(context) < target_tokens:
            context += " " + base_text[:20]
        
        return context


# ============ æµ‹è¯•ç”¨ä¾‹ ============

class TestDay02Tokenizer:
    """Day 02: Tokenizerä¸ä¸Šä¸‹æ–‡çª—å£æµ‹è¯•ç±»"""
    
    @pytest.fixture(scope="class")
    def tester(self):
        """æµ‹è¯•å™¨fixture"""
        return TokenizerTester()
    
    def test_chinese_vs_english_efficiency(self, tester):
        """
        æµ‹è¯•ä¸­è‹±æ–‡tokenå‹ç¼©ç‡å·®å¼‚
        
        é£é™©ç‚¹ï¼šä¸­æ–‡å®¢æœç³»ç»Ÿæˆæœ¬å¯èƒ½æ¯”è‹±æ–‡é«˜40-100%
        é¢„æœŸï¼šä¸­æ–‡chars_per_tokenåº”æ˜¾è‘—ä½äºè‹±æ–‡
        """
        print("\n" + "="*60)
        print("ï¿½ æµ‹è¯•ç”¨ä¾‹ 1: ä¸­è‹±æ–‡Tokenå‹ç¼©ç‡å¯¹æ¯”")
        print("="*60)
        
        # è¯­ä¹‰ç›¸è¿‘çš„ä¸­è‹±æ–‡æ–‡æœ¬
        test_cases = [
            ("Hello world, this is a test.", "è‹±æ–‡çŸ­å¥"),
            ("ä½ å¥½ä¸–ç•Œï¼Œè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ã€‚", "ä¸­æ–‡çŸ­å¥"),
            ("Artificial intelligence is transforming the world.", "è‹±æ–‡AIä¸»é¢˜"),
            ("äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œã€‚", "ä¸­æ–‡AIä¸»é¢˜"),
            ("The quick brown fox jumps over the lazy dog.", "è‹±æ–‡å…¨å­—æ¯"),
            ("æ•æ·çš„æ£•è‰²ç‹ç‹¸è·³è¿‡äº†æ‡’æƒ°çš„ç‹—ã€‚", "ä¸­æ–‡å…¨å­—ç¬¦"),
        ]
        
        results = []
        for text, label in test_cases:
            result = tester.analyze_text(text, label)
            results.append((label, result))
            print(f"\nğŸ“„ {label}:")
            print(f"   æ–‡æœ¬: {text[:40]}...")
            print(f"   å­—ç¬¦æ•°: {result.text_length}, Tokenæ•°: {result.token_count}")
            print(f"   å‹ç¼©ç‡: {result.chars_per_token} å­—ç¬¦/token")
        
        # éªŒè¯ï¼šä¸­æ–‡å‹ç¼©ç‡åº”ä½äºè‹±æ–‡
        cn_result = results[1][1]  # ä¸­æ–‡çŸ­å¥
        en_result = results[0][1]  # è‹±æ–‡çŸ­å¥
        
        print(f"\nğŸ“Š å¯¹æ¯”ç»“è®º:")
        print(f"   è‹±æ–‡: {en_result.chars_per_token} å­—ç¬¦/token")
        print(f"   ä¸­æ–‡: {cn_result.chars_per_token} å­—ç¬¦/token")
        print(f"   å·®å¼‚: {en_result.chars_per_token / cn_result.chars_per_token:.1f}x")
        
        assert cn_result.chars_per_token < en_result.chars_per_token, \
            "ä¸­æ–‡å‹ç¼©ç‡åº”ä½äºè‹±æ–‡ï¼Œæˆæœ¬é£é™©ç¡®è®¤ï¼"
        print("âœ… ä¸­è‹±æ–‡å‹ç¼©ç‡å·®å¼‚æµ‹è¯•å®Œæˆ")
    
    def test_special_characters_token_cost(self, tester):
        """
        æµ‹è¯•ç‰¹æ®Šå­—ç¬¦çš„tokenæ¶ˆè€—
        
        é£é™©ç‚¹ï¼šemojiã€ç‰¹æ®Šç¬¦å·å¯èƒ½å¯¼è‡´æˆæœ¬æ¿€å¢
        """
        print("\n" + "="*60)
        print("ğŸ”¬ æµ‹è¯•ç”¨ä¾‹ 2: ç‰¹æ®Šå­—ç¬¦Tokenæ¶ˆè€—æµ‹è¯•")
        print("="*60)
        
        test_cases = [
            ("Hello", "çº¯è‹±æ–‡"),
            ("Hello ğŸŒ", "å¸¦emoji"),
            ("Hello ğŸ‘‹ğŸŒğŸ‰", "å¤šemoji"),
            ("Hello <b>world</b>", "HTMLæ ‡ç­¾"),
            ("Hello\\nWorld\\t!", "è½¬ä¹‰å­—ç¬¦"),
            ("Helloä¸–ç•Œ", "ä¸­è‹±æ··åˆ"),
            ("HelloğŸŒä¸–ç•Œ", "ä¸­è‹±emojiæ··åˆ"),
        ]
        
        print(f"\n{'æ–‡æœ¬ç±»å‹':<20} {'å­—ç¬¦æ•°':<8} {'Tokenæ•°':<8} {'å­—ç¬¦/Token':<12}")
        print("-" * 50)
        
        for text, label in test_cases:
            result = tester.analyze_text(text, label)
            print(f"{label:<20} {result.text_length:<8} {result.token_count:<8} {result.chars_per_token:<12}")
        
        print("\nâš ï¸  é£é™©æé†’ï¼šemojiå¯èƒ½æ¶ˆè€—1-3ä¸ªtokenï¼Œå¤§é‡ä½¿ç”¨ä¼šå¢åŠ æˆæœ¬")
        print("âœ… ç‰¹æ®Šå­—ç¬¦æ¶ˆè€—æµ‹è¯•å®Œæˆ")
    
    def test_context_window_boundary(self, tester):
        """
        æµ‹è¯•ä¸Šä¸‹æ–‡çª—å£è¾¹ç•Œè¡Œä¸º
        
        é£é™©ç‚¹ï¼šè¶…è¿‡çª—å£é™åˆ¶çš„è¾“å…¥ä¼šè¢«é™é»˜æˆªæ–­
        """
        print("\n" + "="*60)
        print("ğŸ”¬ æµ‹è¯•ç”¨ä¾‹ 3: ä¸Šä¸‹æ–‡çª—å£è¾¹ç•Œæµ‹è¯•")
        print("="*60)
        
        # æ¨¡æ‹Ÿ4Kçª—å£æ¨¡å‹
        window_sizes = [512, 1024, 2048, 4096]
        base_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­ï¼Œç”¨äºæ¨¡æ‹Ÿé•¿æ–‡æœ¬å†…å®¹ã€‚"
        
        print("\nğŸ“ çª—å£å¤§å°ä¸æ–‡æœ¬å®¹é‡å…³ç³»:")
        print(f"{'çª—å£å¤§å°':<12} {'å¯å®¹çº³ä¸­æ–‡å­—æ•°':<16} {'ä¼°ç®—é¡µæ•°':<10}")
        print("-" * 40)
        
        for window in window_sizes:
            # ä¼°ç®—å¯å®¹çº³çš„ä¸­æ–‡å­—æ•°ï¼ˆå‡è®¾1.5å­—ç¬¦/tokenï¼‰
            chinese_chars = int(window * 1.5)
            pages = chinese_chars / 500  # å‡è®¾æ¯é¡µ500å­—
            print(f"{window:<12} {chinese_chars:<16} {pages:<10.1f}")
        
        # æ„é€ æ¥è¿‘è¾¹ç•Œçš„è¾“å…¥
        print("\nğŸ§ª è¾¹ç•Œæµ‹è¯•:")
        target = 100  # æ¨¡æ‹Ÿå°çª—å£ä¾¿äºæµ‹è¯•
        context = tester.generate_context(base_text, target)
        actual_tokens = tester.count_tokens(context)
        
        print(f"   ç›®æ ‡Tokenæ•°: {target}")
        print(f"   å®é™…Tokenæ•°: {actual_tokens}")
        print(f"   è¯¯å·®: {abs(actual_tokens - target)} tokens")
        
        print("\nâš ï¸  é£é™©æé†’ï¼šè¶…è¿‡çª—å£é™åˆ¶çš„è¾“å…¥ä¼šè¢«é™é»˜æˆªæ–­ï¼Œå…³é”®ä¿¡æ¯å¯èƒ½ä¸¢å¤±ï¼")
        print("âœ… ä¸Šä¸‹æ–‡çª—å£è¾¹ç•Œæµ‹è¯•å®Œæˆ")
    
    def test_position_sensitivity_lost_in_middle(self, tester):
        """
        æµ‹è¯•"Lost in the Middle"ä½ç½®æ•æ„Ÿæ€§
        
        é£é™©ç‚¹ï¼šå…³é”®ä¿¡æ¯æ”¾åœ¨é•¿æ–‡æ¡£ä¸­é—´ä¼šè¢«æ¨¡å‹å¿½ç•¥
        """
        print("\n" + "="*60)
        print("ğŸ”¬ æµ‹è¯•ç”¨ä¾‹ 4: ä½ç½®æ•æ„Ÿæ€§æµ‹è¯• (Lost in the Middle)")
        print("="*60)
        
        # æ„é€ æµ‹è¯•åœºæ™¯
        filler_text = "è¿™æ˜¯ä¸€æ®µå¡«å……æ–‡æœ¬ï¼Œç”¨äºæ¨¡æ‹Ÿé•¿æ–‡æ¡£å†…å®¹ã€‚æ–‡æ¡£åŒ…å«å¤šä¸ªæ®µè½å’Œä¸»é¢˜ã€‚"
        key_info = "ã€å…³é”®ä¿¡æ¯ï¼šéªŒè¯ç æ˜¯123456ã€‘"
        
        # æ¨¡æ‹Ÿä¸åŒä½ç½®çš„å¬å›ç‡ï¼ˆåŸºäºç ”ç©¶æ•°æ®çš„æ¨¡æ‹Ÿï¼‰
        positions = [
            ("å¼€å¤´", 0.95),
            ("1/4å¤„", 0.75),
            ("ä¸­é—´", 0.45),  # Lost in the middle!
            ("3/4å¤„", 0.70),
            ("ç»“å°¾", 0.90),
        ]
        
        print("\nğŸ“Š å…³é”®ä¿¡æ¯ä½ç½®ä¸å¬å›ç‡å…³ç³»:")
        print(f"{'ä½ç½®':<10} {'å¬å›ç‡':<10} {'é£é™©ç­‰çº§':<10}")
        print("-" * 35)
        
        for position, recall_rate in positions:
            if recall_rate >= 0.8:
                risk = "ä½"
            elif recall_rate >= 0.6:
                risk = "ä¸­"
            else:
                risk = "é«˜ âš ï¸"
            print(f"{position:<10} {recall_rate:<10.0%} {risk:<10}")
        
        print("\nğŸ“ˆ å¯è§†åŒ–:")
        for position, recall_rate in positions:
            bar = "â–ˆ" * int(recall_rate * 20)
            print(f"   {position:<6} |{bar:<20}| {recall_rate:.0%}")
        
        print("\nâš ï¸  é£é™©æé†’ï¼šå…³é”®ä¿¡æ¯åº”æ”¾åœ¨æ–‡æ¡£å¼€å¤´æˆ–ç»“å°¾ï¼Œé¿å…æ”¾åœ¨ä¸­é—´ä½ç½®ï¼")
        print("âœ… ä½ç½®æ•æ„Ÿæ€§æµ‹è¯•å®Œæˆ")
    
    def test_multilingual_token_efficiency(self, tester):
        """
        æµ‹è¯•å¤šè¯­è¨€tokenæ•ˆç‡å¯¹æ¯”
        
        é£é™©ç‚¹ï¼šå¤šè¯­è¨€æ··åˆåœºæ™¯æˆæœ¬éš¾ä»¥é¢„ä¼°
        """
        print("\n" + "="*60)
        print("ğŸ”¬ æµ‹è¯•ç”¨ä¾‹ 5: å¤šè¯­è¨€Tokenæ•ˆç‡å¯¹æ¯”")
        print("="*60)
        
        # ç›¸åŒè¯­ä¹‰çš„ä¸åŒè¯­è¨€è¡¨è¾¾
        translations = [
            ("Artificial Intelligence", "English"),
            ("äººå·¥æ™ºèƒ½", "Chinese"),
            ("äººå·¥çŸ¥èƒ½", "Japanese"),
            ("ì¸ê³µì§€ëŠ¥", "Korean"),
            ("Intelligence artificielle", "French"),
            ("KÃ¼nstliche Intelligenz", "German"),
        ]
        
        print(f"\n{'è¯­è¨€':<12} {'æ–‡æœ¬':<25} {'Tokenæ•°':<8} {'å­—ç¬¦/Token':<12}")
        print("-" * 60)
        
        results = []
        for text, lang in translations:
            result = tester.analyze_text(text, lang)
            results.append((lang, result))
            print(f"{lang:<12} {text:<25} {result.token_count:<8} {result.chars_per_token:<12}")
        
        # æ‰¾å‡ºæœ€é«˜å’Œæœ€ä½æ•ˆç‡
        sorted_results = sorted(results, key=lambda x: x[1].chars_per_token)
        most_efficient = sorted_results[-1]
        least_efficient = sorted_results[0]
        
        print(f"\nğŸ“Š æ•ˆç‡å¯¹æ¯”:")
        print(f"   æœ€é«˜æ•ˆç‡: {most_efficient[0]} ({most_efficient[1].chars_per_token} å­—ç¬¦/token)")
        print(f"   æœ€ä½æ•ˆç‡: {least_efficient[0]} ({least_efficient[1].chars_per_token} å­—ç¬¦/token)")
        print(f"   æ•ˆç‡å·®å¼‚: {most_efficient[1].chars_per_token / least_efficient[1].chars_per_token:.1f}x")
        
        print("\nâš ï¸  é£é™©æé†’ï¼šå¤šè¯­è¨€ç³»ç»Ÿå¿…é¡»æŒ‰å®é™…è¯­è¨€åˆ†å¸ƒä¼°ç®—æˆæœ¬ï¼")
        print("âœ… å¤šè¯­è¨€æ•ˆç‡æµ‹è¯•å®Œæˆ")
    
    def test_cost_estimation_risk(self, tester):
        """
        æµ‹è¯•æˆæœ¬ä¼°ç®—é£é™©åœºæ™¯
        
        é£é™©ç‚¹ï¼šåŸºäºå­—ç¬¦æ•°çš„æˆæœ¬ä¼°ç®—å¯èƒ½å¯¼è‡´ä¸¥é‡åå·®
        """
        print("\n" + "="*60)
        print("ğŸ”¬ æµ‹è¯•ç”¨ä¾‹ 6: æˆæœ¬ä¼°ç®—é£é™©åˆ†æ")
        print("="*60)
        
        # æ¨¡æ‹Ÿå®¢æœåœºæ™¯
        scenarios = [
            ("è‹±æ–‡å®¢æœå¯¹è¯", "Hello, I need help with my order."),
            ("ä¸­æ–‡å®¢æœå¯¹è¯", "ä½ å¥½ï¼Œæˆ‘éœ€è¦å¸®åŠ©å¤„ç†æˆ‘çš„è®¢å•é—®é¢˜ã€‚"),
            ("æ··åˆå®¢æœå¯¹è¯", "Helloä½ å¥½ï¼ŒI need helpæˆ‘éœ€è¦å¸®åŠ©ã€‚"),
        ]
        
        # å‡è®¾æ¯1K tokens $0.01
        price_per_1k = 0.01
        
        print(f"\n{'åœºæ™¯':<16} {'å­—ç¬¦æ•°':<8} {'Tokenæ•°':<8} {'ä¼°ç®—æˆæœ¬':<12} {'æŒ‰å­—ç¬¦ä¼°ç®—':<12} {'è¯¯å·®':<10}")
        print("-" * 75)
        
        for label, text in scenarios:
            char_count = len(text)
            token_count = tester.count_tokens(text)
            actual_cost = (token_count / 1000) * price_per_1k
            
            # é”™è¯¯ä¼°ç®—ï¼šå‡è®¾1å­—ç¬¦=1token
            wrong_cost = (char_count / 1000) * price_per_1k
            error = ((wrong_cost - actual_cost) / actual_cost * 100) if actual_cost > 0 else 0
            
            print(f"{label:<16} {char_count:<8} {token_count:<8} ${actual_cost:<11.6f} ${wrong_cost:<11.6f} {error:>+6.0f}%")
        
        print("\nâš ï¸  é£é™©æé†’ï¼šæŒ‰å­—ç¬¦æ•°ä¼°ç®—æˆæœ¬å¯èƒ½å¯¼è‡´ä¸¥é‡ä½ä¼°ï¼Œä¸­æ–‡åœºæ™¯å°¤ä¸ºæ˜æ˜¾ï¼")
        print("âœ… æˆæœ¬ä¼°ç®—é£é™©æµ‹è¯•å®Œæˆ")
    
    def test_tokenizer_summary_report(self, tester):
        """
        ç”ŸæˆTokenizeræµ‹è¯•æ±‡æ€»æŠ¥å‘Š
        """
        print("\n" + "="*60)
        print("ğŸ“‹ Tokenizeræµ‹è¯•æ±‡æ€»æŠ¥å‘Š")
        print("="*60)
        
        print("\nğŸ”´ é«˜é£é™©é¡¹:")
        print("   1. ä¸­æ–‡tokenæ•ˆç‡æ¯”è‹±æ–‡ä½40-100%ï¼Œæˆæœ¬éœ€å•ç‹¬è¯„ä¼°")
        print("   2. é•¿æ–‡æ¡£ä¸­é—´ä½ç½®ä¿¡æ¯å¬å›ç‡å¯èƒ½ä½äº50%")
        print("   3. è¶…è¿‡çª—å£é™åˆ¶çš„è¾“å…¥ä¼šè¢«é™é»˜æˆªæ–­")
        
        print("\nğŸŸ¡ ä¸­é£é™©é¡¹:")
        print("   1. Emojiå’Œç‰¹æ®Šç¬¦å·å¯èƒ½æ¶ˆè€—é¢å¤–token")
        print("   2. å¤šè¯­è¨€æ··åˆåœºæ™¯æˆæœ¬éš¾ä»¥é¢„ä¼°")
        print("   3. ä¸åŒæ¨¡å‹Tokenizerå·®å¼‚å¯¼è‡´è¿ç§»æˆæœ¬")
        
        print("\nâœ… æµ‹è¯•å»ºè®®:")
        print("   1. ç”Ÿäº§ç¯å¢ƒå¿…é¡»ä½¿ç”¨å®é™…Tokenizerè¿›è¡Œæˆæœ¬ä¼°ç®—")
        print("   2. å…³é”®ä¿¡æ¯åº”æ”¾åœ¨æ–‡æ¡£å¼€å¤´æˆ–ç»“å°¾")
        print("   3. é•¿æ–‡æœ¬åœºæ™¯éœ€éªŒè¯å®é™…æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦")
        print("   4. å»ºç«‹tokenä½¿ç”¨ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶")
        
        print("\n" + "="*60)
        print("âœ… Day 02 å…¨éƒ¨æµ‹è¯•æ‰§è¡Œå®Œæ¯•")
        print("ğŸ“¤ è¯·å°†ä¸Šæ–¹æ—¥å¿—å‘ç»™ Trae ç”Ÿæˆ report_day02.md è´¨é‡åˆ†ææŠ¥å‘Š")
        print("="*60)


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
