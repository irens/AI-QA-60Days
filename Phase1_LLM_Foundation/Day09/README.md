# Day 09: 幻觉检测与事实一致性验证

## 🎯 1. 核心风险与测试目标

### 1.1 测试工程师视角
> **核心原则**：开发只管跑通，我们要想办法把它搞崩溃。LLM的幻觉不是"偶尔出错"，而是**系统性风险**——它会在你意想不到的时候，用最自信的语气说出最离谱的谎言。

### 1.2 业务风险点

| 风险类别 | 具体风险 | 线上事故场景 |
|---------|---------|-------------|
| **事实幻觉** | 模型生成与事实不符的内容 | 医疗AI给出错误用药建议，导致患者伤害 |
| **引用幻觉** | 编造不存在的来源或数据 | 法律AI引用虚假判例，导致律师败诉 |
| **逻辑幻觉** | 推理过程正确但结论错误 | 金融AI基于错误计算给出投资建议 |
| **一致性幻觉** | 同一问题多次回答相互矛盾 | 客服AI前后说法不一，引发用户投诉 |
| **过度自信** | 错误内容以高置信度输出 | 用户被误导后产生严重后果 |

### 1.3 测试思路

**幻觉检测策略**：
- **NLI-based检测**：使用自然语言推理模型判断输出与事实的蕴含关系
- **知识图谱验证**：将关键事实与结构化知识库交叉验证
- **多源交叉验证**：同一问题查询多个模型，标记不一致回答
- **置信度校准**：检测模型对错误答案的过度自信

**事实一致性验证策略**：
- **声明抽取**：从生成文本中提取可验证的事实声明
- **外部知识检索**：使用搜索引擎/知识库验证声明真伪
- **自洽性检查**：同一问题多次采样，检查回答一致性
- **溯源验证**：要求模型提供信息来源，验证来源真实性

---

## 📚 2. 幻觉检测原理（测试必备知识）

### 2.1 幻觉的本质与分类

```
┌─────────────────────────────────────────────────────────────────┐
│                        LLM幻觉类型图谱                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────┐    ┌─────────────────┐                     │
│  │   事实性幻觉     │    │   忠实性幻觉     │                     │
│  │  (Factuality)   │    │  (Faithfulness) │                     │
│  └────────┬────────┘    └────────┬────────┘                     │
│           │                      │                              │
│     ┌─────┴─────┐          ┌─────┴─────┐                       │
│     │           │          │           │                       │
│  ┌──▼──┐    ┌──▼──┐    ┌──▼──┐    ┌──▼──┐                     │
│  │编造 │    │错误 │    │矛盾 │    │遗漏 │                     │
│  │信息 │    │关联 │    │信息 │    │信息 │                     │
│  └─────┘    └─────┘    └─────┘    └─────┘                     │
│                                                                 │
│  示例：                    示例：                               │
│  "爱因斯坦获得诺贝尔文学奖"  "根据文档A，结论B"（文档不支持）     │
│  "2025年奥运会将在北京举行"  回答与上下文矛盾                    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 2.2 NLI-based幻觉检测原理

自然语言推理（NLI）是检测幻觉的核心技术：

| 关系类型 | 含义 | 幻觉判断 |
|---------|------|---------|
| **蕴含(Entailment)** | 前提支持假设 | ✅ 无幻觉 |
| **矛盾(Contradiction)** | 前提否定假设 | ❌ 事实幻觉 |
| **中立(Neutral)** | 前提与假设无关 | ⚠️ 可能幻觉（需进一步验证） |

```python
# NLI-based幻觉检测核心逻辑

def detect_hallucination_nli(answer: str, reference: str, nli_model) -> Dict:
    """
    使用NLI模型检测幻觉
    
    Args:
        answer: 模型生成的回答
        reference: 参考事实（上下文/知识库）
        nli_model: NLI推理模型
    
    Returns:
        {
            "entailment_score": 0.85,    # 蕴含分数
            "contradiction_score": 0.05,  # 矛盾分数
            "neutral_score": 0.10,        # 中立分数
            "is_hallucination": False,    # 是否幻觉
            "confidence": 0.85            # 检测置信度
        }
    """
    # 1. 将回答拆分为独立声明
    claims = extract_claims(answer)
    
    results = []
    for claim in claims:
        # 2. 使用NLI模型判断声明与参考的关系
        nli_result = nli_model.predict(reference, claim)
        
        # 3. 判断是否为幻觉
        is_hallucination = nli_result["contradiction"] > 0.5 or \
                          (nli_result["neutral"] > 0.7 and not verifiable(claim))
        
        results.append({
            "claim": claim,
            "entailment": nli_result["entailment"],
            "contradiction": nli_result["contradiction"],
            "is_hallucination": is_hallucination
        })
    
    # 4. 综合判断
    hallucination_ratio = sum(r["is_hallucination"] for r in results) / len(results)
    
    return {
        "claims": results,
        "hallucination_ratio": hallucination_ratio,
        "is_hallucinated": hallucination_ratio > 0.3
    }
```

### 2.3 事实一致性验证方法

#### 方法1：基于问答的验证（Question-Answer based）

```
原理：如果模型真的"知道"某个事实，它应该能回答关于该事实的具体问题

步骤：
1. 从生成文本中提取关键事实声明
2. 针对每个声明生成验证性问题
3. 让模型回答验证性问题
4. 对比原始声明与验证答案的一致性

示例：
原始回答："爱因斯坦于1921年获得诺贝尔物理学奖"
验证问题："爱因斯坦哪年获得诺贝尔奖？"
验证答案："1921年" ✅ 一致
```

#### 方法2：多采样自洽性（Self-Consistency）

```
原理：对同一问题多次采样，如果模型在"说真话"，多次回答应该一致

步骤：
1. 使用高温度参数对同一问题采样N次
2. 对比N个回答的语义相似度
3. 低一致性 = 高幻觉风险

阈值参考：
- 一致性 > 0.9：低风险
- 一致性 0.7-0.9：中风险
- 一致性 < 0.7：高风险
```

#### 方法3：外部知识检索验证

```
原理：将模型声明与权威外部知识源交叉验证

步骤：
1. 从回答中提取可验证实体（人名、地名、日期、数字）
2. 使用搜索引擎/知识库API查询验证
3. 标记无法验证或验证失败的声明

验证源优先级：
1. 结构化知识库（Wikidata、企业知识图谱）
2. 权威文档（法规、标准、官方公告）
3. 搜索引擎（Google、Bing）
```

### 2.4 幻觉检测指标体系

| 指标名称 | 计算方法 | 风险阈值 | 业务含义 |
|---------|---------|---------|---------|
| **幻觉率(Hallucination Rate)** | 幻觉样本数 / 总样本数 | > 10% | 整体可信度 |
| **事实准确率(Factual Accuracy)** | 正确事实数 / 总事实数 | < 90% | 事实可靠性 |
| **自洽性分数(Consistency Score)** | 回答间语义相似度 | < 0.8 | 输出稳定性 |
| **引用准确率(Citation Accuracy)** | 真实引用数 / 总引用数 | < 95% | 来源可信度 |
| **置信度校准误差(Calibration Error)** | 置信度与实际准确率差异 | > 0.1 | 过度自信程度 |

---

## 🧪 3. 实验验证任务

请运行本目录下的 `test_day09.py`，观察以下关键输出：

### 3.1 声明抽取与NLI检测验证
- 从回答中抽取可验证声明
- NLI模型判断蕴含/矛盾/中立关系
- 幻觉声明标记与风险评级

### 3.2 多采样自洽性验证
- 同一问题多次采样输出
- 语义相似度计算
- 一致性风险等级判定

### 3.3 事实边界探测验证
- 知识边界内问题（应正确）
- 知识边界外问题（易幻觉）
- 模糊地带问题（需谨慎）

### 3.4 幻觉风险综合评估
- 多维度幻觉指标计算
- 风险等级矩阵
- 检测策略有效性对比

---

## 📝 4. 产出要求

将运行结果贴回给 Trae，让其生成 `report_day09.md` 质量分析报告。

报告应包含：
1. **幻觉检测结果**：幻觉率、事实准确率、高风险声明清单
2. **一致性分析报告**：自洽性分数、矛盾回答示例
3. **风险评级矩阵**：按业务场景分类的幻觉风险等级
4. **检测策略评估**：各检测方法的有效性对比
5. **改进建议**：降低幻觉风险的工程实践建议
