"""
è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬ï¼šDay 04 - å¤šè¯­è¨€æ··åˆä¸å¤æ‚ä¸Šä¸‹æ–‡æµ‹è¯•
ç›®æ ‡ï¼šéªŒè¯å¤šè¯­è¨€æ··åˆåœºæ™¯ä¸‹çš„ç¼–ç è¾¹ç•Œã€tokenæ¶ˆè€—ã€è¯­ä¹‰å…³è”
"""

import os
import pytest
import re
from typing import List, Dict, Tuple
from dataclasses import dataclass
from datetime import datetime


@dataclass
class MultilingualTestResult:
    """å¤šè¯­è¨€æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    text: str
    languages: List[str]
    char_count: int
    token_count: int
    expected_tokens: int
    penalty_ratio: float


@dataclass
class BoundaryTestResult:
    """è¾¹ç•Œæµ‹è¯•ç»“æœæ•°æ®ç±»"""
    boundary_type: str
    text_before: str
    text_after: str
    encoding_success: bool
    token_count: int


class MultilingualTester:
    """å¤šè¯­è¨€æ··åˆæµ‹è¯•å™¨"""
    
    def __init__(self):
        self._encoder = None
    
    @property
    def encoder(self):
        """å»¶è¿Ÿåˆå§‹åŒ–tiktokenç¼–ç å™¨"""
        if self._encoder is None:
            try:
                import tiktoken
                self._encoder = tiktoken.get_encoding("cl100k_base")
            except ImportError:
                print("âš ï¸ tiktokenæœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                self._encoder = None
        return self._encoder
    
    def count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬tokenæ•°é‡"""
        if self.encoder is None:
            return self._estimate_tokens(text)
        try:
            return len(self.encoder.encode(text))
        except:
            return self._estimate_tokens(text)
    
    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—tokenæ•°é‡ï¼ˆæ¨¡æ‹Ÿæ¨¡å¼ï¼‰"""
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        other_chars = len(text) - chinese_chars
        return max(1, int(chinese_chars / 1.5 + other_chars / 4))
    
    def detect_languages(self, text: str) -> List[str]:
        """ç®€å•è¯­è¨€æ£€æµ‹"""
        languages = []
        
        if re.search(r'[a-zA-Z]', text):
            languages.append("English")
        if re.search(r'[\u4e00-\u9fff]', text):
            languages.append("Chinese")
        if re.search(r'[\u3040-\u309f\u30a0-\u30ff]', text):
            languages.append("Japanese")
        if re.search(r'[\uac00-\ud7af]', text):
            languages.append("Korean")
        if re.search(r'[\u0400-\u04ff]', text):
            languages.append("Cyrillic")
        
        return languages if languages else ["Unknown"]


# ============ æµ‹è¯•ç”¨ä¾‹ ============

class TestDay04Multilingual:
    """Day 04: å¤šè¯­è¨€æ··åˆæµ‹è¯•ç±»"""
    
    @pytest.fixture(scope="class")
    def tester(self):
        """æµ‹è¯•å™¨fixture"""
        return MultilingualTester()
    
    def test_multilingual_token_penalty(self, tester):
        """
        æµ‹è¯•å¤šè¯­è¨€æ··åˆçš„Tokenæƒ©ç½šæ•ˆåº”
        
        é£é™©ç‚¹ï¼šå¤šè¯­è¨€æ··åˆæ—¶tokenæ¶ˆè€—é«˜äºå•ä¸€è¯­è¨€ä¹‹å’Œ
        """
        print("\n" + "="*60)
        print("ğŸ”¬ æµ‹è¯•ç”¨ä¾‹ 1: å¤šè¯­è¨€æ··åˆTokenæƒ©ç½šæ•ˆåº”")
        print("="*60)
        
        # æµ‹è¯•ç”¨ä¾‹ï¼š(çº¯è‹±æ–‡, çº¯ä¸­æ–‡, æ··åˆ)
        test_cases = [
            ("Hello World", "çº¯è‹±æ–‡"),
            ("ä½ å¥½ä¸–ç•Œ", "çº¯ä¸­æ–‡"),
            ("Helloä½ å¥½Worldä¸–ç•Œ", "ä¸­è‹±æ··åˆ"),
            ("AIäººå·¥æ™ºèƒ½", "è‹±ä¸­æ··åˆ"),
            ("Productäº§å“Nameåç§°", "è‹±ä¸­äº¤æ›¿"),
        ]
        
        print(f"\n{'æ–‡æœ¬':<25} {'ç±»å‹':<12} {'å­—ç¬¦':<6} {'Token':<6} {'è¯­è¨€':<15}")
        print("-" * 65)
        
        results = []
        for text, label in test_cases:
            char_count = len(text)
            token_count = tester.count_tokens(text)
            languages = tester.detect_languages(text)
            
            print(f"{text:<25} {label:<12} {char_count:<6} {token_count:<6} {','.join(languages):<15}")
            results.append((text, label, char_count, token_count, languages))
        
        # è®¡ç®—æ··åˆæƒ©ç½š
        en_tokens = results[0][3]  # Hello World
        cn_tokens = results[1][3]  # ä½ å¥½ä¸–ç•Œ
        mixed_tokens = results[2][3]  # Helloä½ å¥½Worldä¸–ç•Œ
        
        expected = en_tokens + cn_tokens
        penalty = (mixed_tokens - expected) / expected * 100 if expected > 0 else 0
        
        print(f"\nğŸ“Š æ··åˆæƒ©ç½šåˆ†æ:")
        print(f"   çº¯è‹±æ–‡Token: {en_tokens}")
        print(f"   çº¯ä¸­æ–‡Token: {cn_tokens}")
        print(f"   é¢„æœŸæ··åˆToken: {expected}")
        print(f"   å®é™…æ··åˆToken: {mixed_tokens}")
        print(f"   æ··åˆæƒ©ç½š: {penalty:+.1f}%")
        
        if penalty > 10:
            print(f"\nâš ï¸  å‘ç°æ˜¾è‘—æ··åˆæƒ©ç½š(>{10}%)ï¼")
        
        print("âœ… å¤šè¯­è¨€æ··åˆTokenæƒ©ç½šæµ‹è¯•å®Œæˆ")
    
    def test_language_switching_boundaries(self, tester):
        """
        æµ‹è¯•è¯­è¨€åˆ‡æ¢è¾¹ç•Œå¤„ç†
        
        é£é™©ç‚¹ï¼šè¯­è¨€åˆ‡æ¢ç‚¹å¯èƒ½å‡ºç°ç¼–ç é”™è¯¯æˆ–åˆ†è¯é”™è¯¯
        """
        print("\n" + "="*60)
        print("ğŸ”¬ æµ‹è¯•ç”¨ä¾‹ 2: è¯­è¨€åˆ‡æ¢è¾¹ç•Œå¤„ç†")
        print("="*60)
        
        boundary_cases = [
            ("Helloä½ å¥½", "è‹±æ–‡â†’ä¸­æ–‡"),
            ("ä½ å¥½Hello", "ä¸­æ–‡â†’è‹±æ–‡"),
            ("Helloã“ã‚“ã«ã¡ã¯", "è‹±æ–‡â†’æ—¥æ–‡"),
            ("ì•ˆë…•í•˜ì„¸ìš”Hello", "éŸ©æ–‡â†’è‹±æ–‡"),
            ("Helloä½ å¥½ã“ã‚“ã«ã¡ã¯", "è‹±â†’ä¸­â†’æ—¥"),
            ("AIäººå·¥æ™ºèƒ½MLæœºå™¨å­¦ä¹ ", "æœ¯è¯­æ··åˆ"),
        ]
        
        print(f"\n{'è¾¹ç•Œæ–‡æœ¬':<30} {'åˆ‡æ¢ç±»å‹':<15} {'Tokenæ•°':<8} {'çŠ¶æ€':<10}")
        print("-" * 65)
        
        for text, switch_type in boundary_cases:
            token_count = tester.count_tokens(text)
            languages = tester.detect_languages(text)
            
            # ç®€å•åˆ¤æ–­ï¼šå¦‚æœèƒ½æ£€æµ‹åˆ°å¤šç§è¯­è¨€ï¼Œè®¤ä¸ºè¾¹ç•Œå¤„ç†æˆåŠŸ
            status = "âœ… æ­£å¸¸" if len(languages) >= 2 else "âš ï¸  å¼‚å¸¸"
            
            print(f"{text:<30} {switch_type:<15} {token_count:<8} {status:<10}")
        
        print("\nâš ï¸  é£é™©æé†’ï¼šè¯­è¨€åˆ‡æ¢è¾¹ç•Œæ˜¯ç¼–ç é”™è¯¯çš„æ•æ„ŸåŒºåŸŸï¼")
        print("âœ… è¯­è¨€åˆ‡æ¢è¾¹ç•Œæµ‹è¯•å®Œæˆ")
    
    def test_special_character_interference(self, tester):
        """
        æµ‹è¯•ç‰¹æ®Šå­—ç¬¦ä¸å¤šè¯­è¨€æ··åˆçš„å¹²æ‰°
        
        é£é™©ç‚¹ï¼šemoji/ç¬¦å·ä¸æ–‡å­—ç¼–ç å†²çª
        """
        print("\n" + "="*60)
        print("ğŸ”¬ æµ‹è¯•ç”¨ä¾‹ 3: ç‰¹æ®Šå­—ç¬¦ä¸å¤šè¯­è¨€æ··åˆå¹²æ‰°")
        print("="*60)
        
        special_cases = [
            ("Helloä½ å¥½ğŸŒ", "ä¸­è‹±+Emoji"),
            ("äº§å“ğŸ’¯è´¨é‡âœ…ä¿è¯", "ä¸­æ–‡+ç¬¦å·"),
            ("Price: $100ä»·æ ¼: 100å…ƒ", "è´§å¸ç¬¦å·æ··åˆ"),
            ("Email: test@example.comé‚®ç®±", "é‚®ç®±+ä¸­æ–‡"),
            ("URL: https://example.comé“¾æ¥", "URL+ä¸­æ–‡"),
            ("æ¸©åº¦ğŸŒ¡ï¸25Â°Cæ¸©åº¦", "å•ä½ç¬¦å·æ··åˆ"),
        ]
        
        print(f"\n{'æ–‡æœ¬':<35} {'ç±»å‹':<15} {'å­—ç¬¦':<6} {'Token':<6}")
        print("-" * 65)
        
        for text, label in special_cases:
            char_count = len(text)
            token_count = tester.count_tokens(text)
            print(f"{text:<35} {label:<15} {char_count:<6} {token_count:<6}")
        
        print("\nâš ï¸  é£é™©æé†’ï¼š")
        print("   - Emojiå¯èƒ½æ¶ˆè€—1-3ä¸ªtoken")
        print("   - ç‰¹æ®Šç¬¦å·å¯èƒ½å¯¼è‡´åˆ†è¯é”™è¯¯")
        print("   - URL/é‚®ç®±åœ¨æ··åˆæ–‡æœ¬ä¸­å¯èƒ½è¢«é”™è¯¯åˆ†å‰²")
        
        print("âœ… ç‰¹æ®Šå­—ç¬¦å¹²æ‰°æµ‹è¯•å®Œæˆ")
    
    def test_cross_language_semantic_association(self, tester):
        """
        æµ‹è¯•è·¨è¯­è¨€è¯­ä¹‰å…³è”èƒ½åŠ›
        
        é£é™©ç‚¹ï¼šè·¨è¯­è¨€ä¸Šä¸‹æ–‡å…³è”å¤±è´¥
        """
        print("\n" + "="*60)
        print("ğŸ”¬ æµ‹è¯•ç”¨ä¾‹ 4: è·¨è¯­è¨€è¯­ä¹‰å…³è”èƒ½åŠ›")
        print("="*60)
        
        # æ¨¡æ‹Ÿè·¨è¯­è¨€æŒ‡ä»£åœºæ™¯
        test_scenarios = [
            {
                "context": "The company's flagship product is called æ˜Ÿè¾°å¤§æµ· (Star Ocean)",
                "question": "è¿™å®¶å…¬å¸çš„æ——èˆ°äº§å“å«ä»€ä¹ˆåå­—ï¼Ÿ",
                "expected": "æ˜Ÿè¾°å¤§æµ·",
                "type": "è‹±â†’ä¸­å®ä½“å…³è”"
            },
            {
                "context": "æˆ‘ä»¬çš„æ–°äº§å“AI Assistantå³å°†å‘å¸ƒ",
                "question": "What is the new product name?",
                "expected": "AI Assistant",
                "type": "ä¸­â†’è‹±å®ä½“å…³è”"
            },
            {
                "context": "Project Alphaé¡¹ç›®ç”±å¼ ä¸‰è´Ÿè´£ï¼ŒHe is the tech lead",
                "question": "è°æ˜¯æŠ€æœ¯è´Ÿè´£äººï¼Ÿ",
                "expected": "å¼ ä¸‰",
                "type": "è·¨è¯­è¨€æŒ‡ä»£"
            },
        ]
        
        print(f"\n{'åœºæ™¯ç±»å‹':<15} {'ä¸Šä¸‹æ–‡':<40} {'é¢„æœŸç­”æ¡ˆ':<10}")
        print("-" * 70)
        
        for scenario in test_scenarios:
            context = scenario["context"][:35] + "..." if len(scenario["context"]) > 35 else scenario["context"]
            print(f"{scenario['type']:<15} {context:<40} {scenario['expected']:<10}")
        
        print("\nğŸ“Š è·¨è¯­è¨€å…³è”éš¾åº¦è¯„ä¼°:")
        print("   è‹±â†’ä¸­å®ä½“: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ ä¸­ç­‰")
        print("   ä¸­â†’è‹±å®ä½“: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ ä¸­ç­‰")
        print("   è·¨è¯­è¨€æŒ‡ä»£: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ å›°éš¾")
        
        print("\nâš ï¸  é£é™©æé†’ï¼šè·¨è¯­è¨€å®ä½“å…³è”æ˜¯æ¨¡å‹èƒ½åŠ›çš„è–„å¼±ç¯èŠ‚ï¼")
        print("âœ… è·¨è¯­è¨€è¯­ä¹‰å…³è”æµ‹è¯•å®Œæˆ")
    
    def test_multilingual_density_gradient(self, tester):
        """
        æµ‹è¯•å¤šè¯­è¨€å¯†åº¦æ¢¯åº¦å˜åŒ–
        
        é£é™©ç‚¹ï¼šè¯­è¨€æ¯”ä¾‹å˜åŒ–æ—¶çš„å¤„ç†ç¨³å®šæ€§
        """
        print("\n" + "="*60)
        print("ğŸ”¬ æµ‹è¯•ç”¨ä¾‹ 5: å¤šè¯­è¨€å¯†åº¦æ¢¯åº¦æµ‹è¯•")
        print("="*60)
        
        # æ„é€ ä¸åŒå¯†åº¦çš„æ··åˆæ–‡æœ¬
        base_en = "Hello World this is a test sentence for mixed language analysis"
        base_cn = "è¿™æ˜¯ä¸€ä¸ªç”¨äºæ··åˆè¯­è¨€åˆ†æçš„æµ‹è¯•å¥å­"
        
        gradients = [
            ("10%ä¸­æ–‡", base_en + " " + base_cn[:3]),
            ("30%ä¸­æ–‡", base_en + " " + base_cn[:9]),
            ("50%ä¸­æ–‡", base_en + " " + base_cn),
            ("70%ä¸­æ–‡", base_cn + " " + base_en[:20]),
            ("90%ä¸­æ–‡", base_cn + " " + base_en[:5]),
        ]
        
        print(f"\n{'å¯†åº¦':<10} {'æ–‡æœ¬é¢„è§ˆ':<45} {'Token':<6}")
        print("-" * 65)
        
        for label, text in gradients:
            preview = text[:40] + "..." if len(text) > 40 else text
            token_count = tester.count_tokens(text)
            print(f"{label:<10} {preview:<45} {token_count:<6}")
        
        print("\nğŸ“ˆ å¯†åº¦å˜åŒ–å¯¹å¤„ç†çš„å½±å“:")
        print("   - ä½å¯†åº¦(<30%): æ¨¡å‹å¯èƒ½å¿½ç•¥éä¸»è¦è¯­è¨€")
        print("   - å‡è¡¡å¯†åº¦(50%): è¯­è¨€åˆ‡æ¢é¢‘ç¹ï¼Œè¾¹ç•Œé”™è¯¯é£é™©é«˜")
        print("   - é«˜å¯†åº¦(>70%): ç±»ä¼¼çº¯è¯­è¨€åœºæ™¯ï¼Œç›¸å¯¹ç¨³å®š")
        
        print("âœ… å¤šè¯­è¨€å¯†åº¦æ¢¯åº¦æµ‹è¯•å®Œæˆ")
    
    def test_code_comment_multilingual(self, tester):
        """
        æµ‹è¯•ä»£ç ä¸å¤šè¯­è¨€æ³¨é‡Šæ··åˆ
        
        é£é™©ç‚¹ï¼šä»£ç ä¸æ³¨é‡Šå…³è”å¤±è´¥
        """
        print("\n" + "="*60)
        print("ğŸ”¬ æµ‹è¯•ç”¨ä¾‹ 6: ä»£ç ä¸å¤šè¯­è¨€æ³¨é‡Šæ··åˆ")
        print("="*60)
        
        code_samples = [
            ("def hello(): # è¿™æ˜¯ä¸€ä¸ªå‡½æ•°\n    pass", "Python+ä¸­æ–‡æ³¨é‡Š"),
            ("// è·å–ç”¨æˆ·ä¿¡æ¯\nfunction getUser() {}", "JS+ä¸­æ–‡æ³¨é‡Š"),
            ("/* This function å¤„ç†æ•°æ® */\nvoid process() {}", "C+++æ··åˆæ³¨é‡Š"),
            ("# TODO: ä¿®å¤è¿™ä¸ªbug\nprint('fix me')", "Python+TODO"),
            ("class User:  # ç”¨æˆ·ç±»\n    pass", "Pythonç±»+æ³¨é‡Š"),
        ]
        
        print(f"\n{'ä»£ç é¢„è§ˆ':<40} {'ç±»å‹':<20} {'Token':<6}")
        print("-" * 70)
        
        for code, label in code_samples:
            preview = code.replace('\n', ' ')[:35] + "..."
            token_count = tester.count_tokens(code)
            print(f"{preview:<40} {label:<20} {token_count:<6}")
        
        print("\nâš ï¸  é£é™©æé†’ï¼š")
        print("   - ä»£ç ä¸­çš„å¤šè¯­è¨€æ³¨é‡Šå¯èƒ½è¢«é”™è¯¯è§£æ")
        print("   - æ³¨é‡Šä¸ä»£ç çš„å…³è”å¯èƒ½å› è¯­è¨€åˆ‡æ¢è€Œæ–­è£‚")
        print("   - ç‰¹æ®Šå­—ç¬¦(#, //, /*)ä¸ä¸­æ–‡æ··åˆå¯èƒ½äº§ç”Ÿæ­§ä¹‰")
        
        print("âœ… ä»£ç ä¸å¤šè¯­è¨€æ³¨é‡Šæµ‹è¯•å®Œæˆ")
    
    def test_multilingual_cost_estimation_risk(self, tester):
        """
        æµ‹è¯•å¤šè¯­è¨€æ··åˆåœºæ™¯çš„æˆæœ¬ä¼°ç®—é£é™©
        """
        print("\n" + "="*60)
        print("ğŸ”¬ æµ‹è¯•ç”¨ä¾‹ 7: å¤šè¯­è¨€æ··åˆæˆæœ¬ä¼°ç®—é£é™©")
        print("="*60)
        
        # æ¨¡æ‹Ÿå®¢æœåœºæ™¯
        scenarios = [
            ("çº¯è‹±æ–‡å®¢æœ", "Hello, I need help with my order."),
            ("çº¯ä¸­æ–‡å®¢æœ", "ä½ å¥½ï¼Œæˆ‘éœ€è¦å¸®åŠ©å¤„ç†æˆ‘çš„è®¢å•é—®é¢˜ã€‚"),
            ("ä¸­è‹±æ··åˆå®¢æœ", "Helloä½ å¥½ï¼ŒI need helpæˆ‘éœ€è¦å¸®åŠ©ã€‚"),
            ("ä¸­è‹±å¤¹æ‚å®¢æœ", "ä½ å¥½ï¼Œæˆ‘æƒ³checkä¸€ä¸‹æˆ‘çš„orderçŠ¶æ€ã€‚"),
            ("å¤šè¯­è¨€å®¢æœ", "Helloä½ å¥½ã“ã‚“ã«ã¡ã¯ï¼Œéœ€è¦å¸®åŠ©ë„ì›€ì´ í•„ìš”í•©ë‹ˆë‹¤ã€‚"),
        ]
        
        print(f"\n{'åœºæ™¯':<16} {'æ–‡æœ¬é¢„è§ˆ':<35} {'Token':<6} {'é£é™©':<10}")
        print("-" * 70)
        
        for label, text in scenarios:
            preview = text[:30] + "..." if len(text) > 30 else text
            token_count = tester.count_tokens(text)
            
            # ç®€å•é£é™©è¯„ä¼°
            if "å¤šè¯­è¨€" in label:
                risk = "ğŸ”´ é«˜"
            elif "æ··åˆ" in label or "å¤¹æ‚" in label:
                risk = "ğŸŸ¡ ä¸­"
            else:
                risk = "ğŸŸ¢ ä½"
            
            print(f"{label:<16} {preview:<35} {token_count:<6} {risk:<10}")
        
        print("\nâš ï¸  é£é™©æé†’ï¼š")
        print("   - å¤šè¯­è¨€æ··åˆåœºæ™¯æˆæœ¬éš¾ä»¥é¢„ä¼°")
        print("   - å®é™…æ¶ˆè€—å¯èƒ½æ¯”é¢„æœŸé«˜20-50%")
        print("   - é¢„ç®—è§„åˆ’å¿…é¡»è€ƒè™‘æ··åˆæƒ©ç½š")
        
        print("âœ… å¤šè¯­è¨€æ··åˆæˆæœ¬ä¼°ç®—é£é™©æµ‹è¯•å®Œæˆ")
    
    def test_multilingual_summary_report(self, tester):
        """
        ç”Ÿæˆå¤šè¯­è¨€æ··åˆæµ‹è¯•æ±‡æ€»æŠ¥å‘Š
        """
        print("\n" + "="*60)
        print("ğŸ“‹ å¤šè¯­è¨€æ··åˆæµ‹è¯•æ±‡æ€»æŠ¥å‘Š")
        print("="*60)
        
        print("\nğŸ”´ é«˜é£é™©é¡¹:")
        print("   1. æ··åˆæƒ©ç½šï¼šå¤šè¯­è¨€æ··åˆæ—¶tokenæ¶ˆè€—å¢åŠ 10-30%")
        print("   2. è·¨è¯­è¨€å…³è”ï¼šå®ä½“æŒ‡ä»£å’Œè¯­ä¹‰å…³è”èƒ½åŠ›ä¸‹é™")
        print("   3. æˆæœ¬å¤±æ§ï¼šå¤šè¯­è¨€åœºæ™¯æˆæœ¬éš¾ä»¥å‡†ç¡®é¢„ä¼°")
        
        print("\nğŸŸ¡ ä¸­é£é™©é¡¹:")
        print("   1. è¾¹ç•Œé”™è¯¯ï¼šè¯­è¨€åˆ‡æ¢ç‚¹å¯èƒ½å‡ºç°ç¼–ç é—®é¢˜")
        print("   2. ç‰¹æ®Šå­—ç¬¦å†²çªï¼šemoji/ç¬¦å·ä¸æ–‡å­—æ··åˆé£é™©")
        print("   3. å¯†åº¦æ•æ„Ÿï¼šè¯­è¨€æ¯”ä¾‹å˜åŒ–æ—¶å¤„ç†ä¸ç¨³å®š")
        
        print("\nâœ… æµ‹è¯•å»ºè®®:")
        print("   1. ç”Ÿäº§ç¯å¢ƒå¤šè¯­è¨€è¾“å…¥å¿…é¡»å•ç‹¬è¿›è¡Œtokenè®¡æ•°")
        print("   2. å…³é”®å®ä½“åº”åœ¨åŒä¸€è¯­è¨€ä¸Šä¸‹æ–‡ä¸­å®šä¹‰")
        print("   3. æ··åˆåœºæ™¯é¢„ç®—éœ€é¢„ç•™20-50%ç¼“å†²")
        print("   4. è¯­è¨€åˆ‡æ¢é¢‘ç¹çš„åœºæ™¯å»ºè®®åˆ†æ®µå¤„ç†")
        print("   5. ä»£ç æ³¨é‡Šåº”å°½é‡ä½¿ç”¨å•ä¸€è¯­è¨€")
        
        print("\nğŸ“Š å¤šè¯­è¨€å¤„ç†ä¼˜å…ˆçº§:")
        print("   æ¨è: å•ä¸€è¯­è¨€ > ä½å¯†åº¦æ··åˆ > å‡è¡¡æ··åˆ > é«˜å¯†åº¦æ··åˆ")
        
        print("\n" + "="*60)
        print("âœ… Day 04 å…¨éƒ¨æµ‹è¯•æ‰§è¡Œå®Œæ¯•")
        print("ğŸ“¤ è¯·å°†ä¸Šæ–¹æ—¥å¿—å‘ç»™ Trae ç”Ÿæˆ report_day04.md è´¨é‡åˆ†ææŠ¥å‘Š")
        print("="*60)


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
