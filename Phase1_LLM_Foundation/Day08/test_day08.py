"""
è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬ï¼šDay 08 - A/Bæµ‹è¯•å®éªŒè®¾è®¡
ç›®æ ‡ï¼šæ ·æœ¬é‡è®¡ç®—ã€éšæœºåŒ–åˆ†ç»„ã€ç»Ÿè®¡æ£€éªŒã€å®éªŒå¥åº·åº¦ç›‘æ§
é£é™©è§†è§’ï¼šä¸“æ³¨A/Bæµ‹è¯•å®éªŒè®¾è®¡ç¼ºé™·å’Œç»Ÿè®¡é”™è¯¯é£é™©
"""

import os
import pytest
import json
import random
import hashlib
import math
import statistics
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from collections import defaultdict
import numpy as np


class MetricType(Enum):
    """æŒ‡æ ‡ç±»å‹"""
    CONTINUOUS = "continuous"  # è¿ç»­æŒ‡æ ‡ï¼šå»¶è¿Ÿã€å‡†ç¡®ç‡
    PROPORTION = "proportion"  # æ¯”ä¾‹æŒ‡æ ‡ï¼šè½¬åŒ–ç‡ã€é€šè¿‡ç‡


class TestResult(Enum):
    """æ£€éªŒç»“æœ"""
    SIGNIFICANT = "æ˜¾è‘—"  # æ‹’ç»H0
    NOT_SIGNIFICANT = "ä¸æ˜¾è‘—"  # æ— æ³•æ‹’ç»H0
    INSUFFICIENT = "æ ·æœ¬ä¸è¶³"  # æœªè¾¾åˆ°æœ€å°æ ·æœ¬é‡


@dataclass
class SampleSizeResult:
    """æ ·æœ¬é‡è®¡ç®—ç»“æœ"""
    metric_name: str
    metric_type: MetricType
    baseline_value: float
    mde: float
    alpha: float
    power: float
    sample_size_per_group: int
    total_sample_size: int
    estimated_days: float


@dataclass
class User:
    """ç”¨æˆ·æ•°æ®ç±»"""
    user_id: str
    attributes: Dict[str, any]  # ç”¨æˆ·å±æ€§ï¼ˆå¹´é¾„ã€åœ°åŸŸç­‰ï¼‰
    

@dataclass
class ExperimentGroup:
    """å®éªŒç»„"""
    name: str
    users: List[User]
    metrics: Dict[str, List[float]] = field(default_factory=dict)


@dataclass
class StatisticalTestResult:
    """ç»Ÿè®¡æ£€éªŒç»“æœ"""
    metric_name: str
    control_mean: float
    treatment_mean: float
    difference: float
    relative_change: float
    p_value: float
    confidence_interval: Tuple[float, float]
    is_significant: bool
    effect_size: float  # Cohen's d
    sample_size_control: int
    sample_size_treatment: int


class SampleSizeCalculator:
    """æ ·æœ¬é‡è®¡ç®—å™¨"""
    
    @staticmethod
    def calculate_continuous(baseline_mean: float,
                            baseline_std: float,
                            mde_absolute: float,
                            alpha: float = 0.05,
                            power: float = 0.8) -> int:
        """
        è¿ç»­æŒ‡æ ‡æ ·æœ¬é‡è®¡ç®—
        
        å…¬å¼: n = 2 * (Z_(1-Î±/2) + Z_power)Â² * ÏƒÂ² / MDEÂ²
        
        Args:
            baseline_mean: å¯¹ç…§ç»„å‡å€¼
            baseline_std: å¯¹ç…§ç»„æ ‡å‡†å·®
            mde_absolute: ç»å¯¹MDEï¼ˆæœ€å°å¯æ£€æµ‹æ•ˆåº”ï¼‰
            alpha: æ˜¾è‘—æ€§æ°´å¹³
            power: ç»Ÿè®¡åŠŸæ•ˆ
        
        Returns:
            æ¯ç»„æ‰€éœ€æ ·æœ¬é‡
        """
        # Zå€¼è®¡ç®—ï¼ˆåŒä¾§æ£€éªŒï¼‰
        z_alpha = 1.96 if alpha == 0.05 else 2.576  # 95%æˆ–99%ç½®ä¿¡åº¦
        z_beta = 0.84 if power == 0.8 else 1.28  # 80%æˆ–90%åŠŸæ•ˆ
        
        # åˆå¹¶æ–¹å·®ï¼ˆå‡è®¾ä¸¤ç»„æ–¹å·®ç›¸ç­‰ï¼‰
        pooled_variance = 2 * (baseline_std ** 2)
        
        # æ ·æœ¬é‡è®¡ç®—
        n = ((z_alpha + z_beta) ** 2 * pooled_variance) / (mde_absolute ** 2)
        
        # è€ƒè™‘20%æµå¤±ç‡
        n_with_buffer = n / 0.8
        
        return int(math.ceil(n_with_buffer))
    
    @staticmethod
    def calculate_proportion(baseline_rate: float,
                            mde_relative: float,
                            alpha: float = 0.05,
                            power: float = 0.8) -> int:
        """
        æ¯”ä¾‹æŒ‡æ ‡æ ·æœ¬é‡è®¡ç®—
        
        Args:
            baseline_rate: å¯¹ç…§ç»„è½¬åŒ–ç‡ï¼ˆå¦‚0.15è¡¨ç¤º15%ï¼‰
            mde_relative: ç›¸å¯¹MDEï¼ˆå¦‚0.1è¡¨ç¤ºæå‡10%ï¼‰
            alpha: æ˜¾è‘—æ€§æ°´å¹³
            power: ç»Ÿè®¡åŠŸæ•ˆ
        
        Returns:
            æ¯ç»„æ‰€éœ€æ ·æœ¬é‡
        """
        z_alpha = 1.96 if alpha == 0.05 else 2.576
        z_beta = 0.84 if power == 0.8 else 1.28
        
        p1 = baseline_rate
        p2 = baseline_rate * (1 + mde_relative)
        
        # åˆå¹¶æ¯”ä¾‹
        p_pooled = (p1 + p2) / 2
        
        # æ ·æœ¬é‡è®¡ç®—
        numerator = (z_alpha * math.sqrt(2 * p_pooled * (1 - p_pooled)) + 
                    z_beta * math.sqrt(p1 * (1 - p1) + p2 * (1 - p2))) ** 2
        denominator = (p1 - p2) ** 2
        
        n = numerator / denominator if denominator > 0 else float('inf')
        
        # è€ƒè™‘20%æµå¤±ç‡
        n_with_buffer = n / 0.8
        
        return int(math.ceil(n_with_buffer))
    
    @staticmethod
    def estimate_experiment_duration(sample_size_per_group: int,
                                     daily_traffic: int,
                                     traffic_allocation: float = 0.5) -> float:
        """
        ä¼°ç®—å®éªŒæ‰€éœ€å¤©æ•°
        
        Args:
            sample_size_per_group: æ¯ç»„æ‰€éœ€æ ·æœ¬é‡
            daily_traffic: æ—¥æ´»ç”¨æˆ·æ•°
            traffic_allocation: å®éªŒæµé‡å æ¯”ï¼ˆå¦‚0.5è¡¨ç¤º50%æµé‡å‚ä¸å®éªŒï¼‰
        
        Returns:
            é¢„è®¡å®éªŒå¤©æ•°
        """
        # æ¯å¤©è¿›å…¥å®éªŒçš„ç”¨æˆ·æ•°
        daily_experiment_users = daily_traffic * traffic_allocation
        
        # æ¯ç»„æ¯å¤©çš„ç”¨æˆ·æ•°
        daily_per_group = daily_experiment_users / 2
        
        # æ‰€éœ€å¤©æ•°
        days = sample_size_per_group / daily_per_group
        
        return math.ceil(days)


class RandomizationEngine:
    """éšæœºåŒ–å¼•æ“"""
    
    def __init__(self, salt: str = "ab_test_salt"):
        self.salt = salt
    
    def hash_randomize(self, user_id: str, 
                      num_groups: int = 2) -> int:
        """
        åŸºäºå“ˆå¸Œçš„éšæœºåŒ–
        
        Args:
            user_id: ç”¨æˆ·ID
            num_groups: åˆ†ç»„æ•°ï¼ˆé€šå¸¸ä¸º2ï¼‰
        
        Returns:
            ç»„ç´¢å¼•ï¼ˆ0è¡¨ç¤ºå¯¹ç…§ç»„ï¼Œ1è¡¨ç¤ºå®éªŒç»„ï¼‰
        """
        # ä½¿ç”¨ç”¨æˆ·ID + saltè¿›è¡Œå“ˆå¸Œ
        hash_input = f"{user_id}:{self.salt}"
        hash_value = int(hashlib.md5(hash_input.encode()).hexdigest(), 16)
        
        # å–æ¨¡å¾—åˆ°ç»„ç´¢å¼•
        group_index = hash_value % num_groups
        
        return group_index
    
    def stratified_randomize(self, user: User,
                            strata_vars: List[str],
                            num_groups: int = 2) -> int:
        """
        åˆ†å±‚éšæœºåŒ–
        
        ç¡®ä¿å„å±‚ï¼ˆå¦‚å¹´é¾„ç»„ã€åœ°åŸŸï¼‰å†…ç”¨æˆ·å‡åŒ€åˆ†é…
        
        Args:
            user: ç”¨æˆ·å¯¹è±¡
            strata_vars: åˆ†å±‚å˜é‡åˆ—è¡¨
            num_groups: åˆ†ç»„æ•°
        
        Returns:
            ç»„ç´¢å¼•
        """
        # æ„å»ºåˆ†å±‚æ ‡è¯†
        strata_key = ":".join([str(user.attributes.get(var, "")) for var in strata_vars])
        
        # åœ¨å±‚å†…è¿›è¡Œå“ˆå¸ŒéšæœºåŒ–
        hash_input = f"{user.user_id}:{strata_key}:{self.salt}"
        hash_value = int(hashlib.md5(hash_input.encode()).hexdigest(), 16)
        
        return hash_value % num_groups
    
    def assign_users(self, users: List[User],
                    method: str = "hash",
                    strata_vars: List[str] = None) -> Dict[str, ExperimentGroup]:
        """
        æ‰¹é‡åˆ†é…ç”¨æˆ·åˆ°å®éªŒç»„
        
        Args:
            users: ç”¨æˆ·åˆ—è¡¨
            method: éšæœºåŒ–æ–¹æ³•ï¼ˆhash/stratifiedï¼‰
            strata_vars: åˆ†å±‚å˜é‡ï¼ˆåˆ†å±‚éšæœºåŒ–æ—¶ä½¿ç”¨ï¼‰
        
        Returns:
            å®éªŒç»„å­—å…¸
        """
        groups = {
            "control": ExperimentGroup("control", []),
            "treatment": ExperimentGroup("treatment", [])
        }
        
        for user in users:
            if method == "stratified" and strata_vars:
                group_idx = self.stratified_randomize(user, strata_vars)
            else:
                group_idx = self.hash_randomize(user.user_id)
            
            group_name = "control" if group_idx == 0 else "treatment"
            groups[group_name].users.append(user)
        
        return groups


class StatisticalTester:
    """ç»Ÿè®¡æ£€éªŒå™¨"""
    
    @staticmethod
    def two_sample_t_test(control_values: List[float],
                         treatment_values: List[float],
                         alpha: float = 0.05) -> StatisticalTestResult:
        """
        åŒæ ·æœ¬Tæ£€éªŒ
        
        ç”¨äºè¿ç»­æŒ‡æ ‡ï¼ˆå¦‚å»¶è¿Ÿã€å‡†ç¡®ç‡ï¼‰çš„å·®å¼‚æ£€éªŒ
        
        Args:
            control_values: å¯¹ç…§ç»„æ•°å€¼åˆ—è¡¨
            treatment_values: å®éªŒç»„æ•°å€¼åˆ—è¡¨
            alpha: æ˜¾è‘—æ€§æ°´å¹³
        
        Returns:
            ç»Ÿè®¡æ£€éªŒç»“æœ
        """
        n1 = len(control_values)
        n2 = len(treatment_values)
        
        # è®¡ç®—å‡å€¼
        mean1 = statistics.mean(control_values)
        mean2 = statistics.mean(treatment_values)
        
        # è®¡ç®—æ ‡å‡†å·®
        std1 = statistics.stdev(control_values) if n1 > 1 else 0
        std2 = statistics.stdev(treatment_values) if n2 > 1 else 0
        
        # åˆå¹¶æ ‡å‡†è¯¯
        se = math.sqrt((std1**2 / n1) + (std2**2 / n2))
        
        # Tç»Ÿè®¡é‡
        t_stat = (mean2 - mean1) / se if se > 0 else 0
        
        # è‡ªç”±åº¦ï¼ˆWelch's t-testï¼‰
        df = ((std1**2 / n1 + std2**2 / n2) ** 2) / \
             ((std1**2 / n1) ** 2 / (n1 - 1) + (std2**2 / n2) ** 2 / (n2 - 1)) if se > 0 else n1 + n2 - 2
        
        # på€¼ï¼ˆè¿‘ä¼¼ï¼‰
        # ä½¿ç”¨æ ‡å‡†æ­£æ€åˆ†å¸ƒè¿‘ä¼¼ï¼ˆå¤§æ ·æœ¬æ—¶tåˆ†å¸ƒæ¥è¿‘æ­£æ€ï¼‰
        p_value = 2 * (1 - 0.5 * (1 + math.erf(abs(t_stat) / math.sqrt(2))))
        
        # ç½®ä¿¡åŒºé—´
        z_alpha = 1.96  # 95% CI
        ci_lower = (mean2 - mean1) - z_alpha * se
        ci_upper = (mean2 - mean1) + z_alpha * se
        
        # Cohen's dï¼ˆæ•ˆåº”é‡ï¼‰
        pooled_std = math.sqrt(((n1 - 1) * std1**2 + (n2 - 1) * std2**2) / (n1 + n2 - 2)) if (n1 + n2) > 2 else 1
        cohens_d = (mean2 - mean1) / pooled_std if pooled_std > 0 else 0
        
        return StatisticalTestResult(
            metric_name="continuous",
            control_mean=mean1,
            treatment_mean=mean2,
            difference=mean2 - mean1,
            relative_change=(mean2 - mean1) / mean1 if mean1 != 0 else 0,
            p_value=p_value,
            confidence_interval=(ci_lower, ci_upper),
            is_significant=p_value < alpha,
            effect_size=cohens_d,
            sample_size_control=n1,
            sample_size_treatment=n2
        )
    
    @staticmethod
    def chi_square_test(control_success: int, control_total: int,
                       treatment_success: int, treatment_total: int,
                       alpha: float = 0.05) -> Dict:
        """
        å¡æ–¹æ£€éªŒ
        
        ç”¨äºæ¯”ä¾‹æŒ‡æ ‡ï¼ˆå¦‚è½¬åŒ–ç‡ï¼‰çš„å·®å¼‚æ£€éªŒ
        
        Args:
            control_success: å¯¹ç…§ç»„æˆåŠŸæ•°
            control_total: å¯¹ç…§ç»„æ€»æ•°
            treatment_success: å®éªŒç»„æˆåŠŸæ•°
            treatment_total: å®éªŒç»„æ€»æ•°
            alpha: æ˜¾è‘—æ€§æ°´å¹³
        
        Returns:
            æ£€éªŒç»“æœå­—å…¸
        """
        # æ„å»ºåˆ—è”è¡¨
        control_failure = control_total - control_success
        treatment_failure = treatment_total - treatment_success
        
        # è®¡ç®—æœŸæœ›é¢‘æ•°
        total_success = control_success + treatment_success
        total_failure = control_failure + treatment_failure
        total = control_total + treatment_total
        
        expected_control_success = (control_total * total_success) / total
        expected_treatment_success = (treatment_total * total_success) / total
        expected_control_failure = (control_total * total_failure) / total
        expected_treatment_failure = (treatment_total * total_failure) / total
        
        # å¡æ–¹ç»Ÿè®¡é‡
        def chi_square_cell(observed, expected):
            return ((observed - expected) ** 2) / expected if expected > 0 else 0
        
        chi2 = (chi_square_cell(control_success, expected_control_success) +
                chi_square_cell(treatment_success, expected_treatment_success) +
                chi_square_cell(control_failure, expected_control_failure) +
                chi_square_cell(treatment_failure, expected_treatment_failure))
        
        # på€¼ï¼ˆè‡ªç”±åº¦=1ï¼‰
        p_value = 1 - 0.5 * (1 + math.erf(math.sqrt(chi2 / 2)))
        
        # è½¬åŒ–ç‡
        control_rate = control_success / control_total if control_total > 0 else 0
        treatment_rate = treatment_success / treatment_total if treatment_total > 0 else 0
        
        return {
            "metric_name": "proportion",
            "control_rate": control_rate,
            "treatment_rate": treatment_rate,
            "difference": treatment_rate - control_rate,
            "relative_change": (treatment_rate - control_rate) / control_rate if control_rate > 0 else 0,
            "chi2_statistic": chi2,
            "p_value": p_value,
            "is_significant": p_value < alpha,
            "sample_size_control": control_total,
            "sample_size_treatment": treatment_total
        }
    
    @staticmethod
    def bonferroni_correction(p_values: List[float], alpha: float = 0.05) -> List[bool]:
        """
        Bonferroniæ ¡æ­£
        
        å¤šé‡æ¯”è¾ƒæ ¡æ­£ï¼Œé™ä½å‡é˜³æ€§ç‡
        
        Args:
            p_values: på€¼åˆ—è¡¨
            alpha: åŸå§‹æ˜¾è‘—æ€§æ°´å¹³
        
        Returns:
            æ ¡æ­£åçš„æ˜¾è‘—æ€§åˆ¤æ–­åˆ—è¡¨
        """
        m = len(p_values)
        corrected_alpha = alpha / m  # Bonferroniæ ¡æ­£
        
        return [p < corrected_alpha for p in p_values]


class ExperimentHealthMonitor:
    """å®éªŒå¥åº·åº¦ç›‘æ§å™¨"""
    
    @staticmethod
    def check_srm(control_size: int, treatment_size: int,
                 expected_ratio: float = 0.5,
                 alpha: float = 0.01) -> Dict:
        """
        SRM (Sample Ratio Mismatch) æ£€æµ‹
        
        æ£€æµ‹åˆ†ç»„æ¯”ä¾‹æ˜¯å¦åç¦»é¢„æœŸï¼Œå‘ç°éšæœºåŒ–é—®é¢˜
        
        Args:
            control_size: å¯¹ç…§ç»„æ ·æœ¬é‡
            treatment_size: å®éªŒç»„æ ·æœ¬é‡
            expected_ratio: é¢„æœŸå¯¹ç…§ç»„æ¯”ä¾‹
            alpha: æ˜¾è‘—æ€§æ°´å¹³
        
        Returns:
            SRMæ£€æµ‹ç»“æœ
        """
        total = control_size + treatment_size
        observed_ratio = control_size / total if total > 0 else 0
        
        # å¡æ–¹æ£€éªŒæ£€æµ‹æ¯”ä¾‹åç¦»
        expected_control = total * expected_ratio
        expected_treatment = total * (1 - expected_ratio)
        
        chi2 = ((control_size - expected_control) ** 2 / expected_control +
                (treatment_size - expected_treatment) ** 2 / expected_treatment)
        
        # på€¼
        p_value = 1 - 0.5 * (1 + math.erf(math.sqrt(chi2 / 2)))
        
        is_srm = p_value < alpha
        
        return {
            "is_srm": is_srm,
            "p_value": p_value,
            "observed_ratio": observed_ratio,
            "expected_ratio": expected_ratio,
            "control_size": control_size,
            "treatment_size": treatment_size,
            "severity": "HIGH" if is_srm else "OK"
        }
    
    @staticmethod
    def check_guardrail_metrics(control_metrics: Dict[str, float],
                               treatment_metrics: Dict[str, float],
                               thresholds: Dict[str, float]) -> List[Dict]:
        """
        æŠ¤æ æŒ‡æ ‡æ£€æŸ¥
        
        æ£€æŸ¥å¿…é¡»ä¿æŠ¤çš„æŒ‡æ ‡æ˜¯å¦é€€åŒ–
        
        Args:
            control_metrics: å¯¹ç…§ç»„æŒ‡æ ‡
            treatment_metrics: å®éªŒç»„æŒ‡æ ‡
            thresholds: å„æŒ‡æ ‡çš„é€€åŒ–é˜ˆå€¼
        
        Returns:
            æŠ¤æ æ£€æŸ¥ç»“æœåˆ—è¡¨
        """
        alerts = []
        
        for metric_name, threshold in thresholds.items():
            control_val = control_metrics.get(metric_name, 0)
            treatment_val = treatment_metrics.get(metric_name, 0)
            
            # è®¡ç®—å˜åŒ–
            if control_val > 0:
                change_pct = (treatment_val - control_val) / control_val
            else:
                change_pct = 0
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é˜ˆå€¼
            is_violation = change_pct < -threshold  # è´Ÿå‘å˜åŒ–è¶…è¿‡é˜ˆå€¼
            
            alerts.append({
                "metric_name": metric_name,
                "control_value": control_val,
                "treatment_value": treatment_val,
                "change_pct": change_pct,
                "threshold": threshold,
                "is_violation": is_violation,
                "severity": "CRITICAL" if is_violation else "OK"
            })
        
        return alerts
    
    @staticmethod
    def should_stop_early(current_sample_size: int,
                         min_sample_size: int,
                         guardrail_violations: List[Dict]) -> Tuple[bool, str]:
        """
        æ˜¯å¦åº”è¯¥æå‰åœæ­¢å®éªŒ
        
        Args:
            current_sample_size: å½“å‰æ ·æœ¬é‡
            min_sample_size: æœ€å°æ ·æœ¬é‡
            guardrail_violations: æŠ¤æ è¿è§„åˆ—è¡¨
        
        Returns:
            (æ˜¯å¦åº”è¯¥åœæ­¢, åŸå› )
        """
        # æŠ¤æ è¿è§„å¿…é¡»åœæ­¢
        critical_violations = [v for v in guardrail_violations if v["severity"] == "CRITICAL"]
        if critical_violations:
            return True, f"æŠ¤æ æŒ‡æ ‡ä¸¥é‡è¿è§„: {', '.join([v['metric_name'] for v in critical_violations])}"
        
        # æœªè¾¾åˆ°æœ€å°æ ·æœ¬é‡ä¸èƒ½åœæ­¢
        if current_sample_size < min_sample_size:
            return False, f"æ ·æœ¬é‡ä¸è¶³ ({current_sample_size}/{min_sample_size})"
        
        return False, "å®éªŒæ­£å¸¸è¿›è¡Œä¸­"


class ABTestExperiment:
    """A/Bæµ‹è¯•å®éªŒä¸»ç±»"""
    
    def __init__(self, experiment_name: str):
        self.experiment_name = experiment_name
        self.sample_calculator = SampleSizeCalculator()
        self.randomizer = RandomizationEngine()
        self.tester = StatisticalTester()
        self.monitor = ExperimentHealthMonitor()
        
        self.groups: Dict[str, ExperimentGroup] = {}
        self.results: Dict[str, any] = {}
    
    def design_experiment(self, metrics_config: List[Dict],
                         daily_traffic: int = 10000) -> Dict:
        """
        è®¾è®¡å®éªŒ - è®¡ç®—æ ·æœ¬é‡å’Œæ—¶é•¿
        
        Args:
            metrics_config: æŒ‡æ ‡é…ç½®åˆ—è¡¨
            daily_traffic: æ—¥æ´»ç”¨æˆ·æ•°
        
        Returns:
            å®éªŒè®¾è®¡æ–¹æ¡ˆ
        """
        print(f"\nğŸ“ å®éªŒè®¾è®¡: {self.experiment_name}")
        print("=" * 60)
        
        sample_size_results = []
        max_sample_size = 0
        
        for config in metrics_config:
            metric_name = config["name"]
            metric_type = config["type"]
            
            if metric_type == MetricType.CONTINUOUS:
                n = self.sample_calculator.calculate_continuous(
                    baseline_mean=config["baseline_mean"],
                    baseline_std=config["baseline_std"],
                    mde_absolute=config["mde"]
                )
            else:  # PROPORTION
                n = self.sample_calculator.calculate_proportion(
                    baseline_rate=config["baseline_rate"],
                    mde_relative=config["mde"]
                )
            
            days = self.sample_calculator.estimate_experiment_duration(
                sample_size_per_group=n,
                daily_traffic=daily_traffic
            )
            
            result = SampleSizeResult(
                metric_name=metric_name,
                metric_type=metric_type,
                baseline_value=config.get("baseline_mean") or config.get("baseline_rate"),
                mde=config["mde"],
                alpha=0.05,
                power=0.8,
                sample_size_per_group=n,
                total_sample_size=n * 2,
                estimated_days=days
            )
            
            sample_size_results.append(result)
            max_sample_size = max(max_sample_size, n)
            
            print(f"\n   ğŸ“Š {metric_name}")
            print(f"      ç±»å‹: {metric_type.value}")
            print(f"      åŸºçº¿: {result.baseline_value}")
            print(f"      MDE: {result.mde}")
            print(f"      æ¯ç»„æ ·æœ¬é‡: {n}")
            print(f"      é¢„è®¡å¤©æ•°: {days}å¤©")
        
        # å–æœ€å¤§æ ·æœ¬é‡ä½œä¸ºå®éªŒè¦æ±‚
        total_days = self.sample_calculator.estimate_experiment_duration(
            max_sample_size, daily_traffic
        )
        
        design = {
            "experiment_name": self.experiment_name,
            "sample_size_per_group": max_sample_size,
            "total_sample_size": max_sample_size * 2,
            "estimated_days": total_days,
            "daily_traffic": daily_traffic,
            "metrics": [{
                "name": r.metric_name,
                "sample_size": r.sample_size_per_group,
                "days": r.estimated_days
            } for r in sample_size_results]
        }
        
        print(f"\n   ğŸ“‹ å®éªŒæ–¹æ¡ˆæ€»ç»“")
        print(f"      æ¯ç»„æœ€å°æ ·æœ¬é‡: {max_sample_size}")
        print(f"      æ€»æ ·æœ¬é‡: {max_sample_size * 2}")
        print(f"      é¢„è®¡å®éªŒæ—¶é•¿: {total_days}å¤©")
        
        return design
    
    def run_experiment(self, num_users: int = 10000,
                      treatment_effect: Dict[str, float] = None) -> Dict:
        """
        è¿è¡Œæ¨¡æ‹Ÿå®éªŒ
        
        Args:
            num_users: æ¨¡æ‹Ÿç”¨æˆ·æ•°
            treatment_effect: å®éªŒç»„æ•ˆæœï¼ˆå¦‚{"accuracy": 0.05}è¡¨ç¤ºå‡†ç¡®ç‡+5%ï¼‰
        
        Returns:
            å®éªŒç»“æœ
        """
        print(f"\nğŸ§ª è¿è¡Œå®éªŒ: {self.experiment_name}")
        print("=" * 60)
        
        treatment_effect = treatment_effect or {}
        
        # 1. ç”Ÿæˆæ¨¡æ‹Ÿç”¨æˆ·
        users = self._generate_mock_users(num_users)
        print(f"   ç”Ÿæˆç”¨æˆ·: {len(users)}äºº")
        
        # 2. éšæœºåŒ–åˆ†ç»„
        self.groups = self.randomizer.assign_users(users, method="hash")
        control_size = len(self.groups["control"].users)
        treatment_size = len(self.groups["treatment"].users)
        print(f"   åˆ†ç»„ç»“æœ: å¯¹ç…§ç»„{control_size}äºº, å®éªŒç»„{treatment_size}äºº")
        
        # 3. SRMæ£€æµ‹
        srm_result = self.monitor.check_srm(control_size, treatment_size)
        if srm_result["is_srm"]:
            print(f"   âš ï¸  SRMè­¦å‘Š: åˆ†ç»„æ¯”ä¾‹å¼‚å¸¸ (p={srm_result['p_value']:.4f})")
        else:
            print(f"   âœ… SRMæ£€æŸ¥é€šè¿‡")
        
        # 4. æ¨¡æ‹ŸæŒ‡æ ‡æ•°æ®
        self._simulate_metrics(treatment_effect)
        
        # 5. ç»Ÿè®¡æ£€éªŒ
        test_results = self._analyze_results()
        
        # 6. ç”ŸæˆæŠ¥å‘Š
        report = self._generate_report(test_results, srm_result)
        
        return report
    
    def _generate_mock_users(self, num_users: int) -> List[User]:
        """ç”Ÿæˆæ¨¡æ‹Ÿç”¨æˆ·"""
        users = []
        for i in range(num_users):
            user_id = f"user_{i:06d}"
            attributes = {
                "age_group": random.choice(["18-25", "26-35", "36-45", "46+"]),
                "region": random.choice(["north", "south", "east", "west"]),
                "device": random.choice(["ios", "android", "web"])
            }
            users.append(User(user_id, attributes))
        return users
    
    def _simulate_metrics(self, treatment_effect: Dict[str, float]):
        """æ¨¡æ‹ŸæŒ‡æ ‡æ•°æ®"""
        # æ¨¡æ‹Ÿå‡†ç¡®ç‡ï¼ˆè¿ç»­æŒ‡æ ‡ï¼‰
        baseline_accuracy = 0.85
        baseline_std = 0.05
        
        control_accuracy = [random.gauss(baseline_accuracy, baseline_std) 
                           for _ in self.groups["control"].users]
        
        accuracy_effect = treatment_effect.get("accuracy", 0)
        treatment_accuracy = [random.gauss(baseline_accuracy + accuracy_effect, baseline_std) 
                             for _ in self.groups["treatment"].users]
        
        self.groups["control"].metrics["accuracy"] = control_accuracy
        self.groups["treatment"].metrics["accuracy"] = treatment_accuracy
        
        # æ¨¡æ‹Ÿå»¶è¿Ÿï¼ˆè¿ç»­æŒ‡æ ‡ï¼‰
        baseline_latency = 500  # ms
        latency_std = 50
        
        control_latency = [random.gauss(baseline_latency, latency_std) 
                          for _ in self.groups["control"].users]
        
        latency_effect = treatment_effect.get("latency", 0)
        treatment_latency = [random.gauss(baseline_latency + latency_effect, latency_std) 
                            for _ in self.groups["treatment"].users]
        
        self.groups["control"].metrics["latency"] = control_latency
        self.groups["treatment"].metrics["latency"] = treatment_latency
        
        # æ¨¡æ‹Ÿè½¬åŒ–ç‡ï¼ˆæ¯”ä¾‹æŒ‡æ ‡ï¼‰
        baseline_conversion = 0.15
        conversion_effect = treatment_effect.get("conversion", 0)
        
        control_conversions = sum(random.random() < baseline_conversion 
                                 for _ in self.groups["control"].users)
        treatment_conversions = sum(random.random() < (baseline_conversion + conversion_effect) 
                                   for _ in self.groups["treatment"].users)
        
        self.groups["control"].metrics["conversions"] = control_conversions
        self.groups["treatment"].metrics["conversions"] = treatment_conversions
    
    def _analyze_results(self) -> List[Dict]:
        """åˆ†æå®éªŒç»“æœ"""
        results = []
        
        # å‡†ç¡®ç‡Tæ£€éªŒ
        accuracy_result = self.tester.two_sample_t_test(
            self.groups["control"].metrics["accuracy"],
            self.groups["treatment"].metrics["accuracy"]
        )
        accuracy_result.metric_name = "accuracy"
        results.append(accuracy_result)
        
        # å»¶è¿ŸTæ£€éªŒ
        latency_result = self.tester.two_sample_t_test(
            self.groups["control"].metrics["latency"],
            self.groups["treatment"].metrics["latency"]
        )
        latency_result.metric_name = "latency"
        results.append(latency_result)
        
        # è½¬åŒ–ç‡å¡æ–¹æ£€éªŒ
        conversion_result = self.tester.chi_square_test(
            self.groups["control"].metrics["conversions"],
            len(self.groups["control"].users),
            self.groups["treatment"].metrics["conversions"],
            len(self.groups["treatment"].users)
        )
        results.append(conversion_result)
        
        return results
    
    def _generate_report(self, test_results: List, srm_result: Dict) -> Dict:
        """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
        print(f"\nğŸ“Š å®éªŒç»“æœåˆ†æ")
        print("-" * 60)
        
        significant_count = 0
        for result in test_results:
            if hasattr(result, 'metric_name'):
                # Tæ£€éªŒç»“æœ
                is_sig = result.is_significant
                icon = "âœ…" if is_sig else "âŒ"
                direction = "â†‘" if result.difference > 0 else "â†“"
                
                print(f"   {icon} {result.metric_name:15s}: "
                      f"{result.control_mean:.4f} â†’ {result.treatment_mean:.4f} "
                      f"({direction}{result.relative_change*100:+.2f}%) "
                      f"p={result.p_value:.4f}")
                
                if is_sig:
                    significant_count += 1
            else:
                # å¡æ–¹æ£€éªŒç»“æœ
                is_sig = result["is_significant"]
                icon = "âœ…" if is_sig else "âŒ"
                direction = "â†‘" if result["difference"] > 0 else "â†“"
                
                print(f"   {icon} {result['metric_name']:15s}: "
                      f"{result['control_rate']:.2%} â†’ {result['treatment_rate']:.2%} "
                      f"({direction}{result['relative_change']*100:+.2f}%) "
                      f"p={result['p_value']:.4f}")
                
                if is_sig:
                    significant_count += 1
        
        # å¤šé‡æ¯”è¾ƒæ ¡æ­£
        p_values = [r.p_value if hasattr(r, 'p_value') else r["p_value"] for r in test_results]
        corrected_significance = self.tester.bonferroni_correction(p_values)
        
        print(f"\n   ğŸ“‹ ç»Ÿè®¡æ‘˜è¦")
        print(f"      æ£€éªŒæŒ‡æ ‡æ•°: {len(test_results)}")
        print(f"      æ˜¾è‘—æŒ‡æ ‡æ•°(æ ¡æ­£å‰): {significant_count}")
        print(f"      æ˜¾è‘—æŒ‡æ ‡æ•°(Bonferroniæ ¡æ­£å): {sum(corrected_significance)}")
        print(f"      SRMæ£€æµ‹: {'é€šè¿‡' if not srm_result['is_srm'] else 'å¼‚å¸¸'}")
        
        report = {
            "experiment_name": self.experiment_name,
            "test_results": test_results,
            "srm_result": srm_result,
            "significant_count": significant_count,
            "bonferroni_corrected": sum(corrected_significance),
            "recommendation": "å»ºè®®å‘å¸ƒ" if significant_count > 0 and not srm_result["is_srm"] else "éœ€è°¨æ…"
        }
        
        return report


# ============ pytest æµ‹è¯•ç”¨ä¾‹ ============

class TestDay08ABTesting:
    """Day 08: A/Bæµ‹è¯•å®éªŒè®¾è®¡æµ‹è¯•ç±»"""
    
    @pytest.fixture(scope="class")
    def experiment(self):
        """å®éªŒfixture"""
        return ABTestExperiment("llm_model_ab_test")
    
    def test_sample_size_calculation(self):
        """
        æµ‹è¯•æ ·æœ¬é‡è®¡ç®—
        
        é£é™©ç‚¹ï¼šæ ·æœ¬é‡ä¸è¶³å¯¼è‡´ç»Ÿè®¡åŠŸæ•ˆä¸è¶³
        éªŒè¯ï¼šæ ·æœ¬é‡è®¡ç®—å…¬å¼æ­£ç¡®æ€§
        """
        calculator = SampleSizeCalculator()
        
        # è¿ç»­æŒ‡æ ‡æ ·æœ¬é‡
        n_continuous = calculator.calculate_continuous(
            baseline_mean=0.85,
            baseline_std=0.05,
            mde_absolute=0.02
        )
        
        # æ–­è¨€ï¼šæ ·æœ¬é‡åˆç†
        assert n_continuous > 100, "è¿ç»­æŒ‡æ ‡æ ·æœ¬é‡è¿‡å°"
        assert n_continuous < 10000, "è¿ç»­æŒ‡æ ‡æ ·æœ¬é‡è¿‡å¤§"
        
        # æ¯”ä¾‹æŒ‡æ ‡æ ·æœ¬é‡
        n_proportion = calculator.calculate_proportion(
            baseline_rate=0.15,
            mde_relative=0.10
        )
        
        # æ–­è¨€ï¼šæ ·æœ¬é‡åˆç†
        assert n_proportion > 100, "æ¯”ä¾‹æŒ‡æ ‡æ ·æœ¬é‡è¿‡å°"
        assert n_proportion < 50000, "æ¯”ä¾‹æŒ‡æ ‡æ ·æœ¬é‡è¿‡å¤§"
        
        print(f"\nâœ… æ ·æœ¬é‡è®¡ç®—æµ‹è¯•é€šè¿‡: è¿ç»­æŒ‡æ ‡n={n_continuous}, æ¯”ä¾‹æŒ‡æ ‡n={n_proportion}")
    
    def test_randomization_consistency(self):
        """
        æµ‹è¯•éšæœºåŒ–ä¸€è‡´æ€§
        
        é£é™©ç‚¹ï¼šåŒä¸€ç”¨æˆ·å¤šæ¬¡åˆ†é…ç»“æœä¸ä¸€è‡´
        éªŒè¯ï¼šå“ˆå¸ŒéšæœºåŒ–çš„ä¸€è‡´æ€§
        """
        randomizer = RandomizationEngine()
        
        user_id = "user_12345"
        
        # å¤šæ¬¡éšæœºåŒ–åŒä¸€ç”¨æˆ·
        assignments = [randomizer.hash_randomize(user_id) for _ in range(10)]
        
        # æ–­è¨€ï¼šåˆ†é…ç»“æœä¸€è‡´
        assert all(a == assignments[0] for a in assignments), "éšæœºåŒ–ä¸ä¸€è‡´"
        
        # æ–­è¨€ï¼šåˆ†å¸ƒå‡åŒ€
        test_users = [f"user_{i}" for i in range(1000)]
        group_0 = sum(1 for u in test_users if randomizer.hash_randomize(u) == 0)
        group_1 = sum(1 for u in test_users if randomizer.hash_randomize(u) == 1)
        
        ratio = group_0 / (group_0 + group_1)
        assert 0.45 < ratio < 0.55, f"åˆ†ç»„æ¯”ä¾‹ä¸å‡è¡¡: {ratio:.2%}"
        
        print(f"\nâœ… éšæœºåŒ–ä¸€è‡´æ€§æµ‹è¯•é€šè¿‡: åˆ†ç»„æ¯”ä¾‹ {ratio:.1%}")
    
    def test_statistical_test_accuracy(self):
        """
        æµ‹è¯•ç»Ÿè®¡æ£€éªŒå‡†ç¡®æ€§
        
        é£é™©ç‚¹ï¼šç»Ÿè®¡æ£€éªŒæ–¹æ³•é”™è¯¯å¯¼è‡´å‡é˜³æ€§/å‡é˜´æ€§
        éªŒè¯ï¼šTæ£€éªŒå’Œå¡æ–¹æ£€éªŒæ­£ç¡®æ€§
        """
        tester = StatisticalTester()
        
        # æµ‹è¯•Tæ£€éªŒ - æœ‰æ˜æ˜¾å·®å¼‚çš„æ•°æ®
        control = [0.80, 0.82, 0.79, 0.81, 0.83] * 20  # å‡å€¼0.81
        treatment = [0.85, 0.87, 0.86, 0.88, 0.84] * 20  # å‡å€¼0.86
        
        result = tester.two_sample_t_test(control, treatment)
        
        # æ–­è¨€ï¼šæ£€æµ‹åˆ°æ˜¾è‘—å·®å¼‚
        assert result.is_significant, "åº”æ£€æµ‹åˆ°æ˜¾è‘—å·®å¼‚"
        assert result.difference > 0, "å®éªŒç»„åº”ä¼˜äºå¯¹ç…§ç»„"
        
        # æµ‹è¯•å¡æ–¹æ£€éªŒ
        chi2_result = tester.chi_square_test(
            control_success=80, control_total=100,
            treatment_success=90, treatment_total=100
        )
        
        # æ–­è¨€ï¼šè½¬åŒ–ç‡å·®å¼‚è¢«æ£€æµ‹
        assert chi2_result["treatment_rate"] > chi2_result["control_rate"]
        
        print(f"\nâœ… ç»Ÿè®¡æ£€éªŒæµ‹è¯•é€šè¿‡: Tæ£€éªŒp={result.p_value:.4f}, å¡æ–¹æ£€éªŒp={chi2_result['p_value']:.4f}")
    
    def test_srm_detection(self):
        """
        æµ‹è¯•SRMæ£€æµ‹
        
        é£é™©ç‚¹ï¼šåˆ†ç»„æ¯”ä¾‹å¤±è¡¡æœªè¢«å‘ç°
        éªŒè¯ï¼šSRMæ£€æµ‹èƒ½å‘ç°æ¯”ä¾‹å¼‚å¸¸
        """
        monitor = ExperimentHealthMonitor()
        
        # æ­£å¸¸åˆ†ç»„
        normal_result = monitor.check_srm(500, 500)
        assert not normal_result["is_srm"], "æ­£å¸¸åˆ†ç»„ä¸åº”è§¦å‘SRM"
        
        # å¼‚å¸¸åˆ†ç»„ï¼ˆä¸¥é‡å¤±è¡¡ï¼‰
        abnormal_result = monitor.check_srm(700, 300)
        assert abnormal_result["is_srm"], "å¼‚å¸¸åˆ†ç»„åº”è§¦å‘SRM"
        
        print(f"\nâœ… SRMæ£€æµ‹æµ‹è¯•é€šè¿‡: æ­£å¸¸åˆ†ç»„p={normal_result['p_value']:.4f}, å¼‚å¸¸åˆ†ç»„p={abnormal_result['p_value']:.4f}")
    
    def test_full_experiment_workflow(self, experiment):
        """
        æµ‹è¯•å®Œæ•´å®éªŒæµç¨‹
        
        é£é™©ç‚¹ï¼šå®éªŒæµç¨‹å„ç¯èŠ‚é›†æˆå¤±è´¥
        éªŒè¯ï¼šä»è®¾è®¡åˆ°åˆ†æçš„å®Œæ•´æµç¨‹
        """
        # 1. å®éªŒè®¾è®¡
        metrics_config = [
            {
                "name": "accuracy",
                "type": MetricType.CONTINUOUS,
                "baseline_mean": 0.85,
                "baseline_std": 0.05,
                "mde": 0.02
            },
            {
                "name": "conversion",
                "type": MetricType.PROPORTION,
                "baseline_rate": 0.15,
                "mde": 0.10
            }
        ]
        
        design = experiment.design_experiment(metrics_config, daily_traffic=1000)
        
        # æ–­è¨€ï¼šè®¾è®¡æ–¹æ¡ˆå®Œæ•´
        assert "sample_size_per_group" in design
        assert "estimated_days" in design
        
        # 2. è¿è¡Œå®éªŒï¼ˆæ¨¡æ‹Ÿæœ‰æ­£å‘æ•ˆæœï¼‰
        report = experiment.run_experiment(
            num_users=2000,
            treatment_effect={"accuracy": 0.03}  # å‡†ç¡®ç‡+3%
        )
        
        # æ–­è¨€ï¼šå®éªŒç»“æœå®Œæ•´
        assert "test_results" in report
        assert "srm_result" in report
        
        print(f"\nâœ… å®Œæ•´å®éªŒæµç¨‹æµ‹è¯•é€šè¿‡: å®éªŒæ—¶é•¿{design['estimated_days']}å¤©")


# ä¸»æ‰§è¡Œå…¥å£
if __name__ == "__main__":
    print("\n" + "="*70)
    print("ğŸš€ AI QA System Test - Day 08: A/Bæµ‹è¯•å®éªŒè®¾è®¡")
    print("="*70)
    print("\næµ‹è¯•å†…å®¹:")
    print("  1. æ ·æœ¬é‡è®¡ç®—éªŒè¯")
    print("  2. éšæœºåŒ–åˆ†ç»„éªŒè¯")
    print("  3. ç»Ÿè®¡æ£€éªŒéªŒè¯")
    print("  4. å®éªŒå¥åº·åº¦ç›‘æ§")
    print("\n" + "-"*70)
    
    # åˆ›å»ºå®éªŒå¹¶è¿è¡Œå®Œæ•´æµç¨‹
    experiment = ABTestExperiment("llm_model_upgrade_ab_test")
    
    # å®éªŒè®¾è®¡
    metrics_config = [
        {
            "name": "accuracy",
            "type": MetricType.CONTINUOUS,
            "baseline_mean": 0.85,
            "baseline_std": 0.05,
            "mde": 0.02  # æ£€æµ‹2%çš„å‡†ç¡®ç‡æå‡
        },
        {
            "name": "latency",
            "type": MetricType.CONTINUOUS,
            "baseline_mean": 500,
            "baseline_std": 50,
            "mde": 30  # æ£€æµ‹30msçš„å»¶è¿Ÿå˜åŒ–
        },
        {
            "name": "conversion",
            "type": MetricType.PROPORTION,
            "baseline_rate": 0.15,
            "mde": 0.10  # æ£€æµ‹10%çš„ç›¸å¯¹è½¬åŒ–ç‡æå‡
        }
    ]
    
    design = experiment.design_experiment(metrics_config, daily_traffic=5000)
    
    # è¿è¡Œå¤šä¸ªå®éªŒåœºæ™¯
    scenarios = [
        ("æ— æ•ˆæœ", {"accuracy": 0, "latency": 0, "conversion": 0}),
        ("è½»å¾®æ”¹å–„", {"accuracy": 0.02, "latency": -20, "conversion": 0.05}),
        ("æ˜¾è‘—æ”¹å–„", {"accuracy": 0.05, "latency": -50, "conversion": 0.15}),
    ]
    
    print("\n" + "="*70)
    print("ğŸ§ª å¤šåœºæ™¯å®éªŒæ¨¡æ‹Ÿ")
    print("="*70)
    
    for scenario_name, effects in scenarios:
        print(f"\nã€åœºæ™¯: {scenario_name}ã€‘")
        exp = ABTestExperiment(f"ab_test_{scenario_name}")
        report = exp.run_experiment(num_users=2000, treatment_effect=effects)
    
    print("\n" + "="*70)
    print("âœ… æµ‹è¯•æ‰§è¡Œå®Œæ¯•ï¼Œè¯·å°†ä¸Šæ–¹æ—¥å¿—å‘ç»™ Trae ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šã€‚")
    print("="*70)
