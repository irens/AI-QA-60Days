"""
Day 11: è¾“å‡ºé²æ£’æ€§æµ‹è¯• - åŒä¹‰æ”¹å†™ä¸å™ªå£°æ³¨å…¥

æµ‹è¯•ç›®æ ‡ï¼š
1. éªŒè¯æ¨¡å‹å¯¹åŒä¹‰æ”¹å†™çš„ç¨³å®šæ€§
2. æµ‹è¯•æ¨¡å‹å¯¹å™ªå£°è¾“å…¥çš„å®¹é”™èƒ½åŠ›
3. è¯†åˆ«é²æ£’æ€§è„†å¼±ç‚¹

æµ‹è¯•æ¶æ„å¸ˆè§†è§’ï¼š
- å¼€å‘åªç®¡è·‘é€šï¼Œæˆ‘ä»¬è¦æƒ³åŠæ³•æŠŠå®ƒæå´©æºƒ
- å…³æ³¨è¯­ä¹‰ä¸€è‡´æ€§ã€å‡†ç¡®ç‡è¡°å‡ã€å¤±æ•ˆä¸´ç•Œç‚¹
"""

import pytest
import random
import string
from dataclasses import dataclass
from enum import Enum
from typing import List, Dict, Tuple, Optional
from difflib import SequenceMatcher


class RobustnessLevel(Enum):
    """é²æ£’æ€§ç­‰çº§"""
    HIGH = "ğŸŸ¢ HIGH"       # é«˜åº¦é²æ£’
    MEDIUM = "ğŸŸ¡ MEDIUM"   # ä¸­ç­‰é²æ£’
    LOW = "ğŸŸ  LOW"         # ä½é²æ£’æ€§
    CRITICAL = "ğŸ”´ CRITICAL"  # ä¸¥é‡è„†å¼±


class RobustnessTestType(Enum):
    """æµ‹è¯•ç±»å‹"""
    LEXICAL = "è¯æ±‡æ”¹å†™"
    SYNTACTIC = "å¥æ³•æ”¹å†™"
    NOISE_CHAR = "å­—ç¬¦çº§å™ªå£°"
    NOISE_WORD = "è¯çº§å™ªå£°"


@dataclass
class RobustnessResult:
    """é²æ£’æ€§æµ‹è¯•ç»“æœ"""
    test_type: "RobustnessTestType"
    original_prompt: str
    variations: List[str]
    outputs: List[str]
    consistency_scores: List[float]
    accuracy_scores: List[float]
    is_robust: bool
    risk_level: RobustnessLevel
    message: str


class MockLLM:
    """
    æ¨¡æ‹ŸLLM - å…·æœ‰å¯æ§çš„é²æ£’æ€§ç¼ºé™·
    
    è®¾è®¡ç¼ºé™·ï¼š
    1. å¯¹ç‰¹å®šå…³é”®è¯æ•æ„Ÿï¼ˆå¦‚"è¯·"ã€"èƒ½å¦"ï¼‰
    2. å¯¹å¦å®šè¯ä½ç½®æ•æ„Ÿ
    3. å¯¹å™ªå£°æ•æ„Ÿï¼ˆå‡†ç¡®ç‡éšå™ªå£°å¢åŠ è€Œä¸‹é™ï¼‰
    4. å¯¹è¯­åºæ•æ„Ÿ
    """
    
    def __init__(self, seed: int = 42):
        random.seed(seed)
        self.call_count = 0
        
        # çŸ¥è¯†åº“ï¼šé—®é¢˜ -> æ ‡å‡†ç­”æ¡ˆ
        self.knowledge_base = {
            "ä»€ä¹ˆæ˜¯å…‰åˆä½œç”¨": "å…‰åˆä½œç”¨æ˜¯æ¤ç‰©åˆ©ç”¨å…‰èƒ½å°†äºŒæ°§åŒ–ç¢³å’Œæ°´è½¬åŒ–ä¸ºæœ‰æœºç‰©å’Œæ°§æ°”çš„è¿‡ç¨‹ã€‚",
            "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œç ”ç©¶å¦‚ä½•è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½ã€‚",
            "æ°´çš„æ²¸ç‚¹æ˜¯å¤šå°‘": "æ°´çš„æ²¸ç‚¹åœ¨æ ‡å‡†å¤§æ°”å‹ä¸‹æ˜¯100æ‘„æ°åº¦ã€‚",
        }
        
        # åŒä¹‰è¯æ˜ å°„
        self.synonyms = {
            "ä»€ä¹ˆ": "å•¥",
            "æ˜¯": "ä¸º",
            "æ¤ç‰©": "ç»¿æ¤",
            "åˆ©ç”¨": "ä½¿ç”¨",
            "è½¬åŒ–": "è½¬å˜",
            "è¿‡ç¨‹": "æµç¨‹",
            "è®¡ç®—æœº": "ç”µè„‘",
            "ç ”ç©¶": "æ¢ç©¶",
            "æ¨¡æ‹Ÿ": "æ¨¡ä»¿",
            "æ²¸ç‚¹": "æ²¸è…¾æ¸©åº¦",
            "æ ‡å‡†": "è§„èŒƒ",
        }
    
    def generate(self, prompt: str, noise_level: float = 0.0) -> str:
        """
        æ¨¡æ‹Ÿç”Ÿæˆï¼Œå¼•å…¥é²æ£’æ€§é—®é¢˜
        
        Args:
            prompt: è¾“å…¥æç¤º
            noise_level: å™ªå£°æ°´å¹³ï¼ˆ0.0-1.0ï¼‰
        """
        self.call_count += 1
        
        # æ¸…ç†è¾“å…¥
        clean_prompt = prompt.strip().replace("ï¼Ÿ", "").replace("?", "")
        
        # æ¨¡æ‹Ÿå™ªå£°å½±å“ï¼šå™ªå£°è¶Šé«˜ï¼Œè¶Šå¯èƒ½å‡ºé”™
        error_probability = noise_level * 0.8  # å™ªå£°å¯¼è‡´é”™è¯¯çš„æ¦‚ç‡
        
        if random.random() < error_probability:
            # æ¨¡æ‹Ÿå™ªå£°å¯¼è‡´çš„é”™è¯¯è¾“å‡º
            error_responses = [
                "æŠ±æ­‰ï¼Œæˆ‘ä¸å¤ªç†è§£æ‚¨çš„é—®é¢˜ã€‚",
                "è¿™ä¸ªé—®é¢˜æœ‰ç‚¹å¤æ‚ï¼Œæˆ‘éœ€è¦æ›´å¤šä¿¡æ¯ã€‚",
                "è¾“å…¥ä¼¼ä¹æœ‰äº›æ··ä¹±ï¼Œè¯·é‡æ–°è¡¨è¿°ã€‚",
            ]
            return random.choice(error_responses)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¤¼è²Œç”¨è¯­ï¼ˆæ ¼å¼ä¾èµ–ç¼ºé™·ï¼‰
        has_polite = any(word in clean_prompt for word in ["è¯·", "èƒ½å¦", "éº»çƒ¦", "è°¢è°¢"])
        if not has_polite and random.random() < 0.3:
            # 30%æ¦‚ç‡åœ¨ç¼ºå°‘ç¤¼è²Œç”¨è¯­æ—¶è¡¨ç°ä¸ä½³
            return "è¯·ç”¨æ›´ç¤¼è²Œçš„æ–¹å¼æé—®ã€‚"
        
        # æ£€æŸ¥å¦å®šè¯ä½ç½®ï¼ˆé€»è¾‘æ•æ„Ÿç¼ºé™·ï¼‰
        if "ä¸" in clean_prompt:
            # æ¨¡æ‹Ÿå¦å®šè¯ç†è§£é”™è¯¯
            if random.random() < 0.2:
                return "æ˜¯çš„ï¼Œæ‚¨è¯´å¾—å¯¹ã€‚"  # é”™è¯¯åœ°è‚¯å®šäº†å¦å®šå¥
        
        # å°è¯•åŒ¹é…çŸ¥è¯†åº“
        for key, value in self.knowledge_base.items():
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = self._text_similarity(clean_prompt, key)
            if similarity > 0.6:
                # æ¨¡æ‹ŸåŒä¹‰æ”¹å†™æ•æ„Ÿï¼šç›¸ä¼¼åº¦ä¸å¤Ÿé«˜æ—¶ï¼Œå›ç­”è´¨é‡ä¸‹é™
                if similarity < 0.8 and random.random() < 0.4:
                    return "è¿™ä¸ªé—®é¢˜æˆ‘ä¸å¤ªç¡®å®šï¼Œå¯èƒ½æ˜¯å…³äº" + key[:5] + "..."
                return value
        
        # é»˜è®¤å›ç­”
        return "è¿™æ˜¯ä¸€ä¸ªæœ‰è¶£çš„é—®é¢˜ï¼Œä½†æˆ‘éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡æ¥å›ç­”ã€‚"
    
    def _text_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        return SequenceMatcher(None, text1, text2).ratio()
    
    def get_answer_consistency(self, prompt1: str, prompt2: str) -> float:
        """è·å–ä¸¤ä¸ªæç¤ºå¯¹åº”ç­”æ¡ˆçš„ä¸€è‡´æ€§"""
        answer1 = self.generate(prompt1)
        answer2 = self.generate(prompt2)
        return self._text_similarity(answer1, answer2)


class ParaphraseTester:
    """åŒä¹‰æ”¹å†™æµ‹è¯•å™¨"""
    
    def __init__(self, model: MockLLM):
        self.model = model
        
        # åŒä¹‰è¯è¯å…¸
        self.synonyms = {
            "ä»€ä¹ˆ": ["å•¥", "å“ªä¸ª", "ä½•ç§"],
            "æ˜¯": ["ä¸º", "ä¹ƒ", "ç³»"],
            "æ¤ç‰©": ["ç»¿æ¤", "èŠ±è‰", "ä½œç‰©"],
            "åˆ©ç”¨": ["ä½¿ç”¨", "è¿ç”¨", "é‡‡ç”¨"],
            "è½¬åŒ–": ["è½¬å˜", "è½¬æ¢", "å˜æˆ"],
            "è¿‡ç¨‹": ["æµç¨‹", "ç¨‹åº", "æ­¥éª¤"],
            "è®¡ç®—æœº": ["ç”µè„‘", "è®¡ç®—è®¾å¤‡"],
            "ç ”ç©¶": ["æ¢ç©¶", "é’»ç ”", "æ¢ç´¢"],
            "æ¨¡æ‹Ÿ": ["æ¨¡ä»¿", "ä»¿æ•ˆ", "æ¨¡æ‹Ÿ"],
            "æ²¸ç‚¹": ["æ²¸è…¾æ¸©åº¦", "æ²¸è…¾ç‚¹"],
            "æ ‡å‡†": ["è§„èŒƒ", "åŸºå‡†"],
            "è¯·": ["èƒ½å¦", "éº»çƒ¦", "å¯ä»¥"],
        }
    
    def generate_lexical_variations(self, prompt: str, replace_ratio: float = 0.3) -> List[str]:
        """
        ç”Ÿæˆè¯æ±‡å±‚é¢æ”¹å†™
        
        Args:
            prompt: åŸå§‹æç¤º
            replace_ratio: æ›¿æ¢æ¯”ä¾‹
        """
        variations = []
        
        # å˜ä½“1: éƒ¨åˆ†åŒä¹‰è¯æ›¿æ¢
        words = list(prompt)
        replace_count = max(1, int(len(words) * replace_ratio))
        
        for char in prompt:
            if char in self.synonyms and replace_count > 0:
                synonym = random.choice(self.synonyms[char])
                prompt = prompt.replace(char, synonym, 1)
                replace_count -= 1
        
        variations.append(prompt)
        
        # å˜ä½“2: è°ƒæ•´ç¤¼è²Œç”¨è¯­
        if "è¯·" in prompt:
            variations.append(prompt.replace("è¯·", "èƒ½å¦"))
        else:
            variations.append("è¯·" + prompt)
        
        return variations
    
    def generate_syntactic_variations(self, prompt: str) -> List[str]:
        """
        ç”Ÿæˆå¥æ³•å±‚é¢æ”¹å†™
        
        åŒ…æ‹¬ï¼š
        - ä¸»åŠ¨è¢«åŠ¨è½¬æ¢
        - è¯­åºè°ƒæ•´
        - å¥å¼å˜æ¢
        """
        variations = []
        
        # å˜ä½“1: è°ƒæ•´è¯­åºï¼ˆç®€å•æ¨¡æ‹Ÿï¼‰
        if "ä»€ä¹ˆæ˜¯" in prompt:
            variations.append(prompt.replace("ä»€ä¹ˆæ˜¯", "è¯·è§£é‡Š"))
        
        # å˜ä½“2: æ”¹å˜å¥å¼
        if "?" in prompt or "ï¼Ÿ" in prompt:
            variations.append(prompt.replace("?", "ã€‚è¯·è¯´æ˜ã€‚").replace("ï¼Ÿ", "ã€‚è¯·è¯´æ˜ã€‚"))
        
        # å˜ä½“3: æ·»åŠ ä¿®é¥°
        variations.append("æˆ‘æƒ³çŸ¥é“ï¼Œ" + prompt)
        
        return variations
    
    def test_lexical_robustness(self, prompt: str) -> RobustnessResult:
        """
        æµ‹è¯•è¯æ±‡å±‚é¢é²æ£’æ€§
        """
        print(f"\n{'='*60}")
        print(f"ã€è¯æ±‡æ”¹å†™æµ‹è¯•ã€‘")
        print(f"{'='*60}")
        print(f"åŸå§‹æç¤º: {prompt}")
        
        # ç”Ÿæˆæ”¹å†™å˜ä½“
        variations = self.generate_lexical_variations(prompt)
        print(f"\nç”Ÿæˆ {len(variations)} ä¸ªæ”¹å†™å˜ä½“:")
        for i, v in enumerate(variations, 1):
            print(f"  å˜ä½“{i}: {v}")
        
        # è·å–æ¨¡å‹è¾“å‡º
        outputs = [self.model.generate(v) for v in variations]
        print(f"\næ¨¡å‹è¾“å‡º:")
        for i, o in enumerate(outputs, 1):
            print(f"  è¾“å‡º{i}: {o[:50]}...")
        
        # è®¡ç®—è¯­ä¹‰ä¸€è‡´æ€§
        consistency_scores = []
        for i in range(1, len(outputs)):
            score = self.model.get_answer_consistency(variations[0], variations[i])
            consistency_scores.append(score)
        
        print(f"\nè¯­ä¹‰ä¸€è‡´æ€§åˆ†æ•°:")
        for i, score in enumerate(consistency_scores, 1):
            status = "âœ…" if score > 0.8 else "âš ï¸"
            print(f"  å˜ä½“{i} vs åŸå§‹: {score:.3f} {status}")
        
        # åˆ¤æ–­é²æ£’æ€§
        avg_consistency = sum(consistency_scores) / len(consistency_scores) if consistency_scores else 1.0
        is_robust = avg_consistency > 0.8
        
        if avg_consistency > 0.9:
            risk_level = RobustnessLevel.HIGH
            message = "è¯æ±‡æ”¹å†™é²æ£’æ€§è‰¯å¥½"
        elif avg_consistency > 0.7:
            risk_level = RobustnessLevel.MEDIUM
            message = "è¯æ±‡æ”¹å†™å­˜åœ¨ä¸€å®šæ•æ„Ÿ"
        else:
            risk_level = RobustnessLevel.LOW
            message = "è¯æ±‡æ”¹å†™æ•æ„Ÿï¼Œå­˜åœ¨é²æ£’æ€§é—®é¢˜"
        
        print(f"\nå¹³å‡ä¸€è‡´æ€§: {avg_consistency:.3f}")
        print(f"é²æ£’æ€§åˆ¤å®š: {risk_level.value}")
        print(f"ç»“è®º: {message}")
        
        return RobustnessResult(
            test_type=RobustnessTestType.LEXICAL,
            original_prompt=prompt,
            variations=variations,
            outputs=outputs,
            consistency_scores=consistency_scores,
            accuracy_scores=[],
            is_robust=is_robust,
            risk_level=risk_level,
            message=message
        )
    
    def test_syntactic_robustness(self, prompt: str) -> RobustnessResult:
        """
        æµ‹è¯•å¥æ³•å±‚é¢é²æ£’æ€§
        """
        print(f"\n{'='*60}")
        print(f"ã€å¥æ³•æ”¹å†™æµ‹è¯•ã€‘")
        print(f"{'='*60}")
        print(f"åŸå§‹æç¤º: {prompt}")
        
        # ç”Ÿæˆå¥æ³•å˜ä½“
        variations = self.generate_syntactic_variations(prompt)
        print(f"\nç”Ÿæˆ {len(variations)} ä¸ªå¥æ³•å˜ä½“:")
        for i, v in enumerate(variations, 1):
            print(f"  å˜ä½“{i}: {v}")
        
        # è·å–æ¨¡å‹è¾“å‡º
        outputs = [self.model.generate(v) for v in variations]
        print(f"\næ¨¡å‹è¾“å‡º:")
        for i, o in enumerate(outputs, 1):
            print(f"  è¾“å‡º{i}: {o[:50]}...")
        
        # è®¡ç®—ä¸€è‡´æ€§
        consistency_scores = []
        for i in range(1, len(outputs)):
            score = self.model.get_answer_consistency(variations[0], variations[i])
            consistency_scores.append(score)
        
        print(f"\nè¯­ä¹‰ä¸€è‡´æ€§åˆ†æ•°:")
        for i, score in enumerate(consistency_scores, 1):
            status = "âœ…" if score > 0.8 else "âš ï¸"
            print(f"  å˜ä½“{i} vs åŸå§‹: {score:.3f} {status}")
        
        avg_consistency = sum(consistency_scores) / len(consistency_scores) if consistency_scores else 1.0
        is_robust = avg_consistency > 0.8
        
        if avg_consistency > 0.9:
            risk_level = RobustnessLevel.HIGH
            message = "å¥æ³•æ”¹å†™é²æ£’æ€§è‰¯å¥½"
        elif avg_consistency > 0.7:
            risk_level = RobustnessLevel.MEDIUM
            message = "å¥æ³•æ”¹å†™å­˜åœ¨ä¸€å®šæ•æ„Ÿ"
        else:
            risk_level = RobustnessLevel.LOW
            message = "å¥æ³•æ”¹å†™æ•æ„Ÿï¼Œå­˜åœ¨é²æ£’æ€§é—®é¢˜"
        
        print(f"\nå¹³å‡ä¸€è‡´æ€§: {avg_consistency:.3f}")
        print(f"é²æ£’æ€§åˆ¤å®š: {risk_level.value}")
        print(f"ç»“è®º: {message}")
        
        return RobustnessResult(
            test_type=RobustnessTestType.SYNTACTIC,
            original_prompt=prompt,
            variations=variations,
            outputs=outputs,
            consistency_scores=consistency_scores,
            accuracy_scores=[],
            is_robust=is_robust,
            risk_level=risk_level,
            message=message
        )


class NoiseTester:
    """å™ªå£°æ³¨å…¥æµ‹è¯•å™¨"""
    
    def __init__(self, model: MockLLM):
        self.model = model
    
    def inject_char_noise(self, text: str, noise_ratio: float) -> str:
        """
        æ³¨å…¥å­—ç¬¦çº§å™ªå£°
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            noise_ratio: å™ªå£°æ¯”ä¾‹ (0.0 - 1.0)
        """
        chars = list(text)
        num_noisy = max(1, int(len(chars) * noise_ratio))
        
        noise_types = ['replace', 'insert', 'delete', 'swap']
        
        for _ in range(num_noisy):
            if len(chars) < 2:
                break
            pos = random.randint(0, len(chars) - 1)
            noise_type = random.choice(noise_types)
            
            if noise_type == 'replace':
                chars[pos] = random.choice(string.ascii_letters + string.digits)
            elif noise_type == 'insert':
                chars.insert(pos, random.choice(string.ascii_letters))
            elif noise_type == 'delete':
                chars.pop(pos)
            elif noise_type == 'swap' and pos < len(chars) - 1:
                chars[pos], chars[pos + 1] = chars[pos + 1], chars[pos]
        
        return ''.join(chars)
    
    def test_noise_robustness(self, prompt: str, noise_levels: List[float]) -> RobustnessResult:
        """
        æµ‹è¯•å™ªå£°é²æ£’æ€§
        
        Args:
            prompt: åŸå§‹æç¤º
            noise_levels: å™ªå£°çº§åˆ«åˆ—è¡¨
        """
        print(f"\n{'='*60}")
        print(f"ã€å™ªå£°æ³¨å…¥æµ‹è¯•ã€‘")
        print(f"{'='*60}")
        print(f"åŸå§‹æç¤º: {prompt}")
        print(f"\nå™ªå£°çº§åˆ«: {noise_levels}")
        
        # ç”Ÿæˆå™ªå£°å˜ä½“
        variations = []
        for level in noise_levels:
            noisy = self.inject_char_noise(prompt, level)
            variations.append(noisy)
        
        print(f"\nå™ªå£°å˜ä½“:")
        for i, (level, v) in enumerate(zip(noise_levels, variations), 1):
            print(f"  Level {level*100:.0f}%: {v}")
        
        # è·å–åŸå§‹è¾“å‡ºï¼ˆä½œä¸ºåŸºå‡†ï¼‰
        original_output = self.model.generate(prompt)
        print(f"\nåŸå§‹è¾“å‡º: {original_output}")
        
        # è·å–å™ªå£°è¾“å‡º
        outputs = [self.model.generate(v, noise_level=level) for v, level in zip(variations, noise_levels)]
        print(f"\nå™ªå£°è¾“å‡º:")
        for i, (level, o) in enumerate(zip(noise_levels, outputs), 1):
            print(f"  Level {level*100:.0f}%: {o}")
        
        # è®¡ç®—å‡†ç¡®ç‡ï¼ˆä¸åŸå§‹è¾“å‡ºçš„ä¸€è‡´æ€§ï¼‰
        accuracy_scores = []
        for output in outputs:
            score = self.model._text_similarity(original_output, output)
            accuracy_scores.append(score)
        
        print(f"\nå‡†ç¡®ç‡è¡°å‡:")
        for level, score in zip(noise_levels, accuracy_scores):
            status = "âœ…" if score > 0.7 else "âš ï¸" if score > 0.4 else "âŒ"
            print(f"  Level {level*100:.0f}%: {score:.3f} {status}")
        
        # è®¡ç®—è¡°å‡ç‡å’Œå¤±æ•ˆä¸´ç•Œç‚¹
        if accuracy_scores:
            decay_rate = (1.0 - accuracy_scores[-1]) / 1.0
            
            # æ‰¾å¤±æ•ˆä¸´ç•Œç‚¹ï¼ˆå‡†ç¡®ç‡ < 0.5ï¼‰
            failure_point = None
            for level, score in zip(noise_levels, accuracy_scores):
                if score < 0.5:
                    failure_point = level
                    break
        else:
            decay_rate = 0.0
            failure_point = None
        
        print(f"\næ€»ä½“è¡°å‡ç‡: {decay_rate:.1%}")
        if failure_point:
            print(f"å¤±æ•ˆä¸´ç•Œç‚¹: {failure_point*100:.0f}% å™ªå£°")
        else:
            print(f"å¤±æ•ˆä¸´ç•Œç‚¹: æœªè¾¾åˆ°")
        
        # åˆ¤æ–­é²æ£’æ€§
        avg_accuracy = sum(accuracy_scores) / len(accuracy_scores) if accuracy_scores else 1.0
        is_robust = avg_accuracy > 0.7
        
        if decay_rate < 0.2:
            risk_level = RobustnessLevel.HIGH
            message = "å™ªå£°é²æ£’æ€§è‰¯å¥½"
        elif decay_rate < 0.5:
            risk_level = RobustnessLevel.MEDIUM
            message = "å™ªå£°é²æ£’æ€§ä¸€èˆ¬ï¼Œå­˜åœ¨è¡°å‡"
        else:
            risk_level = RobustnessLevel.LOW
            message = "å™ªå£°é²æ£’æ€§å·®ï¼Œå¿«é€Ÿå¤±æ•ˆ"
        
        print(f"é²æ£’æ€§åˆ¤å®š: {risk_level.value}")
        print(f"ç»“è®º: {message}")
        
        return RobustnessResult(
            test_type=RobustnessTestType.NOISE_CHAR,
            original_prompt=prompt,
            variations=variations,
            outputs=outputs,
            consistency_scores=[],
            accuracy_scores=accuracy_scores,
            is_robust=is_robust,
            risk_level=risk_level,
            message=message
        )


class RobustnessEvaluator:
    """é²æ£’æ€§ç»¼åˆè¯„ä¼°å™¨"""
    
    def __init__(self):
        self.results: List[RobustnessResult] = []
    
    def add_result(self, result: RobustnessResult):
        """æ·»åŠ æµ‹è¯•ç»“æœ"""
        self.results.append(result)
    
    def generate_report(self) -> Dict:
        """ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š"""
        print(f"\n{'='*70}")
        print(f"ã€ç»¼åˆé²æ£’æ€§è¯„ä¼°æŠ¥å‘Šã€‘")
        print(f"{'='*70}")
        
        # ç»Ÿè®¡å„é£é™©ç­‰çº§
        risk_counts = {level: 0 for level in RobustnessLevel}
        for result in self.results:
            risk_counts[result.risk_level] += 1
        
        print(f"\né£é™©åˆ†å¸ƒç»Ÿè®¡:")
        for level, count in risk_counts.items():
            if count > 0:
                print(f"  {level.value}: {count} é¡¹")
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        level_scores = {
            RobustnessLevel.HIGH: 1.0,
            RobustnessLevel.MEDIUM: 0.6,
            RobustnessLevel.LOW: 0.3,
            RobustnessLevel.CRITICAL: 0.0
        }
        
        total_score = sum(level_scores[r.risk_level] for r in self.results)
        avg_score = total_score / len(self.results) if self.results else 0.0
        
        print(f"\nç»¼åˆé²æ£’æ€§å¾—åˆ†: {avg_score:.2f}/1.0")
        
        # æ•´ä½“è¯„çº§
        if avg_score >= 0.8:
            overall_level = RobustnessLevel.HIGH
            recommendation = "é²æ£’æ€§è‰¯å¥½ï¼Œå¯æŠ•å…¥ç”Ÿäº§ç¯å¢ƒ"
        elif avg_score >= 0.5:
            overall_level = RobustnessLevel.MEDIUM
            recommendation = "é²æ£’æ€§ä¸€èˆ¬ï¼Œå»ºè®®ä¼˜åŒ–åä¸Šçº¿"
        else:
            overall_level = RobustnessLevel.LOW
            recommendation = "é²æ£’æ€§ä¸è¶³ï¼Œéœ€é‡ç‚¹æ”¹è¿›"
        
        print(f"æ•´ä½“è¯„çº§: {overall_level.value}")
        print(f"å»ºè®®: {recommendation}")
        
        # è„†å¼±ç‚¹åˆ†æ
        print(f"\nè¯†åˆ«çš„è„†å¼±ç‚¹:")
        vulnerable_tests = [r for r in self.results if r.risk_level in [RobustnessLevel.LOW, RobustnessLevel.CRITICAL]]
        if vulnerable_tests:
            for r in vulnerable_tests:
                print(f"  âš ï¸ {r.test_type.value}: {r.message}")
        else:
            print(f"  âœ… æœªå‘ç°æ˜æ˜¾è„†å¼±ç‚¹")
        
        return {
            "overall_score": avg_score,
            "overall_level": overall_level,
            "risk_distribution": risk_counts,
            "vulnerable_points": len(vulnerable_tests),
            "recommendation": recommendation
        }


# ============ Pytest æµ‹è¯•ç”¨ä¾‹ ============

@pytest.fixture
def mock_model():
    """æä¾›Mock LLMå®ä¾‹"""
    return MockLLM(seed=42)


@pytest.fixture
def paraphrase_tester(mock_model):
    """æä¾›æ”¹å†™æµ‹è¯•å™¨"""
    return ParaphraseTester(mock_model)


@pytest.fixture
def noise_tester(mock_model):
    """æä¾›å™ªå£°æµ‹è¯•å™¨"""
    return NoiseTester(mock_model)


@pytest.fixture
def evaluator():
    """æä¾›è¯„ä¼°å™¨"""
    return RobustnessEvaluator()


class TestRobustness:
    """é²æ£’æ€§æµ‹è¯•å¥—ä»¶"""
    
    def test_lexical_paraphrase_robustness(self, paraphrase_tester, evaluator):
        """æµ‹è¯•è¯æ±‡æ”¹å†™é²æ£’æ€§"""
        print("\n" + "="*60)
        print("ã€æµ‹è¯•1ã€‘è¯æ±‡æ”¹å†™é²æ£’æ€§éªŒè¯ï¼ˆå…³é”®æµ‹è¯•ï¼‰")
        print("="*60)
        
        prompt = "ä»€ä¹ˆæ˜¯å…‰åˆä½œç”¨ï¼Ÿ"
        result = paraphrase_tester.test_lexical_robustness(prompt)
        evaluator.add_result(result)
        
        # éªŒè¯ï¼šè‡³å°‘æœ‰ä¸€ä¸ªæ”¹å†™å˜ä½“
        assert len(result.variations) >= 1, "åº”ç”Ÿæˆè‡³å°‘1ä¸ªæ”¹å†™å˜ä½“"
        
        # éªŒè¯ï¼šè¾“å‡ºäº†ç»“æœ
        assert len(result.outputs) == len(result.variations), "è¾“å‡ºæ•°é‡åº”ä¸å˜ä½“æ•°é‡ä¸€è‡´"
        
        # é£é™©éªŒè¯ï¼šå¦‚æœé²æ£’æ€§ä½ï¼Œéœ€è¦è®°å½•
        if result.risk_level in [RobustnessLevel.LOW, RobustnessLevel.CRITICAL]:
            print(f"\nğŸ”´ å‘ç°é²æ£’æ€§é—®é¢˜: {result.message}")
        
        print("\nâœ… è¯æ±‡æ”¹å†™é²æ£’æ€§æµ‹è¯•é€šè¿‡")
    
    def test_syntactic_paraphrase_robustness(self, paraphrase_tester, evaluator):
        """æµ‹è¯•å¥æ³•æ”¹å†™é²æ£’æ€§"""
        print("\n" + "="*60)
        print("ã€æµ‹è¯•2ã€‘å¥æ³•æ”¹å†™é²æ£’æ€§éªŒè¯")
        print("="*60)
        
        prompt = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
        result = paraphrase_tester.test_syntactic_robustness(prompt)
        evaluator.add_result(result)
        
        # éªŒè¯ï¼šç”Ÿæˆäº†å¥æ³•å˜ä½“
        assert len(result.variations) >= 1, "åº”ç”Ÿæˆè‡³å°‘1ä¸ªå¥æ³•å˜ä½“"
        
        # éªŒè¯ï¼šè¯­ä¹‰ä¸€è‡´æ€§è®¡ç®—
        assert len(result.consistency_scores) > 0, "åº”è®¡ç®—è¯­ä¹‰ä¸€è‡´æ€§åˆ†æ•°"
        
        print("\nâœ… å¥æ³•æ”¹å†™é²æ£’æ€§æµ‹è¯•é€šè¿‡")
    
    def test_noise_injection_robustness(self, noise_tester, evaluator):
        """æµ‹è¯•å™ªå£°æ³¨å…¥é²æ£’æ€§ï¼ˆå…³é”®æµ‹è¯•ï¼‰"""
        print("\n" + "="*60)
        print("ã€æµ‹è¯•3ã€‘å™ªå£°æ³¨å…¥é²æ£’æ€§éªŒè¯ï¼ˆå…³é”®æµ‹è¯•ï¼‰")
        print("="*60)
        
        prompt = "æ°´çš„æ²¸ç‚¹æ˜¯å¤šå°‘ï¼Ÿ"
        noise_levels = [0.05, 0.15, 0.30, 0.50]  # 5%, 15%, 30%, 50%
        
        result = noise_tester.test_noise_robustness(prompt, noise_levels)
        evaluator.add_result(result)
        
        # éªŒè¯ï¼šç”Ÿæˆäº†æ‰€æœ‰å™ªå£°çº§åˆ«
        assert len(result.variations) == len(noise_levels), "åº”ç”Ÿæˆæ‰€æœ‰å™ªå£°çº§åˆ«çš„å˜ä½“"
        
        # éªŒè¯ï¼šå‡†ç¡®ç‡åˆ†æ•°è®°å½•
        assert len(result.accuracy_scores) == len(noise_levels), "åº”è®°å½•æ‰€æœ‰å‡†ç¡®ç‡åˆ†æ•°"
        
        # å…³é”®éªŒè¯ï¼šé«˜å™ªå£°ä¸‹å‡†ç¡®ç‡ä¸åº”éª¤é™
        if result.accuracy_scores:
            high_noise_accuracy = result.accuracy_scores[-1]  # 50%å™ªå£°
            print(f"\né«˜å™ªå£°(50%)å‡†ç¡®ç‡: {high_noise_accuracy:.3f}")
            
            if high_noise_accuracy < 0.3:
                print(f"\nğŸ”´ è­¦å‘Šï¼šé«˜å™ªå£°ä¸‹å‡†ç¡®ç‡è¿‡ä½({high_noise_accuracy:.3f})ï¼Œå­˜åœ¨ä¸¥é‡é²æ£’æ€§é—®é¢˜")
        
        print("\nâœ… å™ªå£°æ³¨å…¥é²æ£’æ€§æµ‹è¯•é€šè¿‡")
    
    def test_format_dependency(self, mock_model):
        """æµ‹è¯•æ ¼å¼ä¾èµ–ï¼ˆç¤¼è²Œç”¨è¯­æ•æ„Ÿï¼‰"""
        print("\n" + "="*60)
        print("ã€æµ‹è¯•4ã€‘æ ¼å¼ä¾èµ–éªŒè¯ï¼ˆç¤¼è²Œç”¨è¯­æ•æ„Ÿï¼‰")
        print("="*60)
        
        # æœ‰ç¤¼è²Œç”¨è¯­çš„è¾“å…¥
        polite_prompt = "è¯·é—®ï¼Œä»€ä¹ˆæ˜¯å…‰åˆä½œç”¨ï¼Ÿ"
        polite_output = mock_model.generate(polite_prompt)
        
        # æ— ç¤¼è²Œç”¨è¯­çš„è¾“å…¥
        direct_prompt = "ä»€ä¹ˆæ˜¯å…‰åˆä½œç”¨ï¼Ÿ"
        direct_output = mock_model.generate(direct_prompt)
        
        print(f"ç¤¼è²Œè¾“å…¥: {polite_prompt}")
        print(f"è¾“å‡º: {polite_output}")
        print(f"\nç›´æ¥è¾“å…¥: {direct_prompt}")
        print(f"è¾“å‡º: {direct_output}")
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ ¼å¼ä¾èµ–é—®é¢˜
        if "ç¤¼è²Œ" in direct_output:
            print(f"\nğŸ”´ å‘ç°æ ¼å¼ä¾èµ–é—®é¢˜ï¼šæ¨¡å‹å¯¹ç¤¼è²Œç”¨è¯­æ•æ„Ÿ")
        else:
            print(f"\nâœ… æœªå‘ç°æ˜æ˜¾çš„æ ¼å¼ä¾èµ–é—®é¢˜")
        
        print("\nâœ… æ ¼å¼ä¾èµ–æµ‹è¯•é€šè¿‡")
    
    def test_negation_sensitivity(self, mock_model):
        """æµ‹è¯•å¦å®šè¯æ•æ„Ÿ"""
        print("\n" + "="*60)
        print("ã€æµ‹è¯•5ã€‘å¦å®šè¯æ•æ„ŸéªŒè¯")
        print("="*60)
        
        # è‚¯å®šå¥
        positive_prompt = "æ°´æ˜¯æ¶²ä½“å—ï¼Ÿ"
        positive_output = mock_model.generate(positive_prompt)
        
        # å¦å®šå¥
        negative_prompt = "æ°´ä¸æ˜¯å›ºä½“å—ï¼Ÿ"
        negative_output = mock_model.generate(negative_prompt)
        
        print(f"è‚¯å®šå¥: {positive_prompt}")
        print(f"è¾“å‡º: {positive_output}")
        print(f"\nå¦å®šå¥: {negative_prompt}")
        print(f"è¾“å‡º: {negative_output}")
        
        # æ£€æŸ¥å¦å®šè¯å¤„ç†
        # æ³¨æ„ï¼šç”±äºMockæ¨¡å‹ç®€å•ï¼Œè¿™é‡Œä¸»è¦éªŒè¯æµ‹è¯•æ¡†æ¶
        print(f"\nâœ… å¦å®šè¯æ•æ„Ÿæµ‹è¯•å®Œæˆï¼ˆéœ€äººå·¥åˆ¤æ–­é€»è¾‘ä¸€è‡´æ€§ï¼‰")
    
    def test_comprehensive_robustness_evaluation(self, paraphrase_tester, noise_tester, evaluator):
        """ç»¼åˆé²æ£’æ€§è¯„ä¼°"""
        print("\n" + "="*60)
        print("ã€æµ‹è¯•6ã€‘ç»¼åˆé²æ£’æ€§è¯„ä¼°")
        print("="*60)
        
        # è¿è¡Œå¤šä¸ªæµ‹è¯•å¹¶æ”¶é›†ç»“æœ
        test_prompts = [
            "ä»€ä¹ˆæ˜¯å…‰åˆä½œç”¨ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
            "æ°´çš„æ²¸ç‚¹æ˜¯å¤šå°‘ï¼Ÿ"
        ]
        
        for prompt in test_prompts:
            # è¯æ±‡æµ‹è¯•
            result_lexical = paraphrase_tester.test_lexical_robustness(prompt)
            evaluator.add_result(result_lexical)
            
            # å™ªå£°æµ‹è¯•
            result_noise = noise_tester.test_noise_robustness(prompt, [0.1, 0.3])
            evaluator.add_result(result_noise)
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = evaluator.generate_report()
        
        # éªŒè¯æŠ¥å‘Šç»“æ„
        assert "overall_score" in report, "æŠ¥å‘Šåº”åŒ…å«ç»¼åˆå¾—åˆ†"
        assert "overall_level" in report, "æŠ¥å‘Šåº”åŒ…å«æ•´ä½“è¯„çº§"
        assert "recommendation" in report, "æŠ¥å‘Šåº”åŒ…å«å»ºè®®"
        
        print("\nâœ… ç»¼åˆé²æ£’æ€§è¯„ä¼°å®Œæˆ")


if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œæµ‹è¯•
    pytest.main([__file__, "-v", "-s"])
