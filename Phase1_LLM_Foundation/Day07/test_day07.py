"""
è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬ï¼šDay 07 - æ¨¡å‹ç‰ˆæœ¬è¿­ä»£ä¸å›å½’æµ‹è¯•
ç›®æ ‡ï¼šåŸºå‡†æµ‹è¯•å¥—ä»¶æ„å»ºã€ç‰ˆæœ¬å¯¹æ¯”ã€å›å½’å†³ç­–
é£é™©è§†è§’ï¼šä¸“æ³¨æ¨¡å‹å‡çº§å›å½’é£é™©å’Œç‰ˆæœ¬å¯¹æ¯”ç›²åŒº
"""

import os
import pytest
import json
import time
import random
import statistics
from typing import List, Dict, Tuple, Any, Optional
from dataclasses import dataclass, field, asdict
from datetime import datetime
from enum import Enum
from collections import defaultdict


class TestCategory(Enum):
    """æµ‹è¯•ç”¨ä¾‹åˆ†ç±»"""
    CUSTOMER_SERVICE = "å®¢æœåœºæ™¯"
    CODE_GENERATION = "ä»£ç ç”Ÿæˆ"
    CONTENT_CREATION = "æ–‡æ¡ˆåˆ›ä½œ"
    DATA_EXTRACTION = "æ•°æ®æå–"
    REASONING = "é€»è¾‘æ¨ç†"


class RiskLevel(Enum):
    """é£é™©ç­‰çº§"""
    CRITICAL = "ğŸ”´ CRITICAL"
    HIGH = "ğŸŸ  HIGH"
    MEDIUM = "ğŸŸ¡ MEDIUM"
    LOW = "ğŸŸ¢ LOW"
    PASS = "âœ… PASS"


@dataclass
class TestCase:
    """æµ‹è¯•ç”¨ä¾‹æ•°æ®ç±»"""
    id: str
    category: TestCategory
    prompt: str
    expected_keywords: List[str]
    expected_format: Optional[str] = None
    difficulty: str = "medium"  # easy/medium/hard


@dataclass
class TestResult:
    """å•ä¸ªæµ‹è¯•ç”¨ä¾‹ç»“æœ"""
    test_id: str
    category: TestCategory
    response: str
    latency_ms: float
    accuracy_score: float  # 0-1
    format_valid: bool
    safety_pass: bool
    timestamp: str


@dataclass
class VersionMetrics:
    """ç‰ˆæœ¬æŒ‡æ ‡æ±‡æ€»"""
    version_name: str
    accuracy: float
    latency_p50: float
    latency_p95: float
    stability_score: float
    safety_score: float
    test_count: int
    pass_count: int
    timestamp: str
    
    def to_dict(self) -> Dict:
        return {
            "version_name": self.version_name,
            "accuracy": round(self.accuracy, 4),
            "latency_p50": round(self.latency_p50, 2),
            "latency_p95": round(self.latency_p95, 2),
            "stability_score": round(self.stability_score, 4),
            "safety_score": round(self.safety_score, 4),
            "test_count": self.test_count,
            "pass_count": self.pass_count,
            "pass_rate": round(self.pass_count / self.test_count, 4) if self.test_count > 0 else 0,
            "timestamp": self.timestamp
        }


@dataclass
class RegressionComparison:
    """å›å½’å¯¹æ¯”ç»“æœ"""
    metric: str
    baseline_value: float
    new_value: float
    change_percent: float
    threshold: float
    status: str  # PASS / WARNING / FAIL
    risk_level: RiskLevel


class BenchmarkSuite:
    """åŸºå‡†æµ‹è¯•å¥—ä»¶ç®¡ç†å™¨"""
    
    def __init__(self, suite_name: str = "default"):
        self.suite_name = suite_name
        self.test_cases: List[TestCase] = []
        self.results: Dict[str, List[TestResult]] = defaultdict(list)
        self.suite_dir = f"./benchmark_suites/{suite_name}"
        os.makedirs(self.suite_dir, exist_ok=True)
    
    def load_golden_dataset(self) -> List[TestCase]:
        """åŠ è½½é»„é‡‘æ•°æ®é›†"""
        # æ¨¡æ‹Ÿé»„é‡‘æ•°æ®é›† - è¦†ç›–5å¤§æ ¸å¿ƒåœºæ™¯
        test_cases = [
            # å®¢æœåœºæ™¯
            TestCase("CS-001", TestCategory.CUSTOMER_SERVICE, 
                    "å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ", ["å¯†ç ", "é‡ç½®", "è®¾ç½®"]),
            TestCase("CS-002", TestCategory.CUSTOMER_SERVICE,
                    "è®¢å•ä»€ä¹ˆæ—¶å€™å‘è´§ï¼Ÿ", ["è®¢å•", "å‘è´§", "ç‰©æµ"]),
            TestCase("CS-003", TestCategory.CUSTOMER_SERVICE,
                    "å¦‚ä½•ç”³è¯·é€€æ¬¾ï¼Ÿ", ["é€€æ¬¾", "ç”³è¯·", "é€€è´§"]),
            
            # ä»£ç ç”Ÿæˆ
            TestCase("CODE-001", TestCategory.CODE_GENERATION,
                    "ç”¨Pythonå†™ä¸€ä¸ªå¿«é€Ÿæ’åº", ["def", "quicksort", "sort"], "python"),
            TestCase("CODE-002", TestCategory.CODE_GENERATION,
                    "å†™ä¸€ä¸ªåˆ¤æ–­è´¨æ•°çš„å‡½æ•°", ["def", "prime", "return"], "python"),
            TestCase("CODE-003", TestCategory.CODE_GENERATION,
                    "ç”¨JavaScriptå®ç°é˜²æŠ–å‡½æ•°", ["function", "debounce", "setTimeout"], "javascript"),
            
            # æ–‡æ¡ˆåˆ›ä½œ
            TestCase("CONTENT-001", TestCategory.CONTENT_CREATION,
                    "å†™ä¸€æ®µäº§å“æ¨å¹¿æ–‡æ¡ˆï¼Œæ¨å¹¿ä¸€æ¬¾æ™ºèƒ½æ‰‹è¡¨", ["æ™ºèƒ½", "æ‰‹è¡¨", "åŠŸèƒ½"]),
            TestCase("CONTENT-002", TestCategory.CONTENT_CREATION,
                    "ä¸ºå’–å•¡åº—å†™ä¸€å¥slogan", ["å’–å•¡", "slogan", "å“ç‰Œ"]),
            
            # æ•°æ®æå–
            TestCase("DATA-001", TestCategory.DATA_EXTRACTION,
                    "ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å§“åå’Œç”µè¯ï¼šè”ç³»äººï¼šå¼ ä¸‰ï¼Œç”µè¯ï¼š13800138000",
                    ["å¼ ä¸‰", "13800138000"], "json"),
            TestCase("DATA-002", TestCategory.DATA_EXTRACTION,
                    "æå–æ—¥æœŸï¼šä¼šè®®å®šäº2024å¹´3æœˆ15æ—¥ä¸‹åˆ2ç‚¹", ["2024", "3æœˆ", "15æ—¥"]),
            
            # é€»è¾‘æ¨ç†
            TestCase("REASON-001", TestCategory.REASONING,
                    "å¦‚æœæ‰€æœ‰çš„Aéƒ½æ˜¯Bï¼Œæ‰€æœ‰çš„Béƒ½æ˜¯Cï¼Œé‚£ä¹ˆAå’ŒCçš„å…³ç³»æ˜¯ï¼Ÿ", ["A", "C", "å±äº"]),
            TestCase("REASON-002", TestCategory.REASONING,
                    "ä¸‰ä¸ªå¼€å…³æ§åˆ¶ä¸‰ä¸ªç¯æ³¡ï¼Œæœ€å°‘éœ€è¦è¿›æˆ¿é—´å‡ æ¬¡æ‰èƒ½ç¡®å®šå¯¹åº”å…³ç³»ï¼Ÿ", ["1æ¬¡", "ä¸€æ¬¡", "å¼€å…³"]),
        ]
        
        self.test_cases = test_cases
        print(f"âœ… åŠ è½½é»„é‡‘æ•°æ®é›†: {len(test_cases)} æ¡æµ‹è¯•ç”¨ä¾‹")
        for cat in TestCategory:
            count = sum(1 for tc in test_cases if tc.category == cat)
            print(f"   - {cat.value}: {count}æ¡")
        
        return test_cases
    
    def run_test(self, version_name: str, simulate_degradation: Dict = None) -> VersionMetrics:
        """
        è¿è¡ŒåŸºå‡†æµ‹è¯•
        
        Args:
            version_name: ç‰ˆæœ¬åç§°
            simulate_degradation: æ¨¡æ‹Ÿé€€åŒ– {"accuracy": -0.1, "latency": 1.5}
        """
        print(f"\nğŸ§ª è¿è¡ŒåŸºå‡†æµ‹è¯•: {version_name}")
        print("-" * 50)
        
        if not self.test_cases:
            self.load_golden_dataset()
        
        simulate_degradation = simulate_degradation or {}
        results = []
        
        for tc in self.test_cases:
            # æ¨¡æ‹Ÿæµ‹è¯•ç»“æœ
            result = self._simulate_test_execution(tc, simulate_degradation)
            results.append(result)
        
        self.results[version_name] = results
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = self._calculate_metrics(version_name, results)
        
        print(f"   æµ‹è¯•å®Œæˆ: {metrics.pass_count}/{metrics.test_count} é€šè¿‡")
        print(f"   å‡†ç¡®ç‡: {metrics.accuracy:.2%}")
        print(f"   P50å»¶è¿Ÿ: {metrics.latency_p50:.0f}ms")
        print(f"   P95å»¶è¿Ÿ: {metrics.latency_p95:.0f}ms")
        
        return metrics
    
    def _simulate_test_execution(self, test_case: TestCase, 
                                 degradation: Dict) -> TestResult:
        """æ¨¡æ‹Ÿæµ‹è¯•æ‰§è¡Œ"""
        random.seed(hash(test_case.id) % 10000)
        
        # åŸºç¡€å‡†ç¡®ç‡
        base_accuracy = 0.85
        category_factor = {
            TestCategory.CUSTOMER_SERVICE: 0.05,
            TestCategory.CODE_GENERATION: -0.05,
            TestCategory.CONTENT_CREATION: 0.0,
            TestCategory.DATA_EXTRACTION: 0.03,
            TestCategory.REASONING: -0.08
        }
        
        accuracy = base_accuracy + category_factor.get(test_case.category, 0)
        accuracy += random.uniform(-0.05, 0.05)  # éšæœºæ³¢åŠ¨
        
        # åº”ç”¨é€€åŒ–æ¨¡æ‹Ÿ
        accuracy += degradation.get("accuracy", 0)
        accuracy = max(0, min(1, accuracy))
        
        # æ¨¡æ‹Ÿå»¶è¿Ÿ
        base_latency = 500  # ms
        latency = base_latency + random.gauss(0, 50)
        latency *= degradation.get("latency", 1.0)
        latency = max(100, latency)
        
        # æ¨¡æ‹Ÿå“åº”
        response = self._generate_mock_response(test_case)
        
        # æ ¼å¼éªŒè¯
        format_valid = self._check_format(response, test_case.expected_format)
        
        # å®‰å…¨æ£€æŸ¥
        safety_pass = random.random() > 0.05  # 95%é€šè¿‡ç‡
        
        return TestResult(
            test_id=test_case.id,
            category=test_case.category,
            response=response,
            latency_ms=latency,
            accuracy_score=accuracy,
            format_valid=format_valid,
            safety_pass=safety_pass,
            timestamp=datetime.now().isoformat()
        )
    
    def _generate_mock_response(self, test_case: TestCase) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿå“åº”"""
        responses = {
            TestCategory.CUSTOMER_SERVICE: [
                f"å…³äº{test_case.prompt[:10]}...ï¼Œæ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š...",
                f"æ‚¨å¥½ï¼Œ{test_case.prompt[:10]}...çš„è§£å†³æ–¹æ³•å¦‚ä¸‹ï¼š..."
            ],
            TestCategory.CODE_GENERATION: [
                f"```python\ndef solution():\n    # {test_case.prompt[:20]}\n    pass\n```",
                f"```javascript\nfunction solution() {{\n    // {test_case.prompt[:20]}\n}}\n```"
            ],
            TestCategory.CONTENT_CREATION: [
                f"ä¸ºæ‚¨ç”Ÿæˆçš„æ–‡æ¡ˆï¼š{test_case.prompt[:15]}...",
                f"åˆ›æ„æ–¹æ¡ˆï¼š{test_case.prompt[:15]}..."
            ],
            TestCategory.DATA_EXTRACTION: [
                '{"name": "å¼ ä¸‰", "phone": "13800138000"}',
                'æå–ç»“æœï¼šå§“å-å¼ ä¸‰ï¼Œç”µè¯-13800138000'
            ],
            TestCategory.REASONING: [
                f"æ¨ç†è¿‡ç¨‹ï¼š{test_case.prompt[:20]}...ç­”æ¡ˆæ˜¯...",
                f"åˆ†æï¼š{test_case.prompt[:20]}...ç»“è®ºä¸º..."
            ]
        }
        
        return random.choice(responses.get(test_case.category, ["é»˜è®¤å“åº”"]))
    
    def _check_format(self, response: str, expected_format: Optional[str]) -> bool:
        """æ£€æŸ¥æ ¼å¼åˆè§„æ€§"""
        if expected_format is None:
            return True
        
        if expected_format == "json":
            return response.strip().startswith("{") or "json" in response.lower()
        elif expected_format in ["python", "javascript"]:
            return f"```{expected_format}" in response.lower() or f"```{expected_format[:2]}" in response.lower()
        return True
    
    def _calculate_metrics(self, version_name: str, results: List[TestResult]) -> VersionMetrics:
        """è®¡ç®—ç‰ˆæœ¬æŒ‡æ ‡"""
        if not results:
            raise ValueError("æ— æµ‹è¯•ç»“æœ")
        
        accuracies = [r.accuracy_score for r in results]
        latencies = [r.latency_ms for r in results]
        sorted_latencies = sorted(latencies)
        n = len(sorted_latencies)
        
        return VersionMetrics(
            version_name=version_name,
            accuracy=statistics.mean(accuracies),
            latency_p50=sorted_latencies[n // 2],
            latency_p95=sorted_latencies[int(n * 0.95)],
            stability_score=1.0 - statistics.stdev(accuracies),  # ç¨³å®šæ€§ä¸å‡†ç¡®ç‡æ–¹å·®è´Ÿç›¸å…³
            safety_score=sum(1 for r in results if r.safety_pass) / len(results),
            test_count=len(results),
            pass_count=sum(1 for r in results if r.accuracy_score > 0.7 and r.safety_pass),
            timestamp=datetime.now().isoformat()
        )
    
    def save_suite(self):
        """ä¿å­˜æµ‹è¯•å¥—ä»¶"""
        filepath = os.path.join(self.suite_dir, "suite_config.json")
        data = {
            "suite_name": self.suite_name,
            "test_cases": [
                {
                    "id": tc.id,
                    "category": tc.category.value,
                    "prompt": tc.prompt,
                    "expected_keywords": tc.expected_keywords,
                    "expected_format": tc.expected_format,
                    "difficulty": tc.difficulty
                }
                for tc in self.test_cases
            ]
        }
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        return filepath


class RegressionTester:
    """å›å½’æµ‹è¯•å™¨"""
    
    def __init__(self):
        # é»˜è®¤å›å½’é˜ˆå€¼
        self.thresholds = {
            "accuracy": {"max_degradation": 0.05, "weight": 1.0},  # å‡†ç¡®ç‡ä¸‹é™<5%
            "latency_p50": {"max_degradation": 0.20, "weight": 0.8},  # P50å»¶è¿Ÿå¢åŠ <20%
            "latency_p95": {"max_degradation": 0.30, "weight": 0.8},  # P95å»¶è¿Ÿå¢åŠ <30%
            "stability_score": {"max_degradation": 0.10, "weight": 0.6},  # ç¨³å®šæ€§ä¸‹é™<10%
            "safety_score": {"max_degradation": 0.0, "weight": 1.0},  # å®‰å…¨æ€§ä¸å…è®¸ä¸‹é™
        }
    
    def compare_versions(self, baseline: VersionMetrics, 
                        new_version: VersionMetrics) -> List[RegressionComparison]:
        """
        å¯¹æ¯”ä¸¤ä¸ªç‰ˆæœ¬
        
        Returns:
            å›å½’å¯¹æ¯”ç»“æœåˆ—è¡¨
        """
        print(f"\nğŸ“Š ç‰ˆæœ¬å¯¹æ¯”: {baseline.version_name} vs {new_version.version_name}")
        print("=" * 70)
        
        comparisons = []
        
        for metric, threshold in self.thresholds.items():
            baseline_val = getattr(baseline, metric)
            new_val = getattr(new_version, metric)
            
            # è®¡ç®—å˜åŒ–ç™¾åˆ†æ¯”
            if metric.startswith("latency"):
                # å»¶è¿Ÿæ˜¯è¶Šå°è¶Šå¥½ï¼Œå¢åŠ ä¸ºè´Ÿå‘å˜åŒ–
                change_pct = (new_val - baseline_val) / baseline_val
                max_deg = threshold["max_degradation"]
                status = "FAIL" if change_pct > max_deg else ("WARNING" if change_pct > max_deg * 0.5 else "PASS")
            else:
                # å…¶ä»–æŒ‡æ ‡æ˜¯è¶Šå¤§è¶Šå¥½ï¼Œä¸‹é™ä¸ºè´Ÿå‘å˜åŒ–
                change_pct = (new_val - baseline_val) / baseline_val if baseline_val > 0 else 0
                max_deg = -threshold["max_degradation"]
                status = "FAIL" if change_pct < max_deg else ("WARNING" if change_pct < max_deg * 0.5 else "PASS")
            
            # é£é™©ç­‰çº§
            if status == "FAIL":
                risk = RiskLevel.CRITICAL if metric in ["accuracy", "safety_score"] else RiskLevel.HIGH
            elif status == "WARNING":
                risk = RiskLevel.MEDIUM
            else:
                risk = RiskLevel.PASS
            
            comp = RegressionComparison(
                metric=metric,
                baseline_value=baseline_val,
                new_value=new_val,
                change_percent=change_pct * 100,
                threshold=threshold["max_degradation"] * 100,
                status=status,
                risk_level=risk
            )
            comparisons.append(comp)
            
            icon = "âŒ" if status == "FAIL" else ("âš ï¸" if status == "WARNING" else "âœ…")
            print(f"   {icon} {metric:20s}: {baseline_val:8.2f} â†’ {new_val:8.2f} "
                  f"({change_pct*100:+6.1f}%) | {risk.value}")
        
        return comparisons
    
    def should_block_release(self, comparisons: List[RegressionComparison]) -> Tuple[bool, str]:
        """
        åˆ¤æ–­æ˜¯å¦åº”é˜»å¡å‘å¸ƒ
        
        Returns:
            (should_block, reason)
        """
        critical_failures = [c for c in comparisons 
                           if c.status == "FAIL" and c.metric in ["accuracy", "safety_score"]]
        high_failures = [c for c in comparisons 
                        if c.status == "FAIL" and c.risk_level == RiskLevel.HIGH]
        
        if critical_failures:
            reasons = ", ".join([f"{c.metric}é€€åŒ–{c.change_percent:.1f}%" for c in critical_failures])
            return True, f"å…³é”®æŒ‡æ ‡ä¸¥é‡é€€åŒ–: {reasons}"
        
        if len(high_failures) >= 2:
            return True, f"å¤šä¸ªé«˜ä¼˜å…ˆçº§æŒ‡æ ‡é€€åŒ–"
        
        return False, "é€šè¿‡å›å½’æµ‹è¯•"
    
    def generate_recommendation(self, comparisons: List[RegressionComparison],
                               baseline: VersionMetrics,
                               new_version: VersionMetrics) -> Dict:
        """ç”Ÿæˆå‘å¸ƒå»ºè®®"""
        should_block, reason = self.should_block_release(comparisons)
        
        improvements = [c for c in comparisons if c.change_percent > 0 and not c.metric.startswith("latency")]
        degradations = [c for c in comparisons if c.status in ["WARNING", "FAIL"]]
        
        recommendation = {
            "should_release": not should_block,
            "block_reason": reason if should_block else None,
            "risk_level": self._assess_overall_risk(comparisons),
            "improvements": [{"metric": c.metric, "change": f"{c.change_percent:+.1f}%"} for c in improvements],
            "degradations": [{"metric": c.metric, "change": f"{c.change_percent:+.1f}%", "severity": c.status} for c in degradations],
            "recommendation": self._generate_advice(should_block, improvements, degradations)
        }
        
        return recommendation
    
    def _assess_overall_risk(self, comparisons: List[RegressionComparison]) -> str:
        """è¯„ä¼°æ•´ä½“é£é™©ç­‰çº§"""
        critical_count = sum(1 for c in comparisons if c.risk_level == RiskLevel.CRITICAL)
        high_count = sum(1 for c in comparisons if c.risk_level == RiskLevel.HIGH)
        
        if critical_count > 0:
            return "CRITICAL"
        elif high_count > 0:
            return "HIGH"
        elif any(c.risk_level == RiskLevel.MEDIUM for c in comparisons):
            return "MEDIUM"
        return "LOW"
    
    def _generate_advice(self, should_block: bool, improvements: List, degradations: List) -> str:
        """ç”Ÿæˆå»ºè®®æ–‡æœ¬"""
        if should_block:
            return "âŒ ä¸å»ºè®®å‘å¸ƒï¼šå­˜åœ¨å…³é”®æŒ‡æ ‡é€€åŒ–ï¼Œè¯·ä¿®å¤åé‡æ–°æµ‹è¯•"
        
        if not degradations:
            return "âœ… å»ºè®®å…¨é‡å‘å¸ƒï¼šæ‰€æœ‰æŒ‡æ ‡æ­£å¸¸æˆ–æ”¹å–„"
        
        if len(improvements) > len(degradations):
            return "âš ï¸ å»ºè®®ç°åº¦å‘å¸ƒï¼šæ•´ä½“æ”¹å–„ä½†å­˜åœ¨éƒ¨åˆ†é€€åŒ–ï¼Œå»ºè®®å°æµé‡éªŒè¯"
        
        return "âš ï¸ å»ºè®®è§‚å¯Ÿå‘å¸ƒï¼šå­˜åœ¨è½»å¾®é€€åŒ–ï¼Œå»ºè®®åŠ å¼ºç›‘æ§åå‘å¸ƒ"


class ModelVersionTester:
    """æ¨¡å‹ç‰ˆæœ¬æµ‹è¯•ä¸»ç±»"""
    
    def __init__(self):
        self.benchmark = BenchmarkSuite("llm_regression")
        self.regression = RegressionTester()
    
    def run_full_regression_test(self):
        """è¿è¡Œå®Œæ•´å›å½’æµ‹è¯•"""
        print("\n" + "="*70)
        print("ğŸš€ æ¨¡å‹ç‰ˆæœ¬å›å½’æµ‹è¯•å¯åŠ¨")
        print("="*70)
        
        # 1. åŠ è½½é»„é‡‘æ•°æ®é›†
        self.benchmark.load_golden_dataset()
        
        # 2. è¿è¡ŒåŸºçº¿ç‰ˆæœ¬æµ‹è¯•
        baseline_metrics = self.benchmark.run_test(
            version_name="gpt-3.5-turbo-0613",
            simulate_degradation={}
        )
        
        # 3. è¿è¡Œæ–°ç‰ˆæœ¬æµ‹è¯•ï¼ˆæ¨¡æ‹Ÿä¸åŒé€€åŒ–åœºæ™¯ï¼‰
        test_scenarios = [
            ("gpt-3.5-turbo-1106-æ­£å¸¸", {}),
            ("gpt-3.5-turbo-1106-è½»å¾®é€€åŒ–", {"accuracy": -0.03, "latency": 1.1}),
            ("gpt-3.5-turbo-1106-ä¸¥é‡é€€åŒ–", {"accuracy": -0.08, "latency": 1.5}),
        ]
        
        results = []
        for version_name, degradation in test_scenarios:
            new_metrics = self.benchmark.run_test(version_name, degradation)
            
            # 4. ç‰ˆæœ¬å¯¹æ¯”
            comparisons = self.regression.compare_versions(baseline_metrics, new_metrics)
            
            # 5. ç”Ÿæˆå»ºè®®
            recommendation = self.regression.generate_recommendation(
                comparisons, baseline_metrics, new_metrics
            )
            
            results.append({
                "version": version_name,
                "metrics": new_metrics,
                "comparisons": comparisons,
                "recommendation": recommendation
            })
        
        # 6. ç”ŸæˆæŠ¥å‘Š
        self._generate_report(baseline_metrics, results)
        
        return results
    
    def _generate_report(self, baseline: VersionMetrics, results: List[Dict]):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*70)
        print("ğŸ“‹ å›å½’æµ‹è¯•æŠ¥å‘Š")
        print("="*70)
        
        # åŸºçº¿ä¿¡æ¯
        print(f"\nã€åŸºçº¿ç‰ˆæœ¬ã€‘{baseline.version_name}")
        print(f"   å‡†ç¡®ç‡: {baseline.accuracy:.2%}")
        print(f"   P50å»¶è¿Ÿ: {baseline.latency_p50:.0f}ms")
        print(f"   P95å»¶è¿Ÿ: {baseline.latency_p95:.0f}ms")
        print(f"   ç¨³å®šæ€§: {baseline.stability_score:.2%}")
        print(f"   å®‰å…¨æ€§: {baseline.safety_score:.2%}")
        
        # å„ç‰ˆæœ¬å¯¹æ¯”ç»“æœ
        print(f"\nã€ç‰ˆæœ¬å¯¹æ¯”ç»“æœã€‘")
        for result in results:
            rec = result["recommendation"]
            status_icon = "âœ…" if rec["should_release"] else "âŒ"
            print(f"\n   {status_icon} {result['version']}")
            print(f"      æ•´ä½“é£é™©: {rec['risk_level']}")
            print(f"      å»ºè®®: {rec['recommendation']}")
            
            if rec["improvements"]:
                print(f"      æ”¹å–„é¡¹: {', '.join([i['metric'] for i in rec['improvements']])}")
            if rec["degradations"]:
                print(f"      é€€åŒ–é¡¹: {', '.join([d['metric'] for d in rec['degradations']])}")
        
        # æ€»ç»“
        passed = sum(1 for r in results if r["recommendation"]["should_release"])
        print(f"\nã€æµ‹è¯•æ€»ç»“ã€‘")
        print(f"   æµ‹è¯•ç‰ˆæœ¬æ•°: {len(results)}")
        print(f"   é€šè¿‡æ•°: {passed}")
        print(f"   é˜»å¡æ•°: {len(results) - passed}")
        
        print("\n" + "="*70)
        print("âœ… æµ‹è¯•æ‰§è¡Œå®Œæ¯•ï¼Œè¯·å°†ä¸Šæ–¹æ—¥å¿—å‘ç»™ Trae ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šã€‚")
        print("="*70)


# ============ pytest æµ‹è¯•ç”¨ä¾‹ ============

class TestDay07ModelRegression:
    """Day 07: æ¨¡å‹ç‰ˆæœ¬å›å½’æµ‹è¯•ç±»"""
    
    @pytest.fixture(scope="class")
    def tester(self):
        """æµ‹è¯•å™¨fixture"""
        return ModelVersionTester()
    
    def test_benchmark_suite_loading(self, tester):
        """
        æµ‹è¯•åŸºå‡†å¥—ä»¶åŠ è½½
        
        é£é™©ç‚¹ï¼šæµ‹è¯•ç”¨ä¾‹è¦†ç›–ä¸è¶³å¯¼è‡´å›å½’ç›²åŒº
        éªŒè¯ï¼šé»„é‡‘æ•°æ®é›†å®Œæ•´åŠ è½½
        """
        test_cases = tester.benchmark.load_golden_dataset()
        
        # æ–­è¨€ï¼šæµ‹è¯•ç”¨ä¾‹æ•°é‡å……è¶³
        assert len(test_cases) >= 10, "æµ‹è¯•ç”¨ä¾‹æ•°é‡ä¸è¶³"
        
        # æ–­è¨€ï¼šè¦†ç›–æ‰€æœ‰æ ¸å¿ƒåœºæ™¯
        categories = set(tc.category for tc in test_cases)
        assert len(categories) >= 3, "æµ‹è¯•åœºæ™¯è¦†ç›–ä¸è¶³"
        
        # æ–­è¨€ï¼šæ¯ä¸ªç”¨ä¾‹æœ‰å…³é”®è¯é¢„æœŸ
        for tc in test_cases:
            assert len(tc.expected_keywords) > 0, f"{tc.id} æ— é¢„æœŸå…³é”®è¯"
        
        print(f"\nâœ… åŸºå‡†å¥—ä»¶åŠ è½½æµ‹è¯•é€šè¿‡: {len(test_cases)}æ¡ç”¨ä¾‹")
    
    def test_baseline_execution(self, tester):
        """
        æµ‹è¯•åŸºçº¿ç‰ˆæœ¬æ‰§è¡Œ
        
        é£é™©ç‚¹ï¼šåŸºçº¿æµ‹è¯•å¤±è´¥å¯¼è‡´æ— æ³•å¯¹æ¯”
        éªŒè¯ï¼šåŸºçº¿ç‰ˆæœ¬æµ‹è¯•æˆåŠŸå®Œæˆ
        """
        tester.benchmark.load_golden_dataset()
        metrics = tester.benchmark.run_test("baseline-test")
        
        # æ–­è¨€ï¼šæŒ‡æ ‡è®¡ç®—æˆåŠŸ
        assert metrics.accuracy > 0, "å‡†ç¡®ç‡è®¡ç®—å¤±è´¥"
        assert metrics.latency_p50 > 0, "å»¶è¿Ÿè®¡ç®—å¤±è´¥"
        assert metrics.test_count > 0, "æµ‹è¯•è®¡æ•°å¤±è´¥"
        
        # æ–­è¨€ï¼šåŸºçº¿è´¨é‡åˆç†
        assert 0.5 < metrics.accuracy < 1.0, "å‡†ç¡®ç‡è¶…å‡ºåˆç†èŒƒå›´"
        assert 100 < metrics.latency_p50 < 5000, "å»¶è¿Ÿè¶…å‡ºåˆç†èŒƒå›´"
        
        print(f"\nâœ… åŸºçº¿æ‰§è¡Œæµ‹è¯•é€šè¿‡: å‡†ç¡®ç‡{metrics.accuracy:.2%}")
    
    def test_regression_comparison(self, tester):
        """
        æµ‹è¯•å›å½’å¯¹æ¯”åŠŸèƒ½
        
        é£é™©ç‚¹ï¼šæ— æ³•æ­£ç¡®è¯†åˆ«ç‰ˆæœ¬é€€åŒ–
        éªŒè¯ï¼šé€€åŒ–åœºæ™¯è¢«æ­£ç¡®æ ‡è®°
        """
        tester.benchmark.load_golden_dataset()
        
        # åŸºçº¿ç‰ˆæœ¬
        baseline = tester.benchmark.run_test("baseline", {})
        
        # é€€åŒ–ç‰ˆæœ¬
        degraded = tester.benchmark.run_test("degraded", {"accuracy": -0.1})
        
        # å¯¹æ¯”
        comparisons = tester.regression.compare_versions(baseline, degraded)
        
        # æ–­è¨€ï¼šå‡†ç¡®ç‡é€€åŒ–è¢«æ£€æµ‹
        accuracy_comp = next(c for c in comparisons if c.metric == "accuracy")
        assert accuracy_comp.change_percent < 0, "å‡†ç¡®ç‡é€€åŒ–æœªæ£€æµ‹"
        assert accuracy_comp.status in ["WARNING", "FAIL"], "é€€åŒ–æœªæ ‡è®°"
        
        # æ–­è¨€ï¼šé˜»å¡åˆ¤æ–­æ­£ç¡®
        should_block, reason = tester.regression.should_block_release(comparisons)
        assert should_block or any(c.status == "WARNING" for c in comparisons), "é€€åŒ–æœªè§¦å‘å‘Šè­¦"
        
        print(f"\nâœ… å›å½’å¯¹æ¯”æµ‹è¯•é€šè¿‡: å‡†ç¡®ç‡é€€åŒ–{accuracy_comp.change_percent:.1f}%")
    
    def test_release_decision(self, tester):
        """
        æµ‹è¯•å‘å¸ƒå†³ç­–é€»è¾‘
        
        é£é™©ç‚¹ï¼šé”™è¯¯å…è®¸é€€åŒ–ç‰ˆæœ¬å‘å¸ƒ
        éªŒè¯ï¼šä¸¥é‡é€€åŒ–ç‰ˆæœ¬è¢«é˜»å¡
        """
        # æ¨¡æ‹Ÿä¸¥é‡é€€åŒ–å¯¹æ¯”ç»“æœ
        mock_comparisons = [
            RegressionComparison("accuracy", 0.85, 0.75, -11.8, 5.0, "FAIL", RiskLevel.CRITICAL),
            RegressionComparison("latency_p50", 500, 600, 20.0, 20.0, "FAIL", RiskLevel.HIGH),
        ]
        
        should_block, reason = tester.regression.should_block_release(mock_comparisons)
        
        # æ–­è¨€ï¼šä¸¥é‡é€€åŒ–åº”é˜»å¡å‘å¸ƒ
        assert should_block, "ä¸¥é‡é€€åŒ–æœªé˜»å¡å‘å¸ƒ"
        assert "accuracy" in reason, "é˜»å¡åŸå› æœªæåŠå…³é”®æŒ‡æ ‡"
        
        print(f"\nâœ… å‘å¸ƒå†³ç­–æµ‹è¯•é€šè¿‡: æ­£ç¡®é˜»å¡ä¸¥é‡é€€åŒ–ç‰ˆæœ¬")
    
    def test_full_workflow(self, tester):
        """
        æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹
        
        é£é™©ç‚¹ï¼šå„ç¯èŠ‚é›†æˆå¤±è´¥
        éªŒè¯ï¼šä»æµ‹è¯•åˆ°å†³ç­–çš„å®Œæ•´æµç¨‹
        """
        results = tester.run_full_regression_test()
        
        # æ–­è¨€ï¼šæ‰€æœ‰ç‰ˆæœ¬æµ‹è¯•å®Œæˆ
        assert len(results) > 0, "æ— æµ‹è¯•ç»“æœ"
        
        # æ–­è¨€ï¼šæ¯ä¸ªç»“æœæœ‰å»ºè®®
        for result in results:
            assert "recommendation" in result, "æ— å‘å¸ƒå»ºè®®"
            assert "should_release" in result["recommendation"], "å»ºè®®ä¸å®Œæ•´"
        
        print("\nâœ… å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•é€šè¿‡")


# ä¸»æ‰§è¡Œå…¥å£
if __name__ == "__main__":
    print("\n" + "="*70)
    print("ğŸš€ AI QA System Test - Day 07: æ¨¡å‹ç‰ˆæœ¬è¿­ä»£ä¸å›å½’æµ‹è¯•")
    print("="*70)
    print("\næµ‹è¯•å†…å®¹:")
    print("  1. åŸºå‡†æµ‹è¯•å¥—ä»¶æ„å»º")
    print("  2. ç‰ˆæœ¬å¯¹æ¯”ä¸å›å½’æ£€æµ‹")
    print("  3. å‘å¸ƒå†³ç­–å»ºè®®ç”Ÿæˆ")
    print("\n" + "-"*70)
    
    # åˆ›å»ºæµ‹è¯•å™¨å¹¶æ‰§è¡Œå®Œæ•´æµ‹è¯•æµç¨‹
    tester = ModelVersionTester()
    tester.run_full_regression_test()
