# 全局大纲 (请将之前梳理的完整大纲贴入此处，作为全局上下文记忆)


# 【AI质量测试工程师 · 一站式实战项目库】完整大纲

## 1. 项目定位与核心价值

### 1.1 三位一体的产品形态

#### 1.1.1 可运行的代码库：每日1个测试视角Demo，最小可用，专注风险与行为验证

本项目的核心交付物是一个高度工程化的代码库，采用**"测试优先"的设计哲学**，彻底区别于面向开发者的AI教程。每个Daily Demo都经过精心裁剪，确保在**1小时内可完整运行并理解**，聚焦特定风险场景的行为验证，而非功能实现的完整性。

代码库的技术选型充分考虑测试工程师的技能背景：

| 技术层级 | 具体选择 | 设计 rationale |
|---------|---------|--------------|
| 主语言 | Python 3.10+ | AI工程领域事实标准，学习曲线平缓 |
| 依赖管理 | Poetry | 可靠的版本锁定，避免"依赖地狱" |
| 测试框架 | pytest + pytest-asyncio + pytest-benchmark | 支持异步AI服务测试与性能基线建立 |
| LLM适配层 | 统一封装OpenAI/Anthropic/智谱AI/通义千问 | 隐藏API差异，专注测试逻辑 |
| 本地运行 | Ollama集成开源模型 | 零外部API依赖，确保可复现性 |

每个Demo的标准结构包含：**测试目标声明**（明确验证的质量属性）、**最小可复现代码**（50-150行核心逻辑）、**预置测试数据集**（含预期输出快照）、**完整CI/CD配置**（企业自动化参考）。关键设计决策包括：所有Demo支持回归测试验证、详细的断言失败诊断信息、以及"测试思维"风格的注释——不仅解释"代码做什么"，更强调"测什么、为什么测、测到什么程度算通过"。

这种设计使得学习者无需深入模型内部机制，即可快速建立对AI系统质量风险的直观认知。例如，RAG系统测试的Demo不会完整搭建检索应用，而是提供精简的检索-生成链路，配合专门设计的测试用例，验证检索召回率、答案忠实度等关键指标的计算逻辑是否正确实现。

#### 1.1.2 可学习的课程大纲：60天阶梯式成长路径，零基础到上岗胜任

课程大纲遵循**"认知建构-技能习得-综合应用"**的三阶段学习理论，60天周期精确计算为**10周×6天/周**，每天1小时投入，专为35-45岁职场人士设计：

| 阶段 | 周期 | 核心目标 | 能力产出标识 |
|-----|------|---------|-----------|
| 认知基线建立 | 第1-2周 | 理解AI与传统软件测试的本质差异 | 能识别LLM主要失效模式 |
| 交互入口控制 | 第3周 | 掌握Prompt质量工程与安全防护 | 能设计可测试的Prompt并建立回归套件 |
| 企业核心场景 | 第4-6周 | 攻克RAG系统完整测试方法论 | 能独立负责企业级RAG应用质量保障 |
| 复杂系统验证 | 第7-8周 | 覆盖Agent智能体系统测试挑战 | 能处理多轮对话、工具调用等动态行为验证 |
| 能力整合输出 | 第9-10周 | 产出可展示的企业级测试方案 | 具备AI系统质量负责人的岗位胜任力 |

课程内容的组织突破传统"先理论后实践"模式，采用**"从问题出发"**的教学设计。第1-2周不直接讲解Transformer架构，而是通过对比实验让学习者感受同一模型在不同温度参数下的输出波动，从而理解"输出稳定性"作为测试关注点的实际意义。每个学习单元都标注明确的技能产出标识，与企业岗位能力模型直接对标。

#### 1.1.3 可沉淀的知识库：实验报告、风险点、测试思路、质量指标，直接用于企业内训

知识库采用**结构化文档模板**，将60天学习过程转化为可直接复用的企业资产：

**实验报告标准七章节**：测试目标与业务背景、测试范围与边界声明、环境配置与版本信息、测试用例设计与覆盖分析、执行结果与数据呈现、缺陷分类与风险评级、改进建议与复测计划——直接对标ISTQB国际软件测试认证体系的文档规范。

**三级风险分类体系**：

| 级别 | 定义 | 典型场景 | 响应机制 |
|-----|------|---------|---------|
| L1 阻断性 | 导致系统完全不可用或严重安全漏洞 | 模型服务崩溃、Prompt注入成功、数据泄露 | 立即阻塞上线，启动应急响应 |
| L2 高优先级 | 关键功能失效或性能严重退化 | RAG检索召回率骤降、Agent工具调用失败率超标 | 限期修复，修复前降级运行 |
| L3 一般风险 | 用户体验瑕疵或边缘场景处理不当 | 输出格式不一致、响应延迟波动 | 纳入迭代 backlog，按计划优化 |

**SMART质量指标库**：区分技术指标（检索准确率、生成BLEU分数、响应延迟P99）、业务指标（用户满意度、任务完成率、错误恢复成功率）和运营指标（测试覆盖率、缺陷逃逸率、平均修复时间），每个指标配有计算公式、Python实现代码及行业基准参考值。

### 1.2 目标用户画像

#### 1.2.1 传统软件测试工程师寻求AI时代转型

这一群体规模估计超过**200万人**（基于中国软件行业协会2024年测试从业人员统计），具备3-10年功能测试、自动化测试或性能测试经验，熟练掌握Selenium、Appium、JMeter等传统工具。他们面临严峻的职业挑战：传统测试方法难以直接迁移到概率性输出的AI系统，企业内部缺乏系统的AI测试培训体系，碎片化学习效率低下。

本项目的针对性设计包括：**"最小算法负担"原则**，刻意回避神经网络架构、损失函数优化等底层数学内容，将学习曲线聚焦于工程实践层面；**经验锚定策略**，大量引用熟悉的传统测试场景（如电商订单系统、银行核心交易），通过对比展示AI测试的差异点；以及**即时反馈机制**，每个学习单元都有可立即运行的代码和可见输出结果，避免长时间投入后的不确定性焦虑。

#### 1.2.2 QA/运维工程师规避编码内卷风险

QA与运维工程师的核心诉求是**保持技术竞争力同时避免深度编码竞争**。他们的既有优势在于系统思维、流程优化、风险管控，而非算法实现。本项目明确区分"AI系统质量"与"模型质量"两个层次，将编码要求严格限制在"能读懂、能修改、能组装"的程度。

关键适配设计：**配置驱动测试工具链**，如Agent测试模块使用LangChain的LCEL声明式语法，学习曲线远低于传统编程；**低代码/无代码选项**，基于Web界面的Prompt测试工作台、可视化RAG流程调试器、预配置监控仪表盘模板；以及**能力迁移路径**，将流程审计、SLI/SLO定义、事件响应等既有经验直接映射到AI场景，形成差异化竞争优势。

#### 1.2.3 35–45岁大龄技术人保命型技能升级

这一群体的刚性约束包括：时间碎片化严重（家庭责任压缩可支配时间）、学习速度下降（相比年轻从业者）、以及强烈的"确定性回报"需求（不愿进行高风险的能力投资）。60天周期、每日1小时、明确岗位对标的"三明确"设计直接回应这些约束。

特别适配机制：**"快速变现"导向**，每个阶段都有可直接用于工作的产出物（测试报告、评估方案、监控配置）；**社交学习支持**，同龄人组成的互助学习群体缓解孤独感；**可验证产出**，结业作品直接作为求职作品集，形成正向激励循环。项目定位"保命型技能升级"——不是追求技术理想，而是确保在劳动力市场中保持可雇佣性。

### 1.3 差异化赛道定位

#### 1.3.1 不教造AI，只教测稳AI、控风险、保上线

这一口号式定位精准区分于市场上泛滥的AI开发课程。当前AI培训市场被"AI开发工程师""大模型算法专家"等方向主导，学习周期6-12个月，对数学基础要求极高。本项目反其道而行，锁定AI系统的**"下游"环节——质量保障与风险控制**。

**九字价值主张的具体内涵**：

| 关键词 | 技术内涵 | 企业痛点回应 |
|-------|---------|-----------|
| 测稳AI | 输出一致性验证、服务可用性保障、性能基线建立 | 模型输出波动导致用户体验不可预期 |
| 控风险 | 幻觉检测、安全漏洞扫描、合规性审查 | AI事故的品牌声誉损失与法律责任 |
| 保上线 | 质量报告撰写、风险评级、发布决策支持 | 技术团队与管理层之间的信息鸿沟 |

这一选择的商业逻辑基于三重判断：AI开发人才供给快速增加而质量人才缺口未被认知；AI质量风险正从"技术问题"升级为"商业问题和合规问题"；测试/QA背景的学习者在质量思维、风险意识、流程规范方面有先发优势，转型路径比从零学习AI开发更为顺畅。

#### 1.3.2 以AI系统质量为主线，模型质量为辅线

**能力建设的资源分配比例**：模型质量约占30%（第1-2周基础认知阶段），AI系统质量占据70%（第3-10周全部内容）。这一划分基于对企业AI项目失败原因的深入分析——大量问题源于系统集成、数据管道、运维监控等环节，而非单点模型精度。

**模型质量的学习边界**：明确设定为**"能识别问题，不做算法研究员"**。涵盖准确性评估（混淆矩阵、F1分数、AUC-ROC）、稳定性验证（温度参数影响、版本回归测试）、幻觉检测（事实一致性检查、知识边界探测）、鲁棒性测试（同义改写、噪声注入）、以及基础对抗Prompt识别。学习目标是通过标准化方法和工具发现问题、与算法团队有效沟通，而非深入诊断和修复。

**AI系统质量的核心深度**：覆盖流程质量（数据管道可靠性、特征工程稳定性）、稳定性与可靠性（SLO设定与验证、降级策略、灾难恢复）、性能与并发（延迟分布分析、GPU资源调度、超时控制）、安全与合规（Prompt注入防护、数据泄露检测、审计追踪）、工具调用正确性（Agent系统中外部工具调用的参数准确性、结果处理鲁棒性）、决策风险（多步推理过程中的错误传播与恢复）、以及可观测性（日志结构化、指标采集、告警策略）。这些能力直接对应**"企业稀缺的AI系统质量负责人"**岗位需求。

#### 1.3.3 培养企业稀缺的AI系统质量负责人

这一角色的典型工作场景与能力要求：

| 工作场景 | 核心能力 | 课程对应模块 |
|---------|---------|-----------|
| 参与AI产品需求评审 | 从质量角度提出可测试性建议 | 第3周 Prompt可测试性设计 |
| 设计AI系统测试策略 | 确定测试重点和资源分配 | 第4-6周 RAG分层测试方法论 |
| 搭建AI测试工具链 | 自动化流水线与持续集成 | 第6周 Ragas框架与LLM-as-a-Judge |
| 执行关键质量验证 | 模型评估、系统集成测试、性能压测 | 第7-8周 Agent核心能力验证 |
| 撰写质量报告 | 向技术团队和管理层沟通风险 | 第9-10周 分层汇报演练 |
| 推动质量问题闭环 | 跨团队协作与持续改进 | 全程项目实战与社区互助 |

结业作品的三个可选方向分别对应三类典型工作场景：**特定行业RAG系统完整测试方案**（展示深度专业能力）、**Agent系统稳定性测试框架**（展示前沿技术掌握）、**企业AI质量体系搭建路线图**（展示战略思维与影响力）。

## 2. 目录架构设计原则

### 2.1 三层嵌套结构

#### 2.1.1 大结构：5大阶段（按能力递进）

五阶段划分遵循**能力形成的自然规律**和**AI质量测试领域的知识依赖关系**：

**阶段一：LLM基础与质量感知（第1-2周）**——建立共同认知基础，消除对AI的神秘感，理解AI系统的可测试性特征。核心产出：能够用专业语言描述AI系统的质量特征，识别明显的质量问题。

**阶段二：Prompt工程与测试（第3周）**——聚焦人机交互入口，Prompt设计质量直接决定系统输出上限。核心产出：能够独立设计并评估业务场景的Prompt质量，建立Prompt级别的质量门禁。

**阶段三：RAG系统与检索质量（第4-6周）**——攻克企业AI应用最主流架构，投入最多时间深入掌握。核心产出：能够独立设计并执行企业级RAG系统的完整测试方案。

**阶段四：Agent & LangChain测试（第7-8周）**——面向AI应用发展方向，应对复杂智能体系统的测试挑战。核心产出：能够对复杂Agent系统进行探索性测试和风险识别，建立针对动态AI系统的测试策略。

**阶段五：综合实战与能力认证（第9-10周）**——整合所学知识，产出可展示的职业能力证明。核心产出：一套完整的、可直接用于求职的专业能力证明。

阶段之间的**严格依赖关系**：第一阶段的基础认知是后续所有模块的前置条件；第二阶段的Prompt技能直接服务于第三、四阶段；第三阶段的RAG测试经验为第四阶段的Agent测试奠定基础（典型Agent系统包含RAG组件）。这种设计确保学习投资的复利效应，避免知识跳跃导致的认知负荷过载。

#### 2.1.2 中结构：技术模块（按知识点聚合）

技术模块划分遵循**"高内聚、低耦合"**原则，每个模块聚焦相对独立的技术领域：

| 阶段 | 模块 | 天数 | 核心主题 |
|-----|------|------|---------|
| 一 | LLM核心概念 | 4天 | 工作原理、Tokenizer、采样参数、版本管理 |
| 一 | 模型质量基础 | 3天 | 幻觉检测、鲁棒性测试、对抗Prompt识别 |
| 二 | Prompt质量工程 | 4天 | 结构设计、Few-shot测试、安全边界、A/B测试 |
| 三 | 检索层质量保障 | 6天 | 文档评分、分块策略、Embedding选型、混合检索 |
| 三 | 生成层质量评估 | 5天 | 忠实度、答案相关性、上下文利用率 |
| 三 | 端到端RAG系统测试 | 5天 | Ragas框架、LLM-as-a-Judge、A/B测试、线上监控 |
| 四 | Agent核心能力验证 | 5天 | 工具调用、状态管理、决策可追溯性 |
| 四 | LangChain系统稳定性 | 5天 | 异常处理、并发控制、记忆模块测试 |
| 四 | AI测试助手实战 | 2天 | 构建测试数据生成Agent |
| 五 | 企业级综合Demo | 5天 | 端到端测试方案设计与执行 |
| 五 | 质量报告与沟通 | 5天 | 报告撰写、分层汇报演练 |

模块内部采用**"原理-工具-实践"**三段式结构，确保既理解"为什么"，又掌握"用什么"，更能落地"怎么做"。

#### 2.1.3 小结构：按天目录（按执行进度落地）

每日任务的标准结构：

| 元素 | 时长 | 内容说明 |
|-----|------|---------|
| 学习目标声明 | 2分钟 | "学完今天的内容，你将能够..." |
| 前置知识检查 | 3分钟 | 确保学习路径连续性 |
| 核心内容讲解 | 15-20分钟 | 配合代码示例和图示 |
| 动手实验任务 | 30-35分钟 | 明确步骤和预期结果 |
| 总结反思 | 5-10分钟 | 学习笔记与疑问记录 |
| 扩展阅读材料 | 可选 | 供学有余力者深入 |

**时间校准与弹性设计**：每个Daily Demo确保在标准1小时内可完成核心流程。对于复杂度较高的主题（如RAG端到端评估），采用"分解-迭代"策略——第一天搭建基础框架，第二天扩展多指标覆盖，第三天集成自动化流水线。"最小完成模式"支持时间受限场景：跳过扩展内容，确保核心知识不遗漏，后续通过"补完清单"追回。

### 2.2 核心功能诉求

#### 2.2.1 零基础用户：明确每日任务边界

针对"不知道从哪里开始"和"不知道学到什么程度算够"的痛点，按天目录提供：

- **清晰的完成标志**：通常是成功运行测试套件并生成报告
- **自检查清单**：量化确认掌握程度
- **常见问题预判**：基于内测反馈的痛点汇总
- **环境准备日（Day 0）**：三平台详细配置指南，GitHub Codespaces一键启动选项

**脚手架教学法**：前两周提供最高程度的结构化支持（详细步骤说明、预设代码模板、常见错误预判），随着学习进展逐步减少支持，到第7-8周时仅提供任务描述与验收标准，鼓励自主探索。

#### 2.2.2 有经验用户：快速定位能力缺口

支持"按需查阅"模式的多维索引：

| 索引维度 | 典型使用场景 |
|---------|-----------|
| 技术主题索引 | "我需要补强RAG评估能力" |
| 能力维度索引 | "我需要学习性能测试方法" |
| 工具类型索引 | "我需要掌握Ragas框架使用" |
| 场景应用索引 | "我需要客服机器人测试案例" |

**能力自评工具**：在线问卷覆盖60个核心技能点，生成个性化学习路径建议——已掌握内容标记为可跳过，薄弱内容标注优先级和资源直达链接。自评结果可导出为技能雷达图，用于求职时的能力可视化展示。

#### 2.2.3 企业/面试官：一眼识别体系产出价值

**能力产出可视化设计**：

| 学习阶段 | 交付物类型 | 岗位能力对标 |
|---------|-----------|-----------|
| 第2周结束 | 模型质量评估报告 | AI测试工程师（初级） |
| 第3周结束 | Prompt测试套件 | Prompt质量专员 |
| 第6周结束 | RAG系统测试方案 | RAG应用测试工程师 |
| 第8周结束 | Agent稳定性框架 | 智能体测试专家 |
| 第10周结束 | 企业级质量报告+结业作品 | AI系统质量负责人 |

**企业内训材料标注**：明确指示哪些内容可直接用于内部培训，包括PPT源文件、实验环境Docker镜像、考核题库等，降低采购后的落地成本。

### 2.3 多场景适配性

#### 2.3.1 支撑图书出版（10章结构）

10章结构与10周学习周期天然对应，每章包含：

- **学习地图图示**：展示本章在整体能力体系中的位置
- **本章目标清单**：量化产出预期
- **理论讲解-代码示例-实战案例-习题巩固**四段式结构
- **章节小结**与**延伸阅读**

**代码呈现策略**：核心算法和关键配置以完整代码块呈现，辅助函数和样板代码通过GitHub仓库引用，避免大段代码堆砌。彩印版本强化代码语法高亮和图示色彩区分。

#### 2.3.2 支撑在线课程（每日解锁机制）

**游戏化学习设计**：

| 机制 | 具体实现 |
|-----|---------|
| 每日解锁 | 完成当日测验解锁次日内容，弹性锁定防止过早跳过 |
| 进度可视化 | 学习进度条、能力成长雷达图 |
| 社群打卡 | 5-8人小组每日互相监督 |
| 直播答疑 | 每周末2小时集中解答+扩展分享 |
| 项目评审 | 第五阶段升级为一对一作品反馈 |

**社交学习元素**：学习小组、周度直播、项目互评，同时保留关闭选项适应不同学习风格。

#### 2.3.3 支撑开源社区（Star驱动增长）

**GitHub仓库运营策略**：

| Star里程碑 | 解锁内容 | 运营目标 |
|-----------|---------|---------|
| 100 Star | 前两周内容完整开源 | 建立初始口碑 |
| 500 Star | 第一阶段全部内容 | 验证市场需求 |
| 2000 Star | 60天完整代码开放 | 形成社区规模 |
| 5000 Star | 英文版翻译启动 | 国际化扩展 |
| 10000 Star | 企业级扩展案例持续更新 | 长期生态建设 |

**贡献者友好设计**：Issue模板（学习反馈/代码问题/内容建议）、详细贡献指南、代码规范与PR流程说明，鼓励行业特定场景测试案例的社区提交。

#### 2.3.4 支撑企业采购（内训材料即拿即用）

**灵活部署选项**：

| 版本 | 时长 | 内容组合 | 适用场景 |
|-----|------|---------|---------|
| 2天浓缩版 | 16小时 | 各阶段核心模块精选 | 技术管理者认知升级 |
| 5天速成版 | 40小时 | 模块一至六完整内容 | 测试团队核心能力建设 |
| 完整10周版 | 60小时+ | 全部内容+项目实战 | AI质量团队体系化培养 |
| 行业定制版 | 灵活 | 替换案例库+增加法规解读 | 金融/医疗/法律等垂直领域 |

**企业专属服务**：需求诊断、学习督导、效果评估全流程支持，知识产权清晰（代码Apache 2.0开源，课程内容商业授权，支持白标定制）。

## 3. 60天学习进度总览

### 3.1 时间节奏设计

#### 3.1.1 每日投入：1小时专注学习

**时间分配的科学依据**：

| 时段 | 时长 | 活动 | 认知科学依据 |
|-----|------|------|-----------|
| 前段 | 20-25分钟 | 视频/阅读学习 | 成人单次专注时长约45-90分钟，1小时处于最佳区间下限 |
| 中段 | 30-35分钟 | 代码跟随实践 | 动手操作促进深度加工，符合"做中学"理论 |
| 后段 | 5-10分钟 | 总结反思 | 间隔效应支持长期记忆巩固 |

**灵活性设计**：支持"碎片化学习"模式（视频通勤观看、代码专注时段完成、反思睡前进行），系统记录进度支持跨设备切换。"最小完成模式"应对时间超支，确保核心知识不遗漏。

#### 3.1.2 每周周期：6天学习+1天休息

**恢复日设计原则**：

| 功能 | 具体安排 |
|-----|---------|
| 知识沉淀 | 本周内容回顾、扩展阅读、社区交流 |
| 弹性缓冲 | 用于补课或处理突发工作家庭事务 |
| 维持连续性 | 系统推送"本周精华"总结，防止学习节奏断裂 |

周与周衔接：**周日"上周回顾+本周预览"**，建立情境连续性；**周六内容相对轻松**（案例分析与最佳实践分享），为休息日前的正向收尾。

#### 3.1.3 总跨度：10周完成转型

**周期设计的平衡考量**：

| 对比参照 | 周期 | 定位差异 |
|---------|------|---------|
| O'Reilly AI Engineering Bootcamp | 2天密集 | 全职沉浸，不适合在职 |
| Microsoft AI Engineer认证 | 6个月 | 过于漫长，机会成本过高 |
| 黑马程序员AI培训班 | 5个月左右 | 算法导向，非测试专项 |
| **本项目** | **10周（2.5个月）** | **在职转型最优平衡点** |

10周与**企业季度规划周期**自然契合，学习成果可及时转化为工作绩效。结业产出水平对应"AI测试工程师"或"AI质量保障专员"初级岗位胜任力，或传统测试团队中的"AI专项负责人"角色。

### 3.2 五阶段能力跃迁

#### 3.2.1 第1–2周：LLM基础与质量感知（建立AI测试认知基线）

**核心目标**：完成从"确定性测试思维"到"概率性质量思维"的范式转换。

**关键洞察**：传统软件测试追求"给定输入必有预期输出"，AI测试管理"输出分布的合理边界"。

**两周内容详解**：

| 天数 | 主题 | 核心实验 | 关键产出 |
|-----|------|---------|---------|
| Day 1-2 | LLM工作原理与测试视角 | 温度参数对比实验、知识边界探测 | 模型行为观察报告 |
| Day 3-4 | Tokenizer与上下文窗口 | Token计数准确性测试、窗口边界压力测试 | 输入边界测试用例集 |
| Day 5-6 | 采样参数与输出稳定性 | 温度×Top-p参数网格扫描、时间维度一致性验证 | 参数-质量响应曲面图 |
| Day 7-8 | 模型版本迭代与回归测试 | 基准测试套件构建、A/B测试实验设计 | 版本管理策略文档 |
| Day 9-10 | 幻觉检测与事实一致性 | NLI-based忠实度评分、知识图谱验证 | 幻觉检测方法对比报告 |
| Day 11-12 | 输出鲁棒性测试 | 同义改写测试、噪声注入压力测试 | 鲁棒性评估流水线 |
| Day 13-14 | 基础对抗Prompt识别 | 直接/间接/多轮注入攻击复现、防御策略评估 | 安全评估报告 |

**阶段产出**：能够针对简单LLM应用（文本摘要、情感分析）设计并执行基础质量评估方案，建立"AI系统需要专门质量保障方法"的核心认知。

#### 3.2.2 第3周：Prompt工程与测试（掌握人机交互质量入口）

**核心目标**：将Prompt工程重新定义为"可测试性设计"，建立Prompt级别的质量门禁。

**独特视角**：不仅关注Prompt的"效果"，更关注其"稳定性"与"安全性"，以及版本管理和A/B测试的工程实践。

**周内容详解**：

| 天数 | 主题 | 核心方法 | 关键产出 |
|-----|------|---------|---------|
| Day 15-16 | Prompt结构设计与可测试性 | CO-STAR框架、角色-任务-约束模板、确定性增强技术 | 可测试Prompt重构案例 |
| Day 17-18 | Few-shot示例选择与效果稳定性 | 示例扰动测试、数量敏感性分析、跨模型迁移测试 | 最小有效示例集 |
| Day 19-20 | 系统Prompt安全边界与注入风险 | 直接/间接/多轮注入攻击识别、纵深防御策略 | 安全评估报告 |
| Day 21 | Prompt版本管理与A/B测试 | Git-based版本控制、效果对比框架、上线决策流程 | Prompt测试套件 |

**阶段产出**：能够独立设计并评估业务场景的Prompt质量，建立Prompt级别的质量门禁，具备与算法团队沟通Prompt相关问题的专业语言。

#### 3.2.3 第4–6周：RAG系统与检索质量（攻克企业级AI应用核心场景）

**核心目标**：掌握当前企业AI应用最主流架构的完整测试方法论，投入最多时间（3周=18天）确保深度。

**三周分层递进**：

**第4周：检索层质量保障**

| 天数 | 主题 | 核心技术 | 行业实践参考 |
|-----|------|---------|-----------|
| Day 22-24 | 文档质量评分与预处理 | 格式规范性、内容完整性、时效性、语义清晰度评估 | 火山引擎"文档质量检测优先"经验 |
| Day 25-27 | 分块策略评估 | 固定长度/语义/递归/Agentic分块对比，召回率优化 | LangChain RecursiveCharacterTextSplitter默认推荐 |
| Day 28-30 | Embedding模型选型 | MTEB评测体系、领域适配性验证、向量相似度校准 | 智谱AI/通义千问等国产模型对比 |
| Day 31-33 | 混合检索架构 | 关键词+语义+重排序效果对比，权重调优与级联策略 | NVIDIA Reranker实践 |

**第5周：生成层质量评估**

| 天数 | 主题 | 核心指标 | 自动化方法 |
|-----|------|---------|-----------|
| Day 34-36 | 忠实度（Faithfulness）自动化检测 | 基于NLI的entailment检测、基于问答的事实验证、基于声明抽取的精细分析 | Ragas Faithfulness指标实战 |
| Day 37-39 | 答案相关性（Answer Relevance）多维度评估 | 主题相关性、意图相关性、完整相关性、时效相关性 | 嵌入相似度+LLM打分结合 |
| Day 40-42 | 上下文利用率与信息压缩效率 | 信息利用不足/过载检测、父文档检索器策略、窗口利用效率评估 | 注意力分析+对比实验 |

**第6周：端到端RAG系统测试**

| 天数 | 主题 | 核心工具/方法 | 企业级实践 |
|-----|------|------------|-----------|
| Day 43-45 | Ragas框架实战与自定义指标 | 五大核心指标（Context Precision/Recall/Relevancy, Faithfulness, Answer Relevancy）、指标扩展开发 | 框架深度定制 |
| Day 46-48 | LLM-as-a-Judge评估流水线 | 评委模型选型、评估Prompt设计、偏见识别与缓解 | 成本-质量权衡优化 |
| Day 49-50 | A/B测试与线上效果监控 | 流量分配、统计显著性、早期停止规则、Embedding漂移检测 | 生产环境部署 |

**阶段产出**：能够独立设计并执行企业级RAG系统的完整测试方案，覆盖检索质量、生成质量、系统稳定性三个层次，产出专业的质量评估报告。

#### 3.2.4 第7–8周：Agent & LangChain测试（覆盖智能体系统复杂性）

**核心目标**：应对AI系统向自主智能体演进带来的新测试挑战——状态空间爆炸、决策路径不可预测、工具调用链复杂。

**两周内容详解**：

**第7周：Agent核心能力验证**

| 天数 | 主题 | 测试挑战 | 验证方法 |
|-----|------|---------|---------|
| Day 51-53 | 工具调用正确性与参数解析 | 工具选择错误、参数格式错误、结果解析失败 | 静态分析+动态执行+故障注入 |
| Day 54-56 | 多轮对话状态管理与上下文一致性 | 历史信息丢失、用户意图变化、外部事件干扰 | 状态转换图覆盖+关键信息持久性验证 |
| Day 57-59 | Agent决策路径可追溯性与可解释性 | 推理过程黑箱、关键决策依据缺失、错误传播难以定位 | Chain-of-Thought记录+归因分析+审计日志 |

**第8周：LangChain系统稳定性与AI测试助手实战**

| 天数 | 主题 | 工程实践 | 测试重点 |
|-----|------|---------|---------|
| Day 60-62 | 链式流程异常处理与降级机制 | 组件失败熔断、备用路径切换、优雅降级策略 | 故障注入测试、恢复时间验证 |
| Day 63-65 | 并发场景下的资源竞争与超时控制 | 连接池管理、请求队列、优先级调度、GPU显存优化 | 负载测试、瓶颈定位、容量规划 |
| Day 66-68 | 记忆模块持久化与数据隔离 | 长期记忆存储、用户数据隔离、敏感信息脱敏 | 隐私合规测试、存储性能验证 |
| Day 69-70 | AI测试助手实战 | 构建测试数据生成Agent | "用AI测试AI"的元能力整合 |

**阶段产出**：能够对复杂Agent系统进行探索性测试和风险识别，建立针对动态AI系统的测试策略，具备处理工具调用、状态管理、决策追溯等高级测试场景的能力。

#### 3.2.5 第9–10周：综合实战与能力认证（产出可展示的能力证明）

**核心目标**：整合分散技能，完成从"学习者"到"实践者"的身份转换，产出可直接用于求职或晋升的能力证明。

**两周实战安排**：

**第9周：企业级综合Demo**

| 天数 | 任务 | 覆盖维度 | 产出要求 |
|-----|------|---------|---------|
| Day 71-73 | 端到端测试方案设计 | 需求分析、风险识别、策略制定、资源规划 | 测试计划文档 |
| Day 74-75 | 测试执行与数据收集 | 功能测试、性能压测、安全扫描、可观测性集成 | 原始数据包 |
| Day 76-78 | 性能压测深度实践 | 负载生成、资源监控、瓶颈定位、优化建议 | 性能基准报告 |
| Day 79-80 | 安全扫描与可观测性集成 | OWASP LLM Top 10覆盖、监控仪表盘搭建 | 安全评估报告+监控配置 |

**第10周：质量报告与沟通认证**

| 天数 | 任务 | 受众适配 | 核心技能 |
|-----|------|---------|---------|
| Day 81-83 | 质量评估报告撰写 | 技术团队版/管理层版/业务方版 | 分层信息组织、数据可视化 |
| Day 84-85 | 风险评级与上线建议 | 决策支持框架、不确定性下的判断 | 风险量化、决策树分析 |
| Day 86-88 | 向技术团队汇报演练 | 详细技术细节、复现步骤、修复建议 | 技术影响力 |
| Day 89-90 | 向管理层/业务方汇报演练 | 业务影响、资源需求、决策建议 | 商业沟通、战略思维 |

**结业作品三选一**：

| 方向 | 适合人群 | 核心产出 | 职业定位 |
|-----|---------|---------|---------|
| A. 特定行业RAG系统完整测试方案 | 专注垂直领域者 | 行业定制化测试方案（金融/医疗/法律/电商） | 领域测试专家 |
| B. Agent系统稳定性测试框架 | 平台型团队倾向者 | 可复用的测试基础设施+自动化流水线 | 测试架构师 |
| C. 企业AI质量体系搭建路线图 | 管理发展倾向者 | 从0到1的质量策略+组织流程+工具链规划 | 质量负责人 |

**最终认证**：通过作品评审的学习者获得"AI系统质量负责人"能力徽章，优秀作品入选项目案例库，作者获得社区荣誉和推荐机会。

## 4. 第一阶段：LLM基础与质量感知（第1–2周）

### 4.1 模块一：大语言模型核心概念

#### 4.1.1 Day 1–2：LLM工作原理与测试视角的模型理解

**学习目标**：建立对LLM作为"黑盒系统"的测试工程师视角理解，而非算法工程师的实现细节。

**核心认知转变**：

| 传统软件测试 | AI系统测试 |
|-----------|-----------|
| 确定性输出：给定输入必有预期结果 | 概率性输出：相同输入可能产生不同结果 |
| 功能正确性二元判断（通过/失败） | 质量分布评估（可接受范围内的波动） |
| 状态机模型：明确的状态转换 | 上下文敏感：输出依赖完整输入历史 |
| 边界值分析覆盖关键路径 | 场景覆盖+统计置信度评估 |

**关键实验：温度参数与输出多样性**

```python
# 核心测试逻辑示意
def test_temperature_stability(prompt, model, temperatures=[0.0, 0.3, 0.7, 1.0], n_samples=20):
    results = {}
    for temp in temperatures:
        outputs = [call_llm(prompt, model, temperature=temp) for _ in range(n_samples)]
        # 计算语义多样性：嵌入向量平均余弦距离
        diversity = compute_semantic_diversity(outputs)
        # 计算事实一致性：与参考答案的匹配率（如适用）
        consistency = compute_factual_consistency(outputs, reference)
        results[temp] = {'diversity': diversity, 'consistency': consistency}
    return results
```

**关键发现记录**：温度=0并非真正确定性（某些实现仍有微小随机性）；极低温度下的"贪婪解码"可能与预期行为有微妙差异；创意任务与事实查询任务的最优温度参数显著不同。

**阶段产出**：温度-多样性关系曲线图、知识边界地图（按主题分类的准确率热力图）、输入敏感性矩阵（不同变换类型的一致性分数）。

#### 4.1.2 Day 3–4：Tokenizer、上下文窗口与输入边界测试

**Tokenizer测试的核心洞察**：不同Tokenizer的编码差异直接影响成本估算和上下文规划。BPE（GPT系列）与SentencePiece（T5、LLaMA）对中文处理的压缩效率差异显著，相同汉字数可能占用差异达30-50%的token配额。

**上下文窗口测试设计**：

| 测试类型 | 构造方法 | 验证目标 |
|---------|---------|---------|
| 边界长度测试 | 精确构造窗口长度±1 token的输入 | 截断行为验证（头部/尾部/摘要截断） |
| 关键信息位置测试 | 在开头/1/4/1/2/3/4/结尾放置测试信息 | "Lost in the Middle"现象量化 |
| 长文本退化测试 | 逐步增加输入长度，记录输出质量 | 有效上下文长度 vs 标称窗口大小 |
| 多语言混合测试 | 中英日韩等语言混合的长输入 | 编码边界处理的鲁棒性 |

**实验任务**：使用tiktoken等工具分析不同文本的token消耗，构建边界测试用例集，对比不同模型在相同上下文长度下的实际表现差异。

#### 4.1.3 Day 5–6：温度参数、Top-p采样与输出稳定性验证

**参数空间探索的系统方法**：固定Prompt，系统扫描温度×Top-p的参数网格，记录输出质量指标变化，绘制响应曲面图。

| 参数组合 | 适用场景 | 风险点 |
|---------|---------|--------|
| 低温度(0.0-0.3) + 低Top-p(0.1-0.5) | 事实查询、代码生成、结构化输出 | 创造性不足、对输入措辞过度敏感 |
| 中温度(0.5-0.7) + 中Top-p(0.7-0.9) | 通用对话、内容创作、头脑风暴 | 输出一致性较难保证 |
| 高温度(>1.0) + 高Top-p(>0.95) | 创意写作、艺术生成、探索性任务 | 幻觉风险显著增加、 coherence 下降 |

**输出稳定性验证的时间维度**：同一参数配置在不同时间点的输出一致性（检测模型版本更新或基础设施变化的影响），以及长期运行中的性能漂移检测。

#### 4.1.4 Day 7–8：模型版本迭代与回归测试策略

**模型版本管理的复杂性**：API版本、模型快照、训练数据截止时间、安全补丁级别等多个维度需要同时追踪。

**基准测试套件设计原则**：

| 维度 | 设计要点 |
|-----|---------|
| 覆盖全面性 | 核心功能、边界场景、已知风险、回归历史 |
| 执行成本 | 平衡覆盖深度与运行时间，支持分层执行 |
| 通过标准 | 允许一定范围内的波动，而非绝对一致 |
| 失败诊断 | 区分预期变化、回归缺陷、测试本身问题 |

**A/B测试的统计基础**：样本量计算、显著性水平选择、功效分析、多重比较校正。实际案例：新版本整体指标略有提升，但在特定用户群体（非英语母语者）上表现下降，如何影响上线决策。

### 4.2 模块二：模型质量基础识别

#### 4.2.1 Day 9–10：幻觉检测与事实一致性评估方法

**幻觉分类与检测策略**：

| 类型 | 定义 | 检测方法 | 适用场景 |
|-----|------|---------|---------|
| 事实性幻觉 | 与客观事实不符 | 知识图谱验证、网络搜索核查、领域专家审核 | 新闻、医疗、金融等高风险领域 |
| 忠实性幻觉 | 与输入上下文矛盾 | NLI-based entailment检测、基于问答的验证 | RAG系统、文档摘要 |
| 内在幻觉 | 模型输出自相矛盾 | 多输出一致性检查、逻辑矛盾检测 | 长文本生成、多步推理 |

**Ragas Faithfulness指标实战**：基于声明抽取和上下文验证的自动化评分，以及其局限性分析（对隐含推理的检测不足、对模糊表述的判断困难）。

#### 4.2.2 Day 11–12：输出鲁棒性测试（同义改写、噪声注入）

**同义改写测试的技术实现**：

| 变换类型 | 实现方法 | 预期行为 | 常见脆弱点 |
|---------|---------|---------|-----------|
| 词汇层面 | 词向量近义词替换 | 核心语义保持 | 领域术语替换导致专业性下降 |
| 句法层面 | 主动被动转换、语序调整 | 命题内容不变 | 否定词位置变化导致逻辑反转 |
| 语篇层面 | 段落重组、衔接词替换 | 整体意义等价 | 长距离依赖关系的破坏 |
| 跨语言 | 翻译往返（中文→英文→中文） | 语义高度一致 | 文化特定概念的丢失 |

**噪声注入的压力测试设计**：从轻微噪声（5%随机字符替换）到极端破坏（50%替换+乱码插入），记录模型性能的衰减曲线，识别失效临界点。

#### 4.2.3 Day 13–14：基础对抗Prompt识别与防御意识

**攻击模式与防御策略**：

| 攻击类型 | 技术特征 | 防御层次 |
|---------|---------|---------|
| 直接注入 | 用户输入中嵌入恶意指令覆盖系统Prompt | 输入过滤、意图分类、输出监控 |
| 间接注入 | 通过外部数据源（网页、文档）植入攻击载荷 | 数据源白名单、内容净化、沙箱执行 |
| 多轮诱导 | 通过对话历史逐步引导模型突破限制 | 会话状态监控、异常行为检测、人工审核触发 |
| 目标劫持 | 使模型执行攻击者指定的未授权任务 | 权限最小化、能力边界声明、行为审计 |

**红队测试实战**：对给定应用进行系统的安全评估，尝试发现漏洞并设计修复方案，产出安全评估报告。

## 5. 第二阶段：Prompt工程与测试（第3周）

### 5.1 模块三：Prompt质量工程

#### 5.1.1 Day 15–16：Prompt结构设计与可测试性原则

**可测试Prompt的设计要素**：

| 要素 | 具体技术 | 测试价值 |
|-----|---------|---------|
| 确定性增强 | Few-shot示例、输出格式强制、约束条件明确 | 降低随机性，提高回归测试可靠性 |
| 边界明确 | 任务范围声明、超出范围的处理指令 | 便于设计边界值测试用例 |
| 评估接口 | 自评估请求（"请评估你的回答质量"）、结构化输出 | 支持自动化质量监控 |
| 版本标识 | Prompt版本号、变更记录 | 支持A/B测试和回归分析 |

**CO-STAR框架应用**：Context（背景）、Objective（目标）、Style（风格）、Tone（语气）、Audience（受众）、Response（响应格式）——每个维度的设计选择对输出质量的影响测试。

#### 5.1.2 Day 17–18：Few-shot示例选择与效果稳定性测试

**示例选择的系统性方法**：

| 考量维度 | 测试方法 | 优化目标 |
|---------|---------|---------|
| 与目标任务相似度 | 嵌入空间距离计算 | 最大化迁移效果 |
| 难度梯度 | 从简单到复杂的示例排序 | 促进渐进式学习 |
| 多样性覆盖 | 聚类分析确保场景覆盖 | 避免偏见和过拟合 |
| 潜在偏见 | 敏感属性平衡检查 | 公平性保障 |

**效果稳定性测试**：示例扰动测试（替换、删除、重排）、数量敏感性分析（0-shot到10-shot的效果曲线）、跨模型迁移测试。

#### 5.1.3 Day 19–20：系统Prompt安全边界与注入风险扫描

**纵深防御策略的具体实现**：

| 层级 | 控制措施 | 测试验证 |
|-----|---------|---------|
| 输入层 | 关键词过滤、模式匹配、意图分类器 | 绕过测试集、误报率评估 |
| 模型层 | 系统Prompt加固、安全微调、能力边界声明 | 越狱尝试、权限提升测试 |
| 输出层 | 内容审核API、异常模式检测、响应后处理 | 漏检率、延迟影响 |
| 架构层 | 权限隔离、人工审核触发、行为审计 | 端到端渗透测试 |

#### 5.1.4 Day 21：Prompt版本管理与A/B测试框架

**Prompt即代码的工程实践**：Git版本控制、变更影响分析、自动化回归测试、灰度发布策略。

## 6. 第三阶段：RAG系统与检索质量（第4–6周）

### 6.1 模块四：检索层质量保障

#### 6.1.1 Day 22–24：文档质量评分与预处理流程测试

**文档质量多维度评估**：

| 维度 | 评估指标 | 自动化方法 |
|-----|---------|-----------|
| 格式规范性 | 结构完整性、编码正确性、元数据完整度 | 规则检查、Schema验证 |
| 内容完整性 | 信息缺失检测、断章取义识别 | 章节连续性分析、引用完整性检查 |
| 语义清晰度 | 语言准确性、歧义检测、可读性评分 | 困惑度计算、语法分析 |
| 时效性 | 版本日期、更新频率、过期内容识别 | 时间戳提取、内容新鲜度模型 |

#### 6.1.2 Day 25–27：分块策略评估（固定长度/语义/递归/Agentic）

**四种分块策略的对比测试**：

| 策略 | 核心思想 | 优势场景 | 测试重点 |
|-----|---------|---------|---------|
| 固定长度 | 按预定token数分割，设置重叠 | 实现简单、处理快速、结构均匀文档 | 语义连贯性、边界信息丢失 |
| 语义分块 | 检测语义相似度变化确定分割点 | 叙事性内容、主题明确文档 | 阈值敏感性、处理速度 |
| 递归分块 | 分层分割，从大到小逐步细化 | 复杂结构文档、平衡效率与质量 | 层级一致性、参数调优 |
| Agentic分块 | 利用LLM智能识别结构和关键信息 | 高度复杂文档、最优信息组织 | 成本效益、可解释性、失败模式 |

**关键发现**：语义分块可将召回率提升高达9%（据行业研究），LangChain的RecursiveCharacterTextSplitter被推荐为新系统默认选择。

#### 6.1.3 Day 28–30：Embedding模型选型与向量相似度验证

**选型评估的多维框架**：

| 维度 | 评估方法 | 关键发现 |
|-----|---------|---------|
| 语义理解能力 | MTEB基准测试、领域特定测试集 | 通用模型 vs 领域微调模型的权衡 |
| 领域适应性 | 目标领域文本的检索效果验证 | 垂直场景专用模型往往更优 |
| 计算效率 | 编码速度、批处理能力、资源占用 | 实时场景 vs 离线场景的差异化需求 |
| 部署便利性 | API稳定性、本地部署支持、许可协议 | 企业合规与成本考量 |

#### 6.1.4 Day 31–33：混合检索架构（关键词+语义+重排序）效果对比

**混合架构的测试设计**：

| 组件 | 功能定位 | 测试方法 |
|-----|---------|---------|
| 关键词检索（稀疏） | 精确匹配、专有名词、ID查询 | 召回率、精确率、查询延迟 |
| 语义检索（密集） | 同义理解、概念关联、开放式查询 | 语义相似度校准、领域适配性 |
| 重排序模型 | 精细化相关性排序 | 排序准确性、延迟开销、稳定性 |

**关键优化**：使用专门的Reranker模型（如NVIDIA实践）可显著提升最终排序准确性，但需权衡延迟成本。

### 6.2 模块五：生成层质量评估

#### 6.2.1 Day 34–36：忠实度（Faithfulness）自动化检测

**Ragas Faithfulness指标深度应用**：基于声明抽取和上下文验证的自动化评分，以及自定义扩展开发（针对特定业务场景的忠实度定义）。

#### 6.2.2 Day 37–39：答案相关性（Answer Relevance）多维度评估

**多维度评估框架**：

| 维度 | 定义 | 计算方法 |
|-----|------|---------|
| 主题相关性 | 答案涉及问题相关主题领域 | 主题分类模型、关键词重叠 |
| 意图相关性 | 答案针对问题的真实意图 | 意图识别模型、问答匹配度 |
| 完整相关性 | 答案覆盖问题的所有方面 | 问题分解、要点覆盖检查 |
| 时效相关性 | 答案中的时间信息符合要求 | 时间表达式提取、时效性验证 |

#### 6.2.3 Day 40–42：上下文利用率与信息压缩效率测试

**信息压缩策略评估**：父文档检索器（检索小片段、返回大段落）的效果验证，上下文窗口利用效率的量化分析。

### 6.3 模块六：端到端RAG系统测试

#### 6.3.1 Day 43–45：Ragas框架实战与自定义指标开发

Ragas五大核心指标（Context Precision、Context Recall、Context Relevancy、Faithfulness、Answer Relevancy）的实战应用，以及针对特定业务需求的指标扩展开发。

#### 6.3.2 Day 46–48：LLM-as-a-Judge评估流水线构建

**评委模型的选择与校准**：

| 考量因素 | 决策要点 | 风险缓解 |
|---------|---------|---------|
| 评委能力 | 与任务匹配的推理和判断能力 | 多评委投票、一致性检查 |
| 成本效率 | API调用成本与评估频率的平衡 | 分层评估（粗筛+精评） |
| 偏见控制 | Position Bias、Verbosity Bias等 | 随机化顺序、多轮评估、校准集 |

#### 6.3.3 Day 49–50：A/B测试与线上效果监控体系

**生产环境监控的关键指标**：检索延迟分布、生成token消耗、用户满意度相关性、Embedding漂移检测、幻觉率趋势。

## 7. 第四阶段：Agent & LangChain测试（第7–8周）

### 7.1 模块七：Agent核心能力验证

#### 7.1.1 Day 51–53：工具调用正确性与参数解析测试

**工具调用的三层验证**：

| 层级 | 验证内容 | 测试方法 |
|-----|---------|---------|
| 工具选择 | 正确识别所需工具 | 意图-工具映射测试集 |
| 参数填充 | 正确解析和格式化参数 | Schema验证、边界值测试 |
| 结果处理 | 正确理解和利用工具返回 | 模拟工具响应、错误注入 |

#### 7.1.2 Day 54–56：多轮对话状态管理与上下文一致性

**状态管理测试的核心挑战**：长期记忆的一致性、用户意图变化的正确处理、外部事件（工具调用结果）对对话走向的影响。

#### 7.1.3 Day 57–59：Agent决策路径可追溯性与可解释性

**可解释性测试方法**：Chain-of-Thought记录完整性、关键决策依据的上下文引用、错误传播路径的追踪分析。

### 7.2 模块八：LangChain系统稳定性

#### 7.2.1 Day 60–62：链式流程异常处理与降级机制

**故障场景与应对策略**：

| 故障类型 | 触发条件 | 降级策略 |
|---------|---------|---------|
| 组件超时 | LLM API响应延迟 | 缓存返回、简化流程、人工转接 |
| 组件失败 | 工具调用异常、格式错误 | 备用工具、错误提示、流程回退 |
| 级联故障 | 多组件连续失败 | 熔断机制、服务降级、告警触发 |

#### 7.2.2 Day 63–65：并发场景下的资源竞争与超时控制

**性能测试的关键维度**：负载生成策略、资源监控指标（GPU利用率、内存占用、连接池状态）、瓶颈定位方法、容量规划模型。

#### 7.2.3 Day 66–68：记忆模块持久化与数据隔离测试

**隐私合规测试要点**：用户数据隔离验证、敏感信息脱敏检查、存储加密与访问控制、数据保留期限执行。

### 7.3 模块九：AI测试助手实战

#### 7.3.1 Day 69–70：构建测试数据生成Agent与数据库交互验证

**"用AI测试AI"的元能力整合**：测试数据生成Agent的设计、数据库Schema理解、约束条件满足验证、生成数据质量评估。

## 8. 第五阶段：综合实战与能力认证（第9–10周）

### 8.1 模块十：企业级综合Demo

#### 8.1.1 Day 71–75：端到端AI系统测试方案设计与执行

**完整测试生命周期覆盖**：需求分析、风险识别、策略制定、用例设计、环境搭建、执行监控、结果分析。

#### 8.1.2 Day 76–80：性能压测、安全扫描与可观测性集成

**OWASP LLM Top 10安全覆盖**：Prompt注入、不安全的输出处理、训练数据投毒、模型拒绝服务、供应链漏洞等。

### 8.2 模块十一：质量报告与沟通

#### 8.2.1 Day 81–85：AI系统质量评估报告撰写（含风险评级与上线建议）

**分层报告结构**：

| 版本 | 核心内容 | 篇幅 | 决策支持 |
|-----|---------|------|---------|
| 执行摘要 | 关键发现、风险评级、上线建议 | 1页 | 管理层快速决策 |
| 技术详情 | 测试设计、执行数据、缺陷分析 | 10-20页 | 开发团队修复指导 |
| 完整报告 | 方法说明、原始数据、附录 | 50页+ | 审计追溯、知识沉淀 |

#### 8.2.2 Day 86–90：向技术团队/管理层/业务方的分层汇报演练

**沟通技能的场景化训练**：技术细节的解释转化、业务影响的量化呈现、不确定性下的决策建议、异议处理与共识建立。

### 8.3 结业作品与能力认证

#### 8.3.1 可选方向A：特定行业RAG系统完整测试方案

深度定制：金融合规审查（关注可解释性与审计追踪）、医疗诊断辅助（关注安全性与知识时效）、法律合同分析（关注引用准确性与时效性）、电商智能客服（关注转化率与用户体验）。

#### 8.3.2 可选方向B：Agent系统稳定性测试框架

平台化建设：可复用的测试工具链、自动化流水线配置、混沌工程集成、监控告警体系。

#### 8.3.3 可选方向C：企业AI质量体系搭建路线图

战略视角：现状评估、目标设定、路径规划、资源需求、里程碑定义、风险预案。

## 9. 知识库沉淀规范

### 9.1 每日项目文档模板

标准化七章节：实验目标与测试范围、环境配置与依赖清单、核心风险点与测试思路、可量化质量指标与通过标准、执行步骤与数据记录、常见问题排查、扩展方向与优化建议。

### 9.2 企业内训材料包

模块化交付：按模块拆分的PPT讲义（含演讲者备注）、配套实验环境（Docker一键部署）、考核题库（选择题/实操题/案例分析题，含评分标准）。

## 10. 商业化形态矩阵

### 10.1 GitHub开源项目

代码库结构：day-01至day-60完整Demo，社区运营机制（Issue模板、贡献指南、Star里程碑激励）。

### 10.2 实战图书

《AI测试实战：60天从入门到上岗》，10章对应10周学习周期，渐进式代码披露、丰富图示、案例叙事。

### 10.3 在线课程产品

每日解锁+社群打卡+直播答疑，结业认证：可验证的能力徽章与技能雷达图。

### 10.4 企业内训服务

快速部署：2天浓缩版/5天速成版/完整10周版/行业定制版，即拿即用的材料包与专属服务支持。

